Title,Course(s),Course name abbreviation,Type (paper or slides),Topic 1,Topic 2,Topic 3,Keywords,Word count,Page count,Author(s),Year created,Abstract
applied machine learning lecture 1,applied machine learning,AML,slides,machine course project intro uva deep assignment open system classification ,data math lab questions kaggle groups representing applied research text,learning week regression tutorial canvas exam lectures code function vector,,1010,49,Pascal Mettes,2019,"Applied Machine Learning lecture  Introduction to Applied Machine Learning Pascal Mettes  – University of Amsterdam Lecturer of today: Thomas Mensink  - Google  Agenda of the day Welcome Introduction Course and syllabus Regression I  Welcome PhD Computer Science @ Grenoble (France) PostDoc researcher @ UvA Assistant Professor @ UvA Research Scientist @ Google Research topics: D, Deep Learning, large scale image classification,  machine learning, learning without examples.  Lecturer PhD Computer Science @ UvA Visiting Researcher @ Columbia University, USA PostDoc researcher @ UvA Assistant professor @ UvA Research topics: action recognition, action localization, deep learning,  learning from limited supervision.  Hi  Other people of the course Cristian Rodriguez  Rivero Ivan Sosnovik Shi Hu Sarah Ibrahimi David Zhang  Students Admissions forcefully stopped at .  Syllabus Full pdf with course details available on canvas. Syllabus states course details, deadline, and required reading  materials.  The course Math tutorial Lab   Intro and Regression I Regression II Lab Intro   Classification I Classification II Math tutorial  Lab Assignment    Deep Learning I Representing Images Lab Assignment    Representing Text Deep Learning II Math tutorial  Peer feedback open questions Kaggle - Project week    Setting up a ML system Intro to Reinforcement Learning Recommender Systems Kaggle - Project week    AI for industry and humanities Autonomous driving Math tutorial  Kaggle - Project week    Q&A Kaggle - Project week    Exam Poster/Demo Session  The good, the bad, and the ugly  The bad news This course is only  weeks long.  theory lectures  expert/applied lectures  Q&A session Note: this course is considered as hard!  The ugly news The course is packed Students come from many different backgrounds and  experiences. We are forced by UvA planning to split the practicals in  groups. Each person is randomly assigned to a group. If your group timeslot is not suitable, you can try and switch with  someone yourself. As long as rooms are not overcrowded, I am fine.  The good news This course is here to teach you a lot. We are all here to help you out. Canvas, lectures, labs, tutorials…  Overall schedule Math tutorial Lab   Intro and Regression I Regression II Lab Intro   Classification I Classification II Math tutorial  Lab Assignment    Deep Learning I Representing Images Lab Assignment    Representing Text Deep Learning II Math tutorial  Peer feedback open questions Kaggle - Project week    Setting up a ML system Intro to Reinforcement Learning Recommender Systems Kaggle - Project week    AI for industry and humanities Autonomous driving Math tutorial  Kaggle - Project week    Q&A Kaggle - Project week    Exam Poster/Demo Session  Lab assignments Three weekly assignment files. Week  has no deliverable, serves as practice. Deadlines week :   -  -  : . Week :   -  -  : . Each week, an ipython notebook to be filled in, with automatic  grading!  Tutorials and open questions Dive deeper into math of machine learning. Questions partially covered in exam. Flipped classroom tutorials in week , , and . Come prepared. Open question assignment (exam practice) on Canvas (due in week ).  Final project Implement and improve your own full machine learning pipeline! Competition amongst groups of three (groups made on Canvas). https://lh.googleusercontent.com/mYuXEreeMmdPnflkzHWdXOpHOOcpgMCqxSKdfFUwIQHCDGbxmdwTkRSryGljkBOxcZcELOHnMNocLTWbEU_Ty_wOV_lNdnUyPfxkairCiOJfNfsLo https://lh.googleusercontent.com/frkWT-YIuGBszMkggkJKZ-mSFkEBueQgsHlmJioJbZjoVCUMaZO_bRndlcHoDTRwTAMmlRX-QuxmDxEeTmoWCGUpeW-uGpcqCQjKxSrETzBKOg https://lh.googleusercontent.com/iPSlewtnIdxHkKbUwNUcbvAiJgLME_VTVCvdlVZhfV-BGGUOVgh-iQOYmnzHnuOHhdZGKQDCojL-UDozYjvZSVOCohNknCLaqcYboBptiEvrk_w Project : Food recognition  challenge Project : Common sense challenge Project : Meme analysis challenge  Exam Consists of closed and open questions. Covers every lecture. Examples from previous years on Canvas, naturally without answers.  Content Skill  Asking questions For general questions and discussions, we use Canvas and lectures. Programming and math questions discussed during labs and tutorials. Questions on grades, groups, etc, mail Cristian:  c.m.rodriguezrivero@uva.n l Any remaining problem or issues, weekly open office with Pascal: Every Tuesday from : to : in C..  Code of conduct We encourage you to help each other… Your grade depends on what you do, not what others do … and us: give feedback on the course, it can always improve  High expectations We from you, you from us Come prepared: read materials, watch videos, look for solutions Invest time However, we do not tolerate blind copy Not from each other Not from the internet  Applied Machine Learning  Wat is machine learning to you?  Machine learning  Given input  x Do some fancy computing Predict output  y Tom Mitchel, Machine Learning, : A computer learns, when for some tasks T; and performance measure P; the value P increases for T with experience E.  Types of data Structured Sensor readings, tax forms, excel tables Unstructured Images, free text, speech This course will especially focus on unstructured data!  Machine learning lingo Concepts Task, system, evaluation, data Mathematics Required to formalize and understand the core Functions, vectors, matrices, loss, probabilities, predictions Code Enable and use machine learning Python current go to, optionally with sklearn, PyTorch etc.  Source: Théo Szymkowiak, “ sectors where automation will take over in the short term”  Break  Regression I  Regression What price?  Machine learning system World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective  Function Features  Input features Represented as a vector  x Representation and values depend on task. What is a possible feature vector of a house? E.g. vector [m  , has_garage, has_balcony, nr_bathrooms, ...] What is a possible feature vector of an image? Hard to represent, topic of lecture !  Machine learning system World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective  Function Learning Model Objective  Function  Estimating house price  Regression  Linear regression  Adding an intercept In a standard setup, the output is solely a function of the input. If  x  = [,,..,] T then the output is always . To overcome this, we add an intercept: x  := [ x ] w  := [ w b]  Beyond linear regression With the proper choice for f, we can do  much more than straight lines.  Fitting a line  t x  Fitting a line  t x  When are predictions good? Data point x n True value t n Predicted  ṫ n f( x n , w )  How good is a model? Each model is a line and corresponds to a score, s(w)  What if I get more data? I get more features? x = [ a, b, c, d, e, f ]  Question: ZIP code feature You want to add zip code as feature New representation: x := [x zip] (zip :   XH )  It is known by real estate agents that the zipcode is a strong feature,  yet your new model performs badly.  . Explain why this is the case . How could you do better?  Content of today Course logistics. Machine learning definition and system. Intro to regression.  What is next Laptopcollege Group D tomorrow, other groups on Friday. Lecture Regression II, Friday : - :, C.  Thank you "
applied machine learning lecture 2,applied machine learning,AML,slides,linear optimization descent test derivative using closed loss lecture overfitting,data gradient learning find evaluation features non form compute small,regression objective function value best model training labels world task,,1034,43,Pascal Mettes,2019,"Applied Machine Learning lecture  Regression II Pascal Mettes  – University of Amsterdam Lecturer of today: Cristian Rodriguez Rivero  Lecturer PhD  in Engineering Sciences @ UNC, Argentina Adjunct. Prof. @ Aeronautical University Institute, Argentina PostDoc researcher @  University of California at Davis Lecturer  @  UvA Research topics:  Forecasting, Bayesian Learning, Optimization, Deep  Learning, Image Processing,  Neuroengineering , Robotics.  Regression recap What price?  Machine learning system World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective  Function  This lecture The objective of regression. Optimization and (stochastic) gradient descent. Non - linear regression and regularization.  From data to predictions  t x  Regression Predict continuous output value t given input vector  x . 𝑡=𝑓(𝒙,𝒘) The goal is: • Determine function class f. • Find best values for  w from training set.  Linear regression Linear score function: 𝑡=𝑓𝒙,𝒘=𝒙𝑇𝒘 Learning objective: Our goal: Use  J to find the best value for  w . 𝐽𝒘=σ 𝑛(𝑡𝑛−𝒙𝑛𝑇 𝒘)  Find the value of w (slope of line left) that minimizes MSE (=J(w)) x w Linear regression: target and MSE  The best value for  w Remember high - school calculus! Find the best  w  using the gradient of J. Gradient of function (blue curve) is zero at  extremes, in this case the  minium .  How to find  w To find the best w, we first need the derivative  of J(w) What is the derivative of J(W)  w.r.t . w ? 𝐽𝑤=σ 𝑛(𝑡𝑛−𝑤𝑇𝑥𝑛)  How to find w To find the best w, we first need the derivative of J(w) What is the derivative of J(W)  w.r.t . w ? 𝐽𝒘=σ 𝑛(𝑡𝑛−𝑤𝑇𝑥𝑛) 𝛻𝑤𝐽𝒘=−σ 𝑛(𝑡𝑛−𝑤𝑇𝑥𝑛)𝒙𝑛  Solving linear regression  – Closed form Solve linear regression in a single equation. First, we write everything in matrix form: 𝛻𝑤𝐽𝒘=−σ 𝑛(𝑡𝑛−𝒙𝑛𝑇 𝒘)𝒙𝑛=−𝑇−𝑊𝑇𝑋 𝑇 𝑋𝑇  Solving linear regression  – Closed form How do we use the derivative to get the best value for  w ? We set the derivative to ! 𝛻𝑤𝐽𝒘=−σ 𝑛(𝑡𝑛−𝒙𝑛𝑇𝒘)𝒙𝑛=−𝑇−𝑊𝑇𝑋 𝑇 𝑋𝑇  Solving linear regression  - Closed form −𝑇−𝑊𝑇𝑋𝑇𝑋𝑇= −𝑇𝑋𝑇+𝑊𝑇𝑋𝑋𝑇= 𝑊𝑇𝑋𝑋𝑇=𝑇𝑋𝑇 𝑋𝑋𝑇𝑊=𝑋𝑇𝑇 𝑊=(𝑋𝑋𝑇)−𝑋𝑇𝑇 Take away the parentheses Move left side over Transpose on both sides Multiply both sides with inverse part Check: W=Dx, X= DxN , T=xn  Solving linear regression  - Gradient descent Start with a w  For t=…. where  𝜸is small 𝑤𝑡+=𝑤𝑡+𝛾𝑑 𝑑𝑤𝑡 𝑓(𝑤𝑡)  Gradient descent vs. Closed form Pros of closed form solution Direct minimum and solution in one step. Cons of gradient descent Handles non - convex optimization, scales better.  Watch out for local minima  Gradient descent Compute exact gradient of loss using  all data Advantages exact advanced optimization techniques (Newton,  … ) Disadvantages Memory Speed of computing gradients Slow convergence  Break  Gradient descent Compute exact gradient of loss using  all data Advantages exact advanced optimization techniques (Newton,  … ) Disadvantages Memory Speed of computing gradients Slow convergence  Stochastic gradient descent Start with w  For t=…. Select a few examples {x} t 𝑤𝑡+=𝑤𝑡+𝛾𝑑 𝑑𝑤𝑡 𝑓(𝑤𝑡,{𝑥}𝑡)  Stochastic gradient descent Approximate gradient by using few data samples Advantages Fast to compute, memory efficient Provable convergence for small  𝜸 Implicit regularization Disadvantages Convergence requires tuning of learning rate ( 𝜸) Less advanced optimization techniques However: recently new optimizers ( eg ADAM, momentum,  … ) Core component for deep learning!  Linear regression as a network Forward: Compute t* using x and w. Fully - connected:  w T x Backward: Compute gradient: x Loss: Square difference: ½ (t - t*)  Gradient: (t - t*) x  x  x  … x d x  ṫ w  w  w  w d  Non - linear regression  Polynomial regression Linear: f( x,w ) =  x T w Non - linear regression example: polynomial regression f n ( x,w ) =  x T w + (x  ) T w + … + ( x n ) T w The higher  n , the more non - linear the function What is the best value for  n in regression?  Underfitting and overfitting  Tackling overfitting with data   Regularization Data point x n True value t n Predicted  ṫ n = f( x n ,w ) Learning objective: minimize ½ ∑ n  ( t n – f( x n ,w ))    +  λR (w) R(w) = ∑ d w d  Intuition: lower weight is lower complexity  Regularization against overfitting λ =  λ = ,  Regularization against overfitting  Regularization: implementation Add to objective function  … +  λ R(w) Add to gradient ∂w  λ ∑ d w d   =  λ  w w   w  – ( ∂ loss w +  λ  w) Drives w towards zero Stochastic Gradient Descent w t  w t -  - 𝜸∂ loss w Small  𝜸value: only small SGD steps, slowly drives Limited number of iterations  Practical test case of regression  Predict price Given a house  Regression input World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective  Function . Structured data  (address, size, rooms) . Textual description . Images  Regression labels World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective  Function . Structured data  (address, size, rooms) . Textual description . Images Prices : € .  Regression train/test World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective  Function Collect  N houses with  their prizes Set another  M  to the side  Regression learning World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective  Function Linear Regression + Regularization Objective: ( t n - ṫ n )  Stochastic Gradient  Descent  Regression evaluation World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective  Function Interval Accuracy ∑ | t n - ṫ n |< K M  houses (with  price)  Question: ZIP code feature You want to add zip code as feature New representation: x  ⃪ [x zip] (zip :   XH )  It is known by real estate agents that the  zipcode is a strong feature, yet  your new model performs badly.  . Explain why this is the case . How could you do better?  This lecture The objective of regression. Optimization and (stochastic) gradient descent. Non - linear regression and regularization.  What’s next Tutorial Groups A - D from Tuesday to Friday next week. Laptop college Group B,C at : today. Lecture Wednesday C.:  Classification I.  Thank you "
applied machine learning lecture 3,applied machine learning,AML,slides,classification regression decision class output function find lab line margin,sigmoid boundaries learning label math tutorial week gradient knn kaggle,linear inputs logistic one binary nearest example intro assignment hinge,,1029,50,Pascal Mettes,2019,"Applied Machine Learning lecture  Classification I Pascal Mettes  – University of Amsterdam  Reiteration of course schedule Math tutorial Lab   Intro and Regression I Regression II Lab Intro   Classification I Classification II Math tutorial  Lab Assignment    Deep Learning I Representing Images Lab Assignment    Representing Text Deep Learning II Math tutorial  Peer feedback open questions Kaggle - Project week    Setting up a ML system Intro to Reinforcement Learning Recommender Systems Kaggle - Project week    AI for industry and humanities Autonomous driving Math tutorial  Kaggle - Project week    Q&A Kaggle - Project week    Exam Poster/Demo Session  Content Skill  Few points of discussion Lecture recordings Required math level Attendance  Recap week  Linear score function: 𝑡=𝑓𝒙,𝒘=𝒙𝑇𝒘 Learning objective: Each line has a score stating how good the solution is. 𝐽𝒘=σ 𝑛(𝑡𝑛−𝒙𝑛𝑇 𝒘)  The best value for  w Find the best  w  using the gradient of J. Gradient of function (blue curve) is zero at  extremes, in this case the minimum. Solutions e.g. in closed form or with gradient  descent.  Which line is best based on  J ?  Generalization  Underfitting and overfitting Find a balance between fitting training data and having a simple model.  Classification versus regression Regression Predict a continuous output given inputs. Find a line that is as close as possible to all input features. Classification Predict discrete output given inputs. Find a line that separates inputs of different classes.  Classification with linear regression Feature Label  Why linear regression is not optimal Linear regression is useful for correlating continuous inputs and outputs. For classification, does not really capture distribution. Also, model is unbounded, but labels are discrete.  Logistic regression Start from a linear model akin to linear regression. Add a activation function that transforms output into “probability”. Easiest solution: step function (not differentiable, discussion next week). Activation function in logistic regression: Sigmoid function.  Sigmoid function Input: any real value. Output: [,]. Differentiable: 𝑑 𝑑𝑧 𝜎𝑧=𝜎𝑧(−𝜎𝑧)  Logistic regression with gradient descent . Linear map: h =  x T w + b . Sigmoid activation: f =  σ (h) . Compute error between sigmoid output and label . Update values of w and b based on error forward pass evaluation backward pass Logistic regression = one - layer network  Negative log likelihood Goal: For examples with labels    low sigmoid output For examples with label    high sigmoid output Formula: 𝐿=−  𝑁 ෍ 𝑖 (𝑦𝑖log𝑓𝑖+−𝑦𝑖log(−𝑓𝑖))  Backward pass 𝑤𝑛𝑒𝑤=𝑤𝑜𝑙𝑑+𝛾 𝛿𝐿 𝛿𝑤 𝛿𝐿 𝛿𝑤 = 𝑑𝐿 𝑑𝑓 𝑑𝑓 𝑑ℎ 𝑑ℎ 𝑑𝑤 Derivative of NLL and full implementation in lab assignment. Gradient descent step Chain rule Derivative of sigmoid x Derivative of NLL  Linear classification with margins Feature  x is N - dimensional representation. Label  y is binary { - , }. Goal Find weights  w such that f( x ,  w ) predicts  ŷ . ŷ = sign(f( x ,  w ))  Beware of dimensions  input (x - axis)  output (y - axis)  inputs (x and y)  output (color)  Decision boundary: change?  Decision boundary: change?  Decision boundary: change? Margin One error, but large margin No errors, but small margin  F( x,w ) = Confidence The further away from the hyperplane, the more  confident my algorithm is f( x,w ) =  f( x,w ) >  f( x,w ) <   When are predictions good? Data point  x n Ground truth label  y n Prediction f( x n , w ) The hinge loss: L = |   – y n f( x n , w )| +  = max(,  - y n f( x n , w )) + -  Break  When are predictions good? Data point  x n Ground truth label  y n Prediction f( x n , w ) The hinge loss: L = |   – y n f( x n , w )| +  = max(,  - y n f( x n , w )) + -  Hinge  - Loss  Classification objective Minimize: 𝐿=σ 𝑖= 𝑛|−𝑦𝑛𝑓(𝒙𝑛,𝒘)| Find  w such that: • There are not too many errors. • There is a large margin between  hyperplane and examples.  SVMs min 𝑤 |𝑤|+𝐶෍ 𝑖= 𝑛 𝜉𝑖 𝑠𝑢𝑏𝑗𝑒𝑐𝑡𝑡𝑜:𝑦𝑖𝑤∙𝑥𝑖+𝑏≥−𝜉𝑖 𝑎𝑛𝑑𝜉𝑖≥∀𝑖 Optimizes both margin maximization and error minimization. Highly effective linear classifier.  What are support vectors?  Multi - class linear models  Multi - class linear models  One - vs - rest classification Main idea: Learn K classifiers for K classes. Each classifier is binary. One class as positive, all others as  negatives.  One - vs - rest decision boundaries Decision rule: ො 𝑦=argmax 𝑘 𝑤𝑘𝑥+𝑏𝑘 Can you color in the right side to  denote the decision boundaries for all  locations?  Nearest  neighbour classification Label of test example is determined by label of nearest training example. KNN figures courtesy of David Sontag, NYU   - nn decision boundaries No explicit hyperplanes and decision boundaries. Decision boundaries formed by Voronoi diagram of training set.  Knn decision  bounaries F:\Dropbox\VSEpruts\knn.png F:\Dropbox\VSEpruts\knn.png F:\Dropbox\VSEpruts\knn.png  Decision rules for multiple  neighbours Value for k can greatly influence  the outcome. General k - nn rule: majority vote.  Ties How to avoid a tie in binary classification? Set k to an odd value. How to a void a tie in multi - class classification? Incorporate weight inversely proportional to distance.  k - nn example Hastie and  Tibshirani , Chapter   Pros and cons of k - nn Pros No training Complex decision boundaries Inherent to binary and multi - class Cons Does not scale well Sensitive to outliers  Nearest Class Mean classifier “Learning” Compute mean of each class. “Recognition/Inference” Find nearest cluster. What is the relation to hyperplane approaches? Decision boundary is a hyperplane orthogonal  to the line going through both clusters.  In between  kNN and NCM Cluster features from each class. For each example: class = label of nearest cluster.  Exercise: drawing boundaries Linear  SVM kNN (k=) NCM  Topics of today Logistic regression SVMs (binary, multi - class, non - linear) kNN  What’s next Lecture Classification I: tomorrow  -  SP H. Labs Group D today  - , A - C on Friday Tutorial Tuesday - Friday this week  Thank you "
applied machine learning lecture 4,applied machine learning,AML,slides,training trees adaboost forests one examples train split node project,tree example multiple select classification positives subset task learning,random entropy decision features weak average models classifier toy many,,797,43,Pascal Mettes,2019,"Applied Machine Learning lecture Classification II Pascal Mettes – University of Amsterdam  Classification yesterday F:\Dropbox\VSEpruts\knnpng  Topics of today Decision trees Random forests Boosting Early introduction to the final project  https://lhgoogleusercontentcom/GPCBpouQHPaFMMwkCHS_GhSMIgoGsrjSzavVGTllcENvk-mAgyfJbxGcdIKEBlJRkGUTTFzVfsyrgrdYJqqcVgANBoMTGfTdfxeKfyVeuXdBeIFfoQuBV-xM  Building a decision tree https://lhgoogleusercontentcom/-YazHJiYZGcafZRLQ_UmYRaJWrzLDwuLNvChOuaTQZgtDphzGoFQslaUELFgzcSMkDydRjdXLIBxZqYvnwxNbhdsTNrAp_GNVfL-ucVLCvPSYEQifPmCwfnVmUYA  Benefits of decision trees Explain the reasoning behind a decision Work on discrete and continuous features Work with incomparable features Fast to train and to evaluate  Alternative decision tree Also explains the data, but which one is preferred? https://lhgoogleusercontentcom/ALmNfwafPpovPSceXsXEphTWgjxVXMgZSdIiNDeJ_FToDusHsMbSjjBPnxLWLSdldhAvDvqTuFxtKEwOFZOXJgKE-eua_yqMvNknVjVtvyIMWEqKpbbONaQ  How to split data into a tree Main notions We want a simple tree Each node should be as “pure” as possible, one feature at a time Pureness measure Evaluation using entropy The more uniform a distribution in a node, the higher the entropy https://lhgoogleusercontentcom/GSKOWFBMlO_bKOkQpkeRLt_RxQxJvLqavxDvyG-XqD-LcgUwyjPbZfJ_msyklwyEPFGzNsHNNZm_yFKHOkXzSyrsAbUj_RBuCf-wxyoFWLW_RTHXjVJ-TDP  Entropy for two classes Notation S denotes the training set P denotes the ratio of positives in S N denotes the ratio of negatives in S Entropy formula 𝐸𝑆=−𝑃𝑙𝑜𝑔𝑃−𝑁𝑙𝑜𝑔(𝑁) https://lhgoogleusercontentcom/HnoiEfPXnbTwNCKMzcZuzhbTyFvvQHPZGQcizOstFfpBdjkiFmppYBhLzrjr-KhCNXU-DGmiurmKZGJEBm-FMTCdkUoLJLXBiacueqK-OiASlCIk  Entropy examples positives, negatives: positives, negatives: positives, negative: 𝐸𝑆=−𝑃𝑙𝑜𝑔𝑃−𝑁𝑙𝑜𝑔(𝑁) −𝑙𝑜𝑔−𝑙𝑜𝑔= −𝑙𝑜𝑔−𝑙𝑜𝑔= −𝑙𝑜𝑔−𝑙𝑜𝑔=  Practicing tree splitting Practice on the board  Multi - class decision trees  Multi - class entropy The entropy at a certain node: Weighted average of entropies at a split A : 𝐸𝑆=−𝑃𝑙𝑜𝑔𝑃−𝑃𝑙𝑜𝑔𝑃−⋯−𝑃𝑛𝑙𝑜𝑔𝑃𝑛 𝐸𝑆=−෍ 𝑖= 𝑛 𝑃𝑖𝑙𝑜𝑔𝑃𝑖 𝐸𝑆=−෍ 𝑖= 𝑇(𝐴) 𝑆𝑖 𝑆 𝐸(𝑆𝑖)  Information gain Which split is the best? The one that decreases the entropy the most G 𝑆,𝐴=𝐸𝑆−𝐼(𝑆,𝐴) https://lhgoogleusercontentcom/DhtEUCafwZskTwqgaiaiIGcxmSjIkGXHqODPagDyGMy-YZnlFkrnPcODMnAUZRtvYTU-DjueDkWGUCGlXkMreGgRsg_wefzfo-QKElMwWW-bvgU_wncls  Limits of decision trees For many examples and features, we obtain deep trees Perfect training separation, but poor generalization Three solutions Stop growing the tree (suboptimal training fit) Grow full tree, then prune (shorten) tree a posteriori Combine multiple trees  Random forests https://lhgoogleusercontentcom/qbuhPpDauWqTioMinCPmLLQxHoXHzhaUCYFDzjglvboCjRBxeFfdrkKnDXcULKrDtnAECADyORewbrrKDVbQwpjTAcNBbyJPhElxRCWiVlmeFabbYvrpUVoSRU  Random forests https://lhgoogleusercontentcom/qbuhPpDauWqTioMinCPmLLQxHoXHzhaUCYFDzjglvboCjRBxeFfdrkKnDXcULKrDtnAECADyORewbrrKDVbQwpjTAcNBbyJPhElxRCWiVlmeFabbYvrpUVoSRU Rather than training one tree perfectly, train multiple trees imperfectly Train many trees, each trained on a random subset of features and/or examples  Training random forests https://lhgoogleusercontentcom/qbuhPpDauWqTioMinCPmLLQxHoXHzhaUCYFDzjglvboCjRBxeFfdrkKnDXcULKrDtnAECADyORewbrrKDVbQwpjTAcNBbyJPhElxRCWiVlmeFabbYvrpUVoSRU Parameters K trees, D tree depth, R #features per split Training • Initialize forest F • For tree k = ,, K • For depth d = ,, D • Randomly select R features • Split node based on features • Add k to F  Inference in random forests https://lhgoogleusercontentcom/qbuhPpDauWqTioMinCPmLLQxHoXHzhaUCYFDzjglvboCjRBxeFfdrkKnDXcULKrDtnAECADyORewbrrKDVbQwpjTAcNBbyJPhElxRCWiVlmeFabbYvrpUVoSRU Traverse each tree separately for a test example Average class scores over trees and select maximum average score  Why are random forests effective? The average of many estimates with low bias and high variance is close to true estimate https://lhgoogleusercontentcom/BTpJBlgYbwaPAqmzBthdDgb_yDikOtKIGGnOeoaCngVKbzkBWzsCLaBcICkqIyVm_jBpjyqrGzzTHXLyjBwllMwwECnebjREUOAZVrVXnEbSRwIqFoFbiE  Using many classifiers for prediction https://lhgoogleusercontentcom/hkVtozoPLjIMzzcJjiQvdTky_jNkmDVWSbViSvwbBRU-_OgXBwNlmh-WitzbegKIIcwwCDqnVySPqTzOcSBsdeEIX-t_GfDOSCQ-wzULvWeUyxrvKlNQ https://lhgoogleusercontentcom/qoMh-ShtlSKSPbgFC-laYuEUVgJjgcdFQsOFK_QUSNoAYsaILKVsmHtSQjUL-zvxNoyHNSuQnkCxAcDrVXmKcMlsy-kJmR-UdEgkmCXnRHGrGadwHGyxqKYbE  Ensembling Main idea: Generate multiple models and combine them into one classifier Two problems: How do we get multiple models? How do we aggregate outputs of multiple models?  Bagging Bootstrap sampling: For training set D containing N examples, create a training subset D’ by drawing N examples at random with replacement from D Bagging: • Create k bootstrap subsets D ,…, D k • Train a classifier for each subset • Classify test example by majority vote over subset classifiers  Break  Boosting Within standard bagging and ensembling , models are trained separately Is there a gain if we train models to complement each other?  AdaBoost: main idea Attach a weight to each training example Select a “weak classifier” with the lowest training error Increase weight of incorrectly classified training examples Iteratively select new best weak classifier with lower weighted error  Toy example of AdaBoost https://lhgoogleusercontentcom/bDIMgcHCVPQhnsVdEIEzLIDjBdlblSfVSVlQTPsLTualSveHEHLlQYlD-hleNSrGYnHBMuOC-_YDsyWjiOhkKRqOeszdvCMvBfM_MNUMStSarrFxSALntxuEDPE https://lhgoogleusercontentcom/OqEPwQBEGSEBA-PYbPFsKpLjdohVTUtruagaPTXnnnaXDGubzSm_QSQgdrAGhlIbxTAvENSumRjIoLNCsWckTsTCcW-VylVGlVvQyPqJrkGwXQGnXc  Toy example of AdaBoost https://lhgoogleusercontentcom/Hcgqn-EYaMREBurCYyhHTpldjuxOuqRDJsuNfZhbvIn-YFepcUuvHwovyfVBGkohoZi_zPjmlJEkgANAofbPe-aBN-fpbrJUFAMgjYVfImlIsCZNNsbU https://lhgoogleusercontentcom/brMjiJZaRQqPBoMlPAEGJntQxYhHPstiaDzxYzgHsLjeRdDqnrwBWpODMjkKdEjbgiLbTwyPWrPofcEcRqXKrr_G_vKoMoh-yiBDZiWooCxPq-TqhSpLPd  Toy example of AdaBoost https://lhgoogleusercontentcom/BvLwXYJCCXINahaFHaOcEzAZTkQNPHoUqDgvizdQkgtRE-kiJpZIlOnIAjyuyzgOpeFaVhjCpfAXrFovQjIrTZXsgWkEJuWWrscafROjunlkkbmFFlEHrvo  Toy example of AdaBoost https://lhgoogleusercontentcom/DiycGDUa-lCMzdnsrUhkRIPpXETDCZfblMXDYawklzujimSkjpCujlZZtfcJFOCU-zWSzbLKVy-kAGnvwDaLWQheMR-OfEiQpZKctNcybOglwWXOQKZlifsIpQLg  Toy example of AdaBoost https://lhgoogleusercontentcom/xjS_hFdGsNWcbyi-SlTNoiCztmGFJjIAzSlAmevstiiKwZsymxWAuORXmwlAdPVdGSwakWCfVxVvZIVyxhYHtqxeeEZudKArisGkrFGPxYr-tDYFbbJTw  https://lhgoogleusercontentcom/icThEOLNflSooKzOlGcKwYnVQjJAhTDhhiBtJEbUPjeYKBJUbVnDasUeIUdxoft-Op_HPhUbBVOKMrzKLxIUdMKqfN_UJUUpaovkPTeHWDSHn-mRjoCRWcJDBhbo Training data and weights Final classifier Number of iterations Select best weak learner Weak learner weight Update example weights  AdaBoost properties “ AdBoost asymptotically converges to the minimum possible exponential training loss, as long as weak learners perform above average” Increased complexity due to more iterations and weak classifiers does not necessarily lead to overfitting https://lhgoogleusercontentcom/bbRkfP-nCiGVrIk_zZbnsLzvuTKLsy_VdCXzstSNccuLHOCOqCAvyzWCywBKho-BDLgDaEejSdQelUsShTTGJBLzfsYfzmeJUbaiFvNtUVgYCCEqzNLbOt  AdaBoost limitations What happens when weak classifiers are strong? No training error, hence no benefit What is an advantage of standard ensembling ? Parallelism!  Classification in practice  Why teach something that can be abstracted?  The final project Kaggle project on one of three highly challenging tasks Leaderboard per task for all groups in the course “Machine learning proving ground” Generally considered to be a highlight of the course  Task : Food recognition challenge Predict food categories from images https://lhgoogleusercontentcom/mYuXEreeMmdPnflkzHWdXOpHOOcpgMCqxSKdfFUwIQHCDGbxmdwTkRSryGljkBOxcZcELOHnMNocLTWbEU_Ty_wOV_lNdnUyPfxkairCiOJfNfsLo  Task : Common sense challenge Identify common sense (or not) from sentences https://lhgoogleusercontentcom/frkWT-YIuGBszMkggkJKZ-mSFkEBueQgsHlmJioJbZjoVCUMaZO_bRndlcHoDTRwTAMmlRX-QuxmDxEeTmoWCGUpeW-uGpcqCQjKxSrETzBKOg  Task : Meme analysis challenge Humor type classification from images with overlaid text https://lhgoogleusercontentcom/iPSlewtnIdxHkKbUwNUcbvAiJgLME_VTVCvdlVZhfV-BGGUOVgh-iQOYmnzHnuOHhdZGKQDCojL-UDozYjvZSVOCohNknCLaqcYboBptiEvrk_w  Grading Poster and presentation after exam Four axes: Innovation Experiments and setup Analysis Pitch and poster design https://lhgoogleusercontentcom/iUyapHYLkVTrxc-jEOdkvWYZuCzUVptGaWAD-HwfTNIj-_sgahkssTdhrgzThwZhWfDtzx-ZCcBmw-m_fuRdxLIEmduskdw-pNhHSFzgZYeNwGNctIHSzuEMCANw https://lhgoogleusercontentcom/SUmI-ctHadUGB-ASwnidGmCcPDvbogAxWDyQulYMaLREqLLkpNc-AMfCIdQvzDYwDBnxTvcKBZVJCZcHJxChrcCQy-TACdBixllnnnEwGmYATniChJIuY  What’s next Lecture Deep Learning I, Monday - : - :, Turingzaal  Thank you "
applied machine learning lecture 5,applied machine learning,AML,slides,learning example backward gradient data perceptrons linear layer non perceptron,deep step weights output prediction hinton bengio loss,networks forward training function backpropagation input credit layers descent ,,1651,56,Pascal Mettes,2019,"Applied Machine Learning lecture  Deep Learning I Pascal Mettes – University of Amsterdam  This lecture The forward pass of deep networks Non - linearities in deep networks The backward pass of deep networks Regularization in deep networks  Deep learning and its impact Traditional machine learning thrives on structured data, for which features are defined They struggle with loosely structured data, with unknown feature representations Deep learning enables joint recognition and feature representation  Deep learning flow World Data Training data Test data Evaluation Features Labels Optimization Objective Function Learning model  Deep learning flow World Data Deep Learning Training data Test data Evaluation Features Labels Optimization Objective Function  Deep learning in one slide A family of parametric , non - linear and hierarchical representation learning functions , which are massively optimized with stochastic gradient descent to encode domain knowledge , ie domain invariances, stationarity 𝑎𝐿𝑥;𝜃,…,L=ℎ𝐿(ℎ𝐿−…ℎ𝑥,θ,θ𝐿−,θ𝐿) 𝑥:input, θ𝑙: parameters for layer l, 𝑎𝑙=ℎ𝑙(𝑥,θ𝑙): (non - )linear function Given training corpus {𝑋,𝑌}find optimal parameters θ∗←argmin𝜃෍ (𝑥,𝑦)⊆(𝑋,𝑌) ℓ(𝑦,𝑎𝐿𝑥;𝜃,…,L)  A recent revolution  A recent revolution  A recent revolution  Historical perspective on deep learning  Perceptrons, Rosenblatt  Adaline , Widrow and Hoff Perceptrons, Minsky and Papert   Backpropagation, Linnainmaa  Backpropagation, Werbos Backpropagation, Rumelhart , Hinton and Williams  LSTM, Hochreiter and Schmidhuber  OCR, LeCun , Bottou , Bengio and Haffner    Alexnet , LeCun , Bottou , Bengio and Haffner today GO, Deepmind  Imagenet , Deng et al Deep Learning, Hinton, Osindero , Teh Resnet ( layers), MSRA   The perceptron Single layer perceptron for binary classification • One weight w i per input x i • Multiple each input with its weight, sum, and add bias • If result larger than threshold, return , otherwise return  http://cse-wikiunledu/wiki/images//f/Perceptronjpg  Training a perceptron Rosenblatt’s innovation was the learning algorithm for perceptrons Learning algorithm: Initialize weights randomly Take one sample 𝑥𝑖and predict 𝑦𝑖 For erroneous predictions update weights If prediction ෝ 𝑦𝑖=and ground truth 𝑦𝑖=, increase weights If prediction ෝ 𝑦𝑖=and ground truth 𝑦𝑖=, decrease weights Repeat until no errors are made p_N  Problems with the perceptron Rosenblatt () at US Navy press conference: “[The perceptron is] the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence” Perceptrons turned out to only solve linearly separable problems  Historical perspective on deep learning  Perceptrons, Rosenblatt  Adaline , Widrow and Hoff Perceptrons, Minsky and Papert   Backpropagation, Linnainmaa  Backpropagation, Werbos Backpropagation, Rumelhart , Hinton and Williams  LSTM, Hochreiter and Schmidhuber  OCR, LeCun , Bottou , Bengio and Haffner    Alexnet , LeCun , Bottou , Bengio and Haffner today GO, Deepmind  Imagenet , Deng et al Deep Learning, Hinton, Osindero , Teh Resnet ( layers), MSRA   An AI winter Results not as promised due to overhyping • Government and industry cut research funding Interestingly, (the most?) significant discoveries made in this period • Backpropagation • Convolutional networks • Recurrent networks  Historical perspective on deep learning  Perceptrons, Rosenblatt  Adaline , Widrow and Hoff Perceptrons, Minsky and Papert   Backpropagation, Linnainmaa  Backpropagation, Werbos Backpropagation, Rumelhart , Hinton and Williams  LSTM, Hochreiter and Schmidhuber  OCR, LeCun , Bottou , Bengio and Haffner    Alexnet , LeCun , Bottou , Bengio and Haffner  GO, Deepmind  Imagenet , Deng et al Deep Learning, Hinton, Osindero , Teh Resnet ( layers), MSRA   New AI hype The new deep learning summer is a result of these elements:  Data  Hardware  Open - source software  Tricks  Deep learning beyond recognition  Tackling the limitations of the perceptron Famous counterexample: the XOR function How can we separate data points beyond lines?  Multi - layer perceptrons h  h  h  … h h h  ẏ h  h  h  … h d h  x  x  x  … x d x   Training goal and overview We have a dataset of inputs and outputs Initialize all weights and biases with random values Learn weights and biases through “forward - backward” propagation • Forward step: Map input to predicted output • Loss step: Compare predicted output to ground truth output • Backward step: Correct predictions by propagating gradients  Forward propagation In the basics, the same as the perceptron: • Start from the input, multiply with weights, sum, add bias • Repeat for all following layers until you reach the end There is one main new element (next to the multiple layers): • Activation functions after each layer  Why have activation functions? Each hidden/output neuron is a linear sum A combination of linear functions is a linear function! 𝑣𝑥=𝑎𝑥+𝑏 𝑤𝑧=𝑐𝑧+𝑑 𝑤𝑣𝑥=𝑐𝑎𝑥+𝑏+𝑑=𝑎𝑐𝑥+(𝑐𝑏+𝑑) Activation functions transforms the outputs of each neuron This results in non - linear functions  Non - linear activations Sigmoid function Range: (,) Differentiable: 𝑑 𝑑𝑧 𝜎𝑧=𝜎𝑧(−𝜎𝑧)  Non - linear activations ReLU function Range: [,inf) Differentiable? 𝑑 𝑑𝑧 R(z) =  if z at least ,  otherwise  Which activation function is better? Pros of sigmoid: Bounded (usefulness depends on application) Sexy math Pros of ReLU : Easy to implement Strong gradient signal  Forward pass conclusion Going from input to output is a standard procedure Go from one layer to the next until you are at the output At each layer, weighted sum with activation function  Going back: gradient descent Image result for stochastic gradient descent  Recap: finding minima We can find the best value of w by examining the gradient of J(w) Minimal loss when gradient is !  Recap: gradient descent The go - to method for optimization in deep networks, we will find out soon why this is so Start with w  For t=,,T w t+ = w t - 𝜸d/dw t f( w t ) with 𝜸a small value  Recap: stochastic gradient descent Start with w  For t=,,T Select a few examples {x} t w t+ = w t - 𝜸∂ loss f( w t, , {x} t ) where 𝜸is small  Break  Training deep networks h  h  h  ẏ h  h  h  x  x  x   Move input through network to yield prediction  Training deep networks  Move input through network to yield prediction  Compare prediction to ground truth label h  h  h  ẏ h  h  h  x  x  x   Training deep networks  Move input through network to yield prediction  Compare prediction to ground truth label  Backpropagate errors to all weights h  h  h  ẏ h  h  h  x  x  x  Repeat multiple times for all training examples  The backward propagation algorithm Backpropagation = gradient propagation over the whole network Propagate gradient obtained at output back to all neurons Propagation over layers is done with the chain rule  Forward - backward by example Credit to hmkcodegithubio for example  Step : Initialize parameters with random values Forward - backward by example Credit to hmkcodegithubio for example  Step : Forward propagation given training example Forward - backward by example Credit to hmkcodegithubio for example  Step : Calculate error at the output Forward - backward by example Credit to hmkcodegithubio for example  Step : Backpropagate error Forward - backward by example Credit to hmkcodegithubio for example  Step : Backpropagate error Forward - backward by example Credit to hmkcodegithubio for example  Step : Backpropagate error Forward - backward by example Credit to hmkcodegithubio for example  Step : Update weights Forward - backward by example Credit to hmkcodegithubio for example  Step : Repeat Forward - backward by example Credit to hmkcodegithubio for example  Binary classification Now, we want the output to give a decision, by clamping between  and  We can do so using the sigmoid function h  h  h  ẏ h  h  h  x  x  x   Binary cross - entropy loss Let y i ∈ {,} denote the binary label of example i Let p i ∈ [,] denote the output of example i Our goal: minimize p i if y i =, maximize if y i =  Maximize: 𝑝𝑖 𝑦𝑖∙(−𝑝𝑖)𝑦𝑖 Ie minimize: −(𝑦𝑖∙log𝑝𝑖+−𝑦𝑖∙log(−𝑝𝑖))  Multi - class classification For K classes, make the network have K output values Normalize outputs using the softmax function: 𝑝(𝑥)𝑖= exp(𝑥𝑖) σ 𝑗= 𝐾exp(𝑥𝑗) Minimize the following loss: −෍ 𝑗= 𝐾 𝑦𝑗log(p(x)𝑗)  Deep learning in practice Many libraries available, easy to setup, a lot of abstraction PyTorch example  Hyperparameters Deep networks come with many “hyperparameters” It is not always known which settings are best Important hyperparameters to think about: • Number of layers and weights per layer • Number of training iterations • Learning rate for (stochastic) gradient descent • Regularization  DropOut Problem: Deep networks overfit easily when not properly regularized One solution: Randomly drop neurons during training  DropOut Each neuron is retained with probability p during training During testing, use whole network  Why does DropOut work? Motivation : Avoid learning co - occurrences Motivation : During training, sample from exponential number of thinned networks  Other tricks Weight decay: Equal to regularization learned in regression! BatchNorm : Re - normalize features in each layer Residuals: Discussed next lecture Most skills and intuitions gathered from practice!  This lecture The forward pass of deep networks Non - linearities in deep networks The backward pass of deep networks Regularization in deep networks  What’s next Lecture Representing Images, Thursday Nov ,  - , H  Thank you "
applied machine learning lecture 6,applied machine learning,AML,slides,size networks source convolutions example convolution outputs filtered architecture design, filter convolutional output pooling images network operation seitz practice examples,layer input image layers filters learn local weights original dimensionality,,947,43,Pascal Mettes,2019,"Applied Machine Learning lecture Representing Images Convolutions and Convolutional Networks Pascal Mettes – University of Amsterdam  Deep network recap h h h … h h h ẏ h h h … h d h x x x … x d x  Q: Number of input neurons required? W x H x (without bias) Q: Two reasons why this is a bad idea Too many parameters to learn Pixels are not independent! Spatial structure lost Plugging images into networks  Image recognition before deep learning Three main components: Extract local features Aggregative features over images Train classical models on aggregations  The convolutional operation Linear operation: 𝑓𝑥,𝑤=𝑥𝑇𝑤 Global operation, separate weight per feature Dimensionality of w = dimensionality of x output value Convolutional operation: 𝑓𝑥,𝑤=𝑥∗𝑤 Local operation, shared weights over local regions Dimensionality of w much smaller than dimensionality of x Output same size as input  D convolutions Transform data points using neighbours on the line Transformation determined by the weights of the convolutional filter smoothing-box Weights: [ ] / Why is there a division by in the weights? Why is the filter of uneven size? What happens when we use a smaller/larger filter size?  D convolutions Input: Image A of size W x H Filter: “Image” B of size K x K (K = set by hand) Output: Image C of size W x H 𝐶𝑥,𝑦=෍ 𝑖=−(𝐾−)/(𝐾−)/෍ 𝑗=−(𝐾−)/(𝐾−)/ 𝐵[𝑖,𝑗]∗𝐴[𝑥+𝑖,𝑦+𝑗]  D convolutions Input image: x Filter: x Move filter over each image location and compute output What is the size of the output? x  Dealing with the border Possible ways to make sure the inputs and outputs have the same size Source: S Marschner Zero padding Wrap around Copy Reflect  D filter example txp_fig  D filter example Source: S Seitz  D filter example Source: S Seitz  D filter example Source: S Seitz  D filter example Source: S Seitz  Practice examples Source: D Lowe Original Filtered (no change)  Practice examples Source: D Lowe Original Filtered (shift left)  Practice examples Source: D Lowe Original Filtered (blur) txp_fig  Practice examples Source: D Lowe Original Filtered (sharpening) txp_fig -  Other application −−−  Possible exam question Original image filter Result Assume we ignore the borders What are the values in the x matrix?  Possible exam question Original image filter Result x x x x x x x x x  Errata on convolutions Mathematically, we have been performing cross - correlation: For a convolution, we flip the filter horizontally and vertically first Only matters for asymmetrical filters and is irrelevant in convolutional networks, since we learn the filter weights either way i h g f e d c b a a b c d e f g h i * =  BREAK  The convolutional network Cow A convolutional network is a network with convolutional layers A convolutional layer convolves the input “image” with X filters to generate X output “images”  The convolutional layer One output value is the result of a convolution with an input area and a convolution filter The convolution is local in width and height, full in depth Convolutional networks on images always have a third dimension! Screen Shot -- at png  The convolutional layer Each filter yields a different output value for the same location We stack the outputs in the depth dimension of the new layer depth_columnpng  The convolutional layer Q: What does the mean? RGB Q: What is the output size for D filters (with padding)? xxD Q: Does the output size depend on the input size or the filter size? Input size depth_volumepng  The convolutional layer The outputs of a single filter over all spatial locations is called a filter map Can be interpreted as a “transformed” output image Screen Shot -- at png  Filters and layer dimensionality First layer: Input: xx and A filters of size xx Output: xxA Second layer: Input: xxA and B filters of size xx[ ? ] Output size: xx[ ? ]  Pooling All convolutional networks perform pooling, but why? Reduces amount of computations Invariance to small deformations What are ways to perform pooling? Convolutions with stride Average pooling Max pooling https://halinriafr/hal-/file/recur_leftpng  Max pooling Forward: Maintain highest activation Backward: Gradient flow through highest activation imax,jmax=argmax 𝑖,𝑗∈Ω(𝑟,𝑐) 𝑥𝑖𝑗→𝑎𝑟𝑐 𝜕𝑎𝑟𝑐 𝜕𝑥𝑖𝑗 =ቊ ,𝑖𝑓𝑖=imax,j=jmax,𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒  The convolutional network Cow  Learn local filters at each layer  Weights of local filters are shared over spatial locations  Each convolutional layer outputs multiple filter maps  Downsize filter maps through pooling  Final layers either standard network layers or global pooling  What does each layer learn?  What does each layer learn?  What does each layer learn? From low to high semantics over the layers  Architecture design - LeNet  Architecture design - AlexNet First to successfully perform large - scale GPU training of images Winner of the ImageNet challenge, a true breakthrough layers: convolutional layers and fully - connected layers  Architecture design – VGG and GoogLeNet A focus on deeper networks, with more layers and more modules Typically to layers, with many parameters  Architecture design – ResNet and DenseNet A new key observation: deeper networks to not obtain better results Proposed solution: add skip - connections and learn residuals Key component of deep networks to this date  Convolutional networks beyond classification Object segmentation [He ] Pose estimation [Cao ] Reinforcement learning from visual input  This lecture The convolution operation Convolutional layers Convolutional networks  What’s next Lab deadlines: Nov th : lab Nov th : lab Next lecture: Representing text - : - : Turingzaal  Thank you "
applied machine learning lecture 8,applied machine learning,AML,slides,conv classification image learning output objects box actor rnns feature,step cnn regression prototypes networks input hyperspherical recurrent estimation fast,tanh object action prototype deep state spatial detection localization network,,2329,85,Pascal Mettes,2019,"Applied Machine Learning lecture  Deep learning II Pascal Mettes – University of Amsterdam  This lecture Deep object detection Per pixel segmentation Recurrent networks Machine learning research examples  Overall schedule Math tutorial Lab   Intro and Regression I Regression II Lab Intro   Classification I Classification II Math tutorial  Lab Assignment    Deep Learning I Representing Images Lab Assignment    Representing Text Deep Learning II Math tutorial  Peer feedback open questions Kaggle - Project week    Setting up a ML system Intro to Reinforcement Learning Recommender Systems Kaggle - Project week    AI for industry and humanities Autonomous driving Math tutorial  Kaggle - Project week    Q&A Kaggle - Project week    Exam Poster/Demo Session  Story so far Dog? Cat? Bike? Car? Plane? Image result for plane How popular will this movie be in IMDB? Image result for matrix  Beyond single value prediction Not all tasks only need a single output What about: • Localization? • Per pixel outputs? • Segmentation, depth estimation, etc • Sequential predictions?  Object detection Predict bounding box around each object of interest in a dataset Each image can have any number of objects, which can be small or even (partially) occluded http://wwwsmaragdamsterdamnl/media//brouwerij_t_ijjpg http://wwwmotorcycleaccidentlawyer-dccom/wp-content/uploads///pedestriansjpg  Deep Learning for Detection: R - CNN Basic idea follows pipeline of “shallow” object detectors of that time For each image, precompute object proposals Feed proposals to pre - trained deep network, rather than fixed features [ Girshick et al CVPR, ]  Backbone of R - CNN: Selective search [ Uijlings et al IJCV, ]  Deep Learning for Detection: Fast R - CNN Making object proposal features part of the network Image result for nascar Conv  Conv  Conv  Conv  Conv  Conv  feature map [ Girshick ICCV, ]  Fast R - CNN: Steps Process the whole image up to conv Compute possible locations for objects Image result for nascar Conv  Conv  Conv  Conv  Conv  Conv  feature map  Fast R - CNN: Steps Process the whole image up to conv Compute possible locations for objects • some correct, most wrong Image result for nascar Conv  Conv  Conv  Conv  Conv  Conv  feature map  Fast R - CNN: Steps Given single location: ROI pooling module extracts fixed length feature Image result for nascar Conv  Conv  Conv  Conv  Conv  Conv  feature map Car/dog/bicycle New box coordinates  ROI pooling Always x no matter the size of candidate location Divide cell area into fixed sized bins  Some results  Fast R - CNN Reuse convolutions for different candidate boxes • Compute feature maps only once Region - of - Interest pooling • From variable box area to fixed areas and fixed representations End - to - end training! (Very) Accurate object detection (Very) Faster External box proposals needed http://notthatmiketheothermikecom/wp-content/uploads/_aeddeaa_zjpg T=  Deep Learning for Detection: Faster R - CNN Region Proposal Network Rather than using pre - defined object proposals, use sliding window over feature maps [Ren et al NeurIPS , ]  Region Proposal Network  Per - pixel predictions Many tasks require detailed prediction all the way at the pixel level Can you name a number of such tasks?  Object segmentation Image result for object segmentation  Optical flow & motion estimation Image result Image result  Depth estimation Godard et al, Unsupervised Monocular Depth Estimation with Left - Right Consistency,  https://pbstwimgcom/media/CstrRLWEAAWhHdjpg  https://ai-s-publicsamazonawscom/figures/--/ceeadccddfd/-Figure-png http://homescswashingtonedu/~krematas/DRM/Teaserpng Normal and reflectance estimation  Fully - convolutional networks [Long et al CVPR, ]  Fully - convolutional networks Two main differences: • Deconvolution to obtain original image size • Loss at pixel - level, instead of a global loss Per - pixel loss means, compute eg squared loss per pixel and backpropagate from all output pixels back to the network  Deconvolution  Object segmentation: Mask R - CNN State - of - the - art in semantic segmentation Heavily relies on Fast R - CNN Can work with different architectures, also ResNet Runs at ms per image on an Nvidia Tesla M GPU Can also be used for Human Pose Estimation  Mask R - CNN: Faster R - CNN +  layers https://cdn-images-mediumcom/max//*QaPtdVXIeIdFhbWActNApng [He et al ICCV, ]  Mask R - CNN: ROI Align https://cdn-images-mediumcom/max//*envHrpgpnfLiQKJOKApng https://cdn-images-mediumcom/max//*ZVjktBNyTrrbn_DpgzWqwpng  Mask R - CNN https://cdn-images-mediumcom/max//*M_ZhHpOXzWxEsfWueEApng  Mask R - CNN https://cdn-images-mediumcom/max//*ezVbcdLgitWjNz-QOwpng https://cdn-images-mediumcom/max//*AmaTWLiSiAOWpgpng https://cdn-images-mediumcom/max//*yw_-UOCOYLiJFzUximaJQpng https://cdn-images-mediumcom/max//*JtWbxWaFtJfTJOvDrGsApng  Sequence prediction in vision Many non - standard tasks are enabled with the use of RNNs Recurrent connections NN Cell State Input Output  Layout of Recurrent Networks Sequence inputs: we model them with parameters 𝑈 Sequence outputs: we model them with parameters 𝑉 Memory I/O: we model it with parameters 𝑊 𝑥𝑡 𝑦𝑡 𝑈 𝑉 𝑊 Output parameters Input parameters Memory parameters Input Output 𝑐𝑡 𝑥𝑡+ 𝑦𝑡+ 𝑈 𝑉 𝑊 𝑥𝑡+ 𝑦𝑡+ 𝑈 𝑉 𝑊 𝑥𝑡+ 𝑦𝑡+𝑛 𝑈 𝑉 𝑊 𝑐𝑡+ 𝑐𝑡+ 𝑐𝑡+𝑛  RNNs vs MLPs Is there a big difference? Outputs at every step (MLP outputs in every layer also possible) Instead of layer - specific parameters, we have layer - shared parameters 𝑦 𝑦 𝑦 𝐿𝑎𝑦𝑒𝑟 𝐿𝑎𝑦𝑒𝑟 𝐿𝑎𝑦𝑒𝑟  - gram Unrolled Recurrent Network  - layer Neural Network Final output 𝑈 𝑉 𝑊 𝑈 𝑉 𝑊 𝑈 𝑉 𝑊 𝑊 𝑊 𝑦 “Layer/Step”  “Layer/Step”  “Layer/Step”  𝑊 𝑥 𝑥  The problem with RNNs As RNNs get deeper, they suffer from exploding/vanishing gradients Especially vanishing gradients are a problem • RNNs tend to focus on short - term transitions, ignoring long - term ones https://cdn-images-mediumcom/max//*FWySTspkMYdLifG_Qpng  Long - Short Term Networks + 𝜎 𝜎 𝜎 tanh tanh Input Output 𝑓𝑡 𝑐𝑡− 𝑐𝑡 𝑜𝑡 𝑖𝑡 ෥ 𝑐𝑡 𝑚𝑡 𝑚𝑡− 𝑥𝑡  LSTMs LSTMs tackle multiple issues in RNNs • At each RNN step the hidden state is overwritten • Not all inputs are important enough to be written down • Not all outputs are important enough to be written • Not all information is important enough to be remembered  LSTMs vs RNNs RNNs 𝑐𝑡=𝑊⋅tanh(𝑐𝑡−)+𝑈⋅𝑥𝑡+𝑏 LSTMs 𝑖=𝜎𝑥𝑡𝑈(𝑖)+𝑚𝑡−𝑊(𝑖) 𝑓=𝜎𝑥𝑡𝑈(𝑓)+𝑚𝑡−𝑊(𝑓) 𝑜=𝜎𝑥𝑡𝑈(𝑜)+𝑚𝑡−𝑊(𝑜) ෥ 𝑐𝑡=tanh(𝑥𝑡𝑈𝑔+𝑚𝑡−𝑊(𝑔)) 𝑐𝑡=𝑐𝑡−⊙𝑓+෥ 𝑐𝑡⊙𝑖 𝑚𝑡=tanh𝑐𝑡⊙𝑜 + 𝜎 𝜎 𝜎 tanh tanh Input Output 𝑓𝑡 𝑐𝑡− 𝑐𝑡 𝑜𝑡 𝑖𝑡 ෥ 𝑐𝑡 𝑚𝑡 𝑚𝑡− 𝑥𝑡  LSTM: step - by - step 𝑖=𝜎𝑥𝑡𝑈(𝑖)+𝑚𝑡−𝑊(𝑖) 𝑓=𝜎𝑥𝑡𝑈(𝑓)+𝑚𝑡−𝑊(𝑓) 𝑜=𝜎𝑥𝑡𝑈(𝑜)+𝑚𝑡−𝑊(𝑜) ෥ 𝑐𝑡=tanh(𝑥𝑡𝑈𝑔+𝑚𝑡−𝑊(𝑔)) 𝑐𝑡=𝑐𝑡−⊙𝑓+෥ 𝑐𝑡⊙𝑖 𝑚𝑡=tanh𝑐𝑡⊙𝑜 Cell state line + 𝜎 𝜎 𝜎 tanh tanh 𝑓𝑡 𝑐𝑡− 𝑐𝑡 𝑜𝑡 𝑖𝑡 ෥ 𝑐𝑡 𝑚𝑡 𝑚𝑡− 𝑥𝑡  LSTM: step - by - step 𝑖=𝜎𝑥𝑡𝑈(𝑖)+𝑚𝑡−𝑊(𝑖) 𝑓=𝜎𝑥𝑡𝑈(𝑓)+𝑚𝑡−𝑊(𝑓) 𝑜=𝜎𝑥𝑡𝑈(𝑜)+𝑚𝑡−𝑊(𝑜) ෥ 𝑐𝑡=tanh(𝑥𝑡𝑈𝑔+𝑚𝑡−𝑊(𝑔)) 𝑐𝑡=𝑐𝑡−⊙𝑓+෥ 𝑐𝑡⊙𝑖 𝑚𝑡=tanh𝑐𝑡⊙𝑜 𝜎∈(,): control gate – something like a switch t anh ∈−,: recurrent nonlinearity + 𝜎 𝜎 𝜎 tanh tanh 𝑓𝑡 𝑐𝑡− 𝑐𝑡 𝑜𝑡 𝑖𝑡 ෥ 𝑐𝑡 𝑚𝑡 𝑚𝑡− 𝑥𝑡  LSTM: step - by - step # 𝑖=𝜎𝑥𝑡𝑈(𝑖)+𝑚𝑡−𝑊(𝑖) 𝑓=𝜎𝑥𝑡𝑈(𝑓)+𝑚𝑡−𝑊(𝑓) 𝑜=𝜎𝑥𝑡𝑈(𝑜)+𝑚𝑡−𝑊(𝑜) ෥ 𝑐𝑡=tanh(𝑥𝑡𝑈𝑔+𝑚𝑡−𝑊(𝑔)) 𝑐𝑡=𝑐𝑡−⊙𝑓+෥ 𝑐𝑡⊙𝑖 𝑚𝑡=tanh𝑐𝑡⊙𝑜 + 𝜎 𝜎 𝜎 tanh tanh 𝑓𝑡 𝑐𝑡− 𝑐𝑡 𝑜𝑡 𝑖𝑡 ෥ 𝑐𝑡 𝑚𝑡 𝑚𝑡− 𝑥𝑡  LSTM: step - by - step # 𝑖=𝜎𝑥𝑡𝑈(𝑖)+𝑚𝑡−𝑊(𝑖) 𝑓=𝜎𝑥𝑡𝑈(𝑓)+𝑚𝑡−𝑊(𝑓) 𝑜=𝜎𝑥𝑡𝑈(𝑜)+𝑚𝑡−𝑊(𝑜) ෥ 𝑐𝑡=tanh(𝑥𝑡𝑈𝑔+𝑚𝑡−𝑊(𝑔)) 𝑐𝑡=𝑐𝑡−⊙𝑓+෥ 𝑐𝑡⊙𝑖 𝑚𝑡=tanh𝑐𝑡⊙𝑜 Decide what new information is relevant from the new input and should be added to the new memory Modulate the input 𝑖𝑡 Generate candidate memories ෥ 𝑐𝑡 + 𝜎 𝜎 𝜎 tanh tanh 𝑓𝑡 𝑐𝑡− 𝑐𝑡 𝑜𝑡 𝑖𝑡 ෥ 𝑐𝑡 𝑚𝑡 𝑚𝑡− 𝑥𝑡  LSTM: step - by - step # 𝑖=𝜎𝑥𝑡𝑈(𝑖)+𝑚𝑡−𝑊(𝑖) 𝑓=𝜎𝑥𝑡𝑈(𝑓)+𝑚𝑡−𝑊(𝑓) 𝑜=𝜎𝑥𝑡𝑈(𝑜)+𝑚𝑡−𝑊(𝑜) ෥ 𝑐𝑡=tanh(𝑥𝑡𝑈𝑔+𝑚𝑡−𝑊(𝑔)) 𝑐𝑡=𝑐𝑡−⊙𝑓+෥ 𝑐𝑡⊙𝑖 𝑚𝑡=tanh𝑐𝑡⊙𝑜 Compute and update the current cell state 𝑐𝑡 Depends on the previous cell state What we decide to forget What inputs we allow The candidate memories + 𝜎 𝜎 𝜎 tanh tanh 𝑓𝑡 𝑐𝑡− 𝑐𝑡 𝑜𝑡 𝑖𝑡 ෥ 𝑐𝑡 𝑚𝑡 𝑚𝑡− 𝑥𝑡  LSTM: step - by - step # 𝑖=𝜎𝑥𝑡𝑈(𝑖)+𝑚𝑡−𝑊(𝑖) 𝑓=𝜎𝑥𝑡𝑈(𝑓)+𝑚𝑡−𝑊(𝑓) 𝑜=𝜎𝑥𝑡𝑈(𝑜)+𝑚𝑡−𝑊(𝑜) ෥ 𝑐𝑡=tanh(𝑥𝑡𝑈𝑔+𝑚𝑡−𝑊(𝑔)) 𝑐𝑡=𝑐𝑡−⊙𝑓+෥ 𝑐𝑡⊙𝑖 𝑚𝑡=tanh𝑐𝑡⊙𝑜 Modulate the output New cell state relevant?  Sigmoid  If not  Sigmoid  Generate the new memory + 𝜎 𝜎 𝜎 tanh tanh 𝑓𝑡 𝑐𝑡− 𝑐𝑡 𝑜𝑡 𝑖𝑡 ෥ 𝑐𝑡 𝑚𝑡 𝑥𝑡  LSTMs in computer vision  Break  What is my research like? https://lhgoogleusercontentcom/TRUqTVGSfLpJeCcXCtNVvuNITczcfJuDpXEAKTCDZNtGOFRMTHLdXAUvchzz-lYtAaWXIWFUNLoBKaCxLykC-lK_iygfJfUbkqtogQOXpgj https://lhgoogleusercontentcom/ONcxIKZhoJLeMyQfTylTAoJhjPHZGHGWMumjZGINei_RbcZBkdAsizvQoQRrfragzFzPKTjPgZdkLMVUYgJqJPWqYxkWUJSMTQYZiuqGjmwJQLrYqyZb https://lhgoogleusercontentcom/ikzhaCadnwzXKWGIiJCQaRImtUziQJTWhaxMigYPxadpJXnGGmCTtUVmPzGtCK_ajYgxxwXkluGelGqSzjxKtxnCZyWQAlmVtCr-dijjzScX https://lhgoogleusercontentcom/R-uTcGosuDcqk-VTpMDKkEmvmcxblzXAuDNQjnbQOczQrCJeiRwQsWRAFjxUb-hWQwXNPYGdFZFLRacnTnqtzYAwLFDMwG_SVm-HpMvubzaaIpP_Gmg https://lhgoogleusercontentcom/tcE-IaNNLGtwVRazhBTPPdEAmNLXfRJgQf-sTKkxHHGpqaRdvljLtkYYDMKo-YlQmCKOpVXMBEDNbDyDSXJkCwdwnSrsFyiBlmFYSJuyEWBNTwM-A  Action localization without examples The only input we need is a text query https://lhgoogleusercontentcom/PCqumaHuRyJlWxSQbJGVNnLRHCPyYHHvnDAyjjcaSRvTZFmypLaemRLHWCDCIafwHcLOtYlyJmPhvfPOJNrYwQIQuVgrmdynBTprLywk_THGScrrfXXA https://lhgoogleusercontentcom/TRUqTVGSfLpJeCcXCtNVvuNITczcfJuDpXEAKTCDZNtGOFRMTHLdXAUvchzz-lYtAaWXIWFUNLoBKaCxLykC-lK_iygfJfUbkqtogQOXpgj [Mettes and Snoek, ICCV ]  Spatial - aware object embeddings  The higher the likelihood of an actor in a box, the higher the overall action score Step : Actor detection box fram e actio n  Detect objects that are relevant for the action Use word embedding for action relevance Step : Object detection box frame action set of objects neighbourhood  Find out where objects are relative to actors Quantify spatial relations into D grids Step : Spatial awareness Bicycle Traffic light Skateboard object actor box object box  Jointly score actors, relevant objects, and spatial awareness Scoring action boxes Localized action Actor likelihood Relevant objects Spatial relations  Jointly score actors, relevant objects, and spatial awareness Scoring action boxes [ Gkioxari and Malik, CVPR ] Localized action Actor likelihood Relevant objects Spatial relations Boxes are linked into tubes through dynamic programming  Global objects help to discriminate tubes from different videos Local and global object interaction Global objects Referee Goal Grass Commercial signs  Experiments UCF Sports [Lan et al ICCV ] UCF -  [Soomro et al ] J - HMDB [Jhuang et al ICCV ] HollywoodTubes [Mettes et al ECCV ] Faster R - CNN [Ren et al NIPS ] wordvec [Mikolov et al NIPS ] [Jain et al ICCV ] MS - COCO [Min et al ECCV ] Datasets Implementation details  Properties Object and spatial awareness improve localization and classification UCF Sports  Qualitative analysis: success Skateboarding ( skateboard ) Riding horse ( horse ) Kicking ( tie ) Bar swinging ( table ) Actions localized through match with objects and spatial relation  Qualitative analysis: failure Skateboarding ( skateboard ) Riding horse ( horse ) Kicking ( tie ) Wrong actor located or ambiguity between actions and objects Bar swinging ( table )  Enabling zero - shot action localization retrieval Backpack ON actor Zero - shot action retrieval with arbitrary object and spatial relations  Enabling zero - shot action localization retrieval Sports ball (  ) RIGHT OF actor Sports ball (  ) RIGHT OF actor Naturally incorporates object size in query  Action localization comparison We outperform state - of - the - art zero - shot action localization work  Action localization comparison Even close to some supervised approaches  State - of - the - art evaluation Classification (UCF - ) Localization  Hyperspherical prototype networks Deep learning on hyperspherical output spaces [Mettes et al, NeurIPS ]  Status - quo in deep classification and regression Hugo van der Goes  softmax cross - entropy squared error [He et al CVPR ]  Limits of the status - quo Hugo van der Goes One dimension for each class So ,, dimensions for ,, classes Fully parametric; many inductive biases and priors ignored Knowledge about classes prior to learning gets you nowhere Classification  One dimension in total A lot of information pressed into a single number Completely different loss and optimization as for classification Difficult to even learn these tasks at the same time, gradient scales differ order of magnitude Regression  Inductive bias Large margin separation Inductive bias Occam’s Razor  Inductive bias Prior knowledge Cat Tiger Airplane  Hyperspherical Prototype classification: main idea Each class is a prototype (point) on the output hypersphere Prototypes are placed with maximal separation on hypersphere prior to training For training, update examples angularly towards prototypes  Hyperspherical Prototype classification Given pre - computed prototypes, we define the following loss: Number of training examples Network output Class prototype  Hyperspherical Prototype classification Backpropagate partial derivative wrt training examples:  Hyperspherical Prototype classification During inference, assign label to class with highest similarity:  Hyperspherical Prototype classification - PyTorch Assign training examples to their class prototypes Use nnCosineSimilarity () for loss and update loss function  Obtaining hyperspherical prototypes PML Tammes (): Pores on pollen grains seem to be uniformly distributed, regardless of the size of the grains or the number of pores How do they do it? Distributing points on a (hyper)sphere is an open mathematical problem to this day  Obtaining hyperspherical prototypes We approximate separation by optimizing the following objective: We extend the objective with a loss that preserves class semantics: Binary label:  if j closer i than k (semantically) Sigmoid of cosine difference of prototypes  We perform continuous hyperspherical learning as: Optimization same as for classification Hyperspherical Prototype Regression Normalized position between upper and lower bound  Joint classification and regression We can now jointly optimize both tasks simultaneously:  in the same space,  without the need to tune the scaling for the gradient backpropagation Regression: z axis Classification: x - y plane  Embedding in related work Prototype networks ✅Clearly structured output space ⛔️Chicken-egg problem ⛔️Constant prototype updating [Snell et al NeurIPS ] Classification with angular separation ✅Improved margins for classification ⛔️Still in softmax cross-entropy shackle ⛔️No application for regression [Liu et al ICML ]  Evaluating hyperspherical prototypes Akin to softmax cross - entropy Prototypes from word embeddings Our prototypes outperform baselines, especially with small outputs  Evaluating prototypes with privileged information The smaller the output, the bigger the benefit of privileged information  Comparative evaluation Faster, more stable, and better than state - of - the - art prototype network Better than softmax cross-entropy when examples are scarce or uneven  Evaluating hyperspherical regression More effective regression than standard mean squared error [ Strezoski et al TMM ]      Evaluating joint classification and regression Proof of concept Jointly predict MNIST digit (classification) and rotation (regression) Quantitative evaluation Multi-task learning of creation year (regression) and art style (classification)  This lecture Deep object detection Per pixel segmentation Recurrent networks Machine learning research examples  Thank you "
applied machine learning lecture 9,applied machine learning,AML,slides,video shot data precision machine err example one accuracy total,shot cnn yes roc train relevant items success evaluation task results,learning validation query test recall curve set measure hyperparameters fold,,741,24,Pascal Mettes,2019,"Applied Machine Learning lecture  Setting up a machine learning system Pascal Mettes – University of Amsterdam  Agenda of the day Setting hyperparameters Evaluating performance Reinforcement earning  Hyperparameters Hyperparameters are parameters that can not be directly learned from data and need to be set manually No way to estimate gamma from data, we have to set it  The one illegal thing to do Never use test set and labels to determine best settings The fields of data science and machine learning are littered with people violating this, both in industry and academia You as future data scientists are responsible for maintaining machine learning ethics and standards!  Validation Take out of portion of training set to be used for evaluation Pro and con of such a setup? + Fair way to determine desirable hyperparameter values - Data data data  Cross - validation Split data into portions, rotated evaluation on different holdout sets Train Test Train Test Train Test Train Video  Shot Video  Shot Video  Shot Video  Shot … Video  Shot Video  Shot Video  Shot Video  Shot … Video  Shot Video  Shot Video  Shot … Fold Fold Fold Video  Shot Video  Shot Video  Shot Video  Shot Video  Shot Video  Shot k= k= k= Err f_= err= err= Avg err= Err f_= Err f_=  What out for specific task in validation What is wrong with the setup to the right? Same video in different splits Solution: Constrain shots from same video to be in a single split Cross-Validation is task specficVideo  shot Video  shot Video  shot Video  shot Video  shot Video  shot Video  shot Video  shot Video  shot Video  shot TrainTestTraditional Cross-Validation:  Validation versus cross - validation Cross - validation preferred when data is scarce and models are of low complexity Validation preferred when data is sufficient and/or models are complex  Machine learning system World Data Learning Model Training data Test data Evaluation Features Labels Optimization Task specific Objective Function  Evaluating a machine learning system Technical How fast does it train How fast is the prediction What is cost/query? Memory size … User happiness UI design Cost … Relevance  Relevance: depends on the user Web search engine: searcher Success: Searcher finds what she was looking for Measure: rate of return to this search engine Web search engine: advertiser Success: Searcher clicks on ad Measure: clickthrough rate Ecommerce: buyer Success: Buyer buys something Measures: fraction of “conversions” of searchers to buyers Ecommerce: seller Success: Seller sells something Measure: profit per item sold  Classification accuracy Accuracy = fraction of correct decisions When is accuracy a bad measure? Highly imbalanced data When % of the data is negative, % accuracy can be achieved simply by assigning  to all test examples png  Which is better? Query A Query B  Recall and precision Recall: # relevant items predicted / total # relevant items in collection Precision: # relevant items predicted / total # items predicted  Recall and precision  Which one has a high precision? Query A Query B  Which one has a high precision? Query A Query B  Which one has a higher recall? Only known if total number of relevant items is known! Query A Query B  Average precision  Sort results by score (from high to low)  Start at the top ranked example and move downwards  Every time you encounter a positive example (recall goes up), compute the precision at that location  Sum the precisions of all encounters  Divide by total number of relevant documents  Example Example right: ordering already given AP = ((/ * ) + (/ * ) + (/ * ) + (/ * ) + (/ * ) / ) or AP = (/ + / + /) /  fr fr shot___CNN_ shot___CNN_ shot___CNN_      yes yes yes no no Results AP  ROC curves Computing an ROC Curve ROC curves Area under the ROC Curve (AUC) (++++)/ = Area Under the Curve is an estimator of correct prediction probability  ROC curve example What is the area under the ROC curve for this example?  /  fr fr shot___CNN_ shot___CNN_ shot___CNN_      yes yes yes no no Results  Break "
applied machine learning Introduction to Reinforcement Learning,applied machine learning,AML,slides,applied action goal move optimal supervised given future formalization model,learning machine reward state learn take reinforcement function possible total,problem decision sequential policy problems environment long-term immediate solve time,,1038,30,Shihan Wang,2019,"Applied Machine Learning Introduction to Reinforcement Learning University of Amsterdam Lecturer of today: Shihan Wang  Lecturer PhD in Computer Science @ Tokyo Institute of Technology, Japan PostDoc researcher @ UvA, machine learning lab Research topics: Applied Machine Learning, Data Mining, Human Centered Computing, Social Network Analysis One recent research project: apply reinforcement learning to promote physical activity using intelligent mobile systems -- Applied Machine Learning   RecapAppliedMachineLearning---RecapAppliedMachineLearning---  What is Reinforcement Learning to you? -Have you heard about RL? -Can you think of some scenarios where RL are applied for? -- Applied Machine Learning   Examples https://wwwyoutubecom/w atch?v=wOzwjb https://wwwyoutubecom/wat ch?v=tqCspV_g http://helistanfordedu/ -- Applied Machine Learning  https://waymocom/  What is RL? • Focus on the sequential decision making problems: How to sequentially interact with an environment to maximize a long-term objective? Key features: -RL learns the best action to take by trial and error -The reward of RL can be delayed It might sacrifice immediate reward to gain more long-term reward -- Applied Machine Learning   Possible sequential interactions? https://wwwyoutubecom/w atch?v=wOzwjb https://wwwyoutubecom/wat ch?v=tqCspV_g http://helistanfordedu/ -- Applied Machine Learning  https://waymocom/  Reinforcement Learning VS Supervised Learning • Given the current input, you • The decisions you make do not make a decision, and the next VS affect what you see in the future input depends on your decision • Learn to do: • Learn to predict: -Learn sequentially over human response -Learn with human responses (labels) -- Applied Machine Learning   Summary of the lecture • What is RL? -Learn from sequential interactions with environment to optimize a long-term objective -RL and supervised learning deal with different ML problem • How to solve a RL problem? -- Applied Machine Learning   A typicalRLtask:robot in mazeA typicalRLtask:robot in maze What is the shortest route to achieve the goal? -- An example   Two Steps to Solve a RL Problem • Formalization: -conceptualize a real-world problem into the appropriatemathematical formation • Finding a solution: -model the learning goal; -use the appropriate method to achieve this goal -- Applied Machine Learning -  AppliedMachineLearning- Formalization: Markov Decision Process • A mathematical framework to model sequential decision making tasks, commonly used for formalizing many RL problems • Sequential interaction flow: --  Can you find the elements? What is the agent? What is the environment? What are possible states? What are possible actions? What is the reward? --   The interaction flow --   At each time step - state s action a reward r state s --   Markov Decision Process Formally, a finite MDP consists of: • A finite set of states: S • A finite set of actions for each state: A • A dynamic function:  p (s ,r|s, a)=Pr{St =s ,Rt =r|St  =s, At  =a} sometimes written as transition function: p (s |s, a) reward function: p(r|s, a, s ) • A discount factor: 𝛾∈[,] -- Applied Machine Learning   Assumptions of MDPAssumptions of MDP Important assumptions: • Discrete time steps • The next state only depends on the current state and action • Rewards depend only on the state, action and next state -- Applied Machine Learning   Two Steps to Solve a RL Problem • Formalization of the problem: -Conceptualize a real-world problem into the appropriatemathematical formation • Find a solution: -Identify the learning goal; -Use the appropriate method to achieve this goal MarkovDecisionProcess -- Applied Machine Learning -  Policy Policyrepresentstheagent’sbehavior:atacertainstate,whataction should be taken • • It is a mapping from state to action: 𝜋 𝑎 𝑠)=Pr 𝐴𝑡=𝑎 𝑆𝑡=𝑠} What is the route towards the final goal with maximum rewards? o What action the agent should take at each time step? o What is the optimal policy at each state? -- Applied Machine Learning   Which policy is better? Move right: total reward - =  Given:  possible actions: Immediate reward: -Take any action at each step 𝑅𝑡=− -Reach the final goal 𝑅𝑡= Move down: total reward  Question: what action should this robot take? Why? --   Formally measure how well a policy is Value function: • It is a expectation of future reward • At the state s, if take an action a, what future reward can be expected #  X v⇡(s)=E⇡ [Gt|St =s]=E⇡ kRt+k+|St =s k=  X discounted return: Gt = Rt+ + Rt+ + Rt+ + ···= kRt+k+ k= -- Applied Machine Learning   Return Gt Move right: total reward  Given:  Take the action of ‘move right’ : 𝐺=𝑅+𝛾𝑅+𝛾𝑅++𝛾𝑅=?  Take the action of ‘move down’ : Move down: total reward  𝐺=𝑅+𝛾𝑅+𝛾𝑅++𝛾𝑅=? • The immediate reward of each action: 𝑅𝑡=− • The immediate reward of reaching the final goal 𝑅𝑡= • The discount factor: 𝛾= --   Summary of the lecture • What is RL? -Learn from sequential interactions with environment to optimize a long-term goal -RL and supervised learning deal with different ML problem • How to solve a RL problem? -Model a practical RL problem into Markov decision process (identify the interaction flow and key elements) -Find the optimal policy for the learning goal (measure different policies based on their value function, select the one with maximum expected return) -- Applied Machine Learning   How to learn optimal policies REINFORCE, etc --Monte Carlo, Q-learning, Sarsa, etc *Thanks to Herke van Hoof  How to learn optimal policies Model-free RL --Monte Carlo, Q-learning, Sarsa, etc *Thanks to Herke van Hoof  Not all RL problems = MDPs • Which ones are not? • For example: -multi-arm bandit problems (https://banditalgscom/) -- Applied Machine Learning   RL is not perfect • Long learning time: lots of data • Uncertainty in practice: high variance -- Applied Machine Learning   Reading materials • Reinforcement learning: An Introduction R S Sutton & AG Barto -nd edition (available online for free http://incompleteideasnet/book/bookdraftjanpdf) -Contents in Chapter ,  –  • Learn more about RL? ii -Reinforcement learning course in Master AI program by Herke van Hoof -- Applied Machine Learning   Questions? -Questions for this lecture -More questions about RL: email to swwang@uvanl -- Applied Machine Learning   Thank you -- Applied Machine Learning  "
A Few Useful Things to Know about Machine Learning,applied machine learning,AML,paper,machine learning algorithms examples cost-effective ambitious fields however developing amount,generalizing feasible programming data becomes problems tackled widely used computer, important learning figure perform tasks ten manual available result science,,101,9,Pedro Domingos,2012,"Machine learning algorithms can figure out how to perform important tasks by generalizing from examples This is of- ten feasible and cost-effective where manual programming is not As more data becomes available, more ambitious problems can be tackled As a result, machine learning is widely used in computer science and other fields However, developing successful machine learning applications requires a substantial amount of “black art” that is hard to find in textbooks This article summarizes twelve key lessons that machine learning researchers and practitioners have learned These include pitfalls to avoid, important issues to focus on, and answers to common questions"
Food/Non-food Image Classification and Food Categorization using Pre-Trained GoogLeNet Model,applied machine learning,AML,paper,recognition assessment classification food/non-food past developments boon specifcally variety paper,food image deep convolutional neural experiments datasets recent seen lot,dietary image-based crucial last couple advancements learning classifcation wide items,"Caffe, convolutional neural network (CNN), food/non-food classication, food recognition, deep learning, GoogLeNet",140,9,"Singla, Ashutosh, Lin Yuan, Touradj Ebrahimi",2016,"Recent past has seen a lot of developments in the field of image-based dietary assessment Food image classification and recognition are crucial steps for dietary assessment In the last couple of years, advancements in the deep learning and convolutional neural networks proved to be a boon for the image classifcation and recognition tasks, specifcally for food recognition because of the wide variety of food items In this paper, we report experiments on food/non-food classi fication and food recognition using a GoogLeNet model based on deep convolutional neural network The experiments were conducted on two image datasets created by our own, where the images were collected from existing image datasets, social media, and imaging devices such as smart phone and wearable cameras Experimental results show a high accuracy of % on the food/non-food classification and % on the food category recognition"
Homebrew Databases: Complexities of Everyday Information Management in Nonprofit Organizations,information organization,IO,paper,information volunteer coordinators identify people assortment lives volunteers yet insufficient,management databases manage homebrew complex organizations exception collectively millions every,systems needs many digital information nonprofit current paper present study,"Homebrew databases, information management, nonprofit, NPO, volunteer coordination, volunteer management.",130,10,"Amy Voida, Ellie Harmon & Ban Al-Ani",2011,"Many people manage a complex assortment of digital information in their lives Volunteer coordinators at nonprofit organizations are no exception; they collectively manage information about millions of volunteers every year Yet current information management systems are insufficient for their needs In this paper, we present results of a qualitative study of the information management practices of volunteer coordinators We identify the resource constraints and the diverse and fluid information needs, stakeholders, and work contexts that motivate their information management strategies We characterize the assemblages of information systems that volunteer coordinators have created to satisfice their needs as ‘homebrew databases’ Finally, we identify additional information management challenges that result from the use of these ‘homebrew databases,’ highlighting deficiencies in the appropriateness and usability of databases and information management systems, more generally"
"Types of Personal Information Categorization: Rigid, Fuzzy, and Flexible",information organization,IO,paper,personal different based data identify styles collect analyze types rigid,information categorizers categorization analysis aims mindscape diary semistructured conducted participants,study digital data questionnaire interviews types iii provides understand mindscapes,,168,14,Kyong Eun Oh,2017,"This study aims to identify different styles of personal digital information categorization based on the mindscape of the categorizers To collect data, a questionnaire, a diary study, and  semistructured interviews were conducted with each of  participants Then a content analysis was used to analyze the data Based on the analysis of the data, this study identified  different types of categorizers: (i) rigid categorizers, (ii) fuzzy categorizers, and (iii) flexible categorizers This study provides a unique way to understand personal information categorization by showing how it reflects the mindscapes of the categorizers In particular, this study explains why people organize their personal information differently and have different tendencies in developing and maintaining their organizational structures The findings provide insights on different ways of categorizing personal information and deepen our knowledge of categorization, personal information management, and information behavior In practice, understanding different types of personal digital information categorization can make contributions to the development of systems, tools, and applications that support effective personal digital information categorization"
The wisdom hierarchy: representations of the DIKW hierarchy,information organization,IO,paper,wisdom definitions terms consensus processes revisits number variously pyramid one,information hierarchy data management articulation widely systems paper data–information–knowledge–wisdom dikw,knowledge textbooks literatures limited discussion read analysing nature models implicitly,"DIKW hierarchy, wisdom hierarchy, wisdom, knowledge management, wisdom management",237,19,Jennifer Rowley,2006,"This paper revisits the data–information–knowledge–wisdom (DIKW) hierarchy by examining the articulation of the hierarchy in a number of widely read textbooks, and analysing their statements about the nature of data, information, knowledge, and wisdom The hierarchy referred to variously as the ‘Knowledge Hierarchy’, the ‘Information Hierarchy’ and the ‘Knowledge Pyramid’ is one of the fundamental, widely recognized and ‘taken-for-granted’ models in the information and knowledge literatures It is often quoted, or used implicitly, in definitions of data, information and knowledge in the information management, information systems and knowledge management literatures, but there has been limited direct discussion of the hierarchy After revisiting Ackoff’s original articulation of the hierarchy, definitions of data, information, knowledge and wisdom as articulated in recent textbooks in information systems and knowledge management are reviewed and assessed, in pursuit of a consensus on definitions and transformation processes This process brings to the surface the extent of agreement and dissent in relation to these definitions, and provides a basis for a discussion as to whether these articulations present an adequate distinction between data, information, and knowledge Typically information is defined in terms of data, knowledge in terms of information, and wisdom in terms of knowledge, but there is less consensus in the description of the processes that transform elements lower in the hierarchy into those above them, leading to a lack of definitional clarity In addition, there is limited reference to wisdom in these texts"
"HT06, Tagging Paper, Taxonomy, Flickr, Academic Article,
To Read",information organization,IO,paper,tagging resources social many taxonomy add keywords internet pages images,systems potential provide enable model possible inform analysis present system,work help become increasingly users videos relying controlled vocabulary improve,"Tagging systems, taxonomy, folksonomy, tagsonomy, Flickr,
categorization, classification, social networks, social software,
models, incentives, research ",235,9,"Marlow, Cameron, Mor Naaman, Danah Boyd, and Marc Davis.",2006,"In recent years, tagging systems have become increasingly popular These systems enable users to add keywords (ie, “tags”) to Internet resources (eg, web pages, images, videos) without relying on a controlled vocabulary Tagging systems have the potential to improve search, spam detection, reputation systems, and personal organization while introducing new modalities of social communication and opportunities for data mining This potential is largely due to the social structure that underlies many of the current systems Despite the rapid expansion of applications that support tagging of resources, tagging systems are still not well studied or understood In this paper, we provide a short description of the academic related work to date We offer a model of tagging systems, specifically in the context of web-based systems, to help us illustrate the possible benefits of these tools Since many such systems already exist, we provide a taxonomy of tagging systems to help inform their analysis and design, and thus enable researchers to frame and compare evidence for the sustainability of such systems We also provide a simple taxonomy of incentives and contribution models to inform potential evaluative frameworks While this work does not present comprehensive empirical results, we present a preliminary study of the photosharing and tagging system Flickr to demonstrate our model and explore some of the issues in one sample system This analysis helps us outline and motivate possible future directions of research in tagging systems"
A Translation Approach to Portable Ontology Specifications,information organization,IO,paper,systems vocabulary definitions called ontology ontologies ontolingua representations sharing define, representational computational useful classes objects portable representation written format predicate,reuse represented knowledge among shared specialized languages problems preserving support,,178,22,Thomas Gruber,1993,"To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms."
What Is a Knowledge Representation?,information organization,IO,paper,one directly five although central argued various still focused back,representation papers different roles concepts answered numerous important notion general,properties question knowledge ways familiar fundamental it what it? has rarely lobbied,,154,17,"Randall Davis, Howard Shrobe, and Peter Szolovits",1993,"Although knowledge representation is one of the central and, in some ways, most familiar concepts in AI, the most fundamental question about it—What is it?—has rarely been answered directly Numerous papers have lobbied for one or another variety of representation, other papers have argued for various properties a representation should have, and still others have focused on properties that are important to the notion of representation in general In this article, we go back to basics to address the question directly We believe that the answer can best be understood in terms of five important and distinctly different roles that a representation plays, each of which places different and, at times, conflicting demands on the properties a representation should have We argue that keeping in mind all five of these roles provides a usefully broad perspective that sheds light on some long-standing disputes and can invigorate both research and practice in the field"
Six Different Kinds of Composition,information organization,IO,paper,kinds relationships object help referred using complexity examines associations answering,objects various aggregation reduces treating one form whole-part also referred,composition also mechanism forming whole asits parts many column ways,,91,9,James J. Odell,1994,Composition (also referred to as aggregation ) is a mechanism for forming an object whole using other objects asits parts It reduces complexity by treating many objects as one object This column examines the ways in whichwe form these whole-part associations by answering the following questions: What are the primary kinds of composition relationships? What kinds of relationships are often confused with composition relationships? How do these various kinds of relationships help us to make correct inferences about whole-partassociations? How will these various kinds of composition help us during system development?
Situating methods in the magic of Big Data and AI,information organization,IO,paper,intelligence political interrogation shape myths supposed magic systems increasingly widespread,practices big captured imagination profoundly shaping social economic spheres problematize,data technologies machine methodological artificial public histories perceptions faith grounding,,115,25,"M. C. Elish, danah boyd",2018,"“Big Data” and “artificial intelligence” have captured the public imagination and are profoundly shaping social, economic, and political spheres Through an interrogation of the histories, perceptions, and practices that shape these technologies, we problematize the myths that animate the supposed “magic” of these systems In the face of an increasingly widespread blind faith in data-driven technologies, we argue for grounding machine learning-based practices and untethering them from hype and fear cycles One path forward is to develop a rich methodological framework for addressing the strengths and weaknesses of doing data analysis Through provocatively reimagining machine learning as computational ethnography, we invite practitioners to prioritize methodological reflection and recognize that all knowledge work is situated practice"
Designing Data Governance,information organization,IO,paper,governance decision domains may framework used approach organizational given proposed,data also presented practitioners develop identified arguments described differing levels,managing important key domain provided positions decentralized shared different similar,,143,5,"Vijay Khatri, Carol V. Brown",2010,"We have presented a data governance framework that can be used by practitioners to develop a data governance strategy and approach for managing data as an organizational asset We have identified five decision domains, presented arguments for why each of these domains is important, described some key decisions to be made for each domain, and provided some examples of organizational positions that may be given accountability We also have proposed that differing levels of centralized, decentralized, and shared decision rights may be appropriate for different decision domains in the same organization Similar to Weill and Ross, we also suggest that a “one page” design matrix (Table ) may be useful for communicating a given organization’s data governance approach The proposed framework also provides a common terminology that can be used by researchers to share their findings with other members of the IS community"
Goods: Organizing Google’s Datasets,information organization,IO,paper,services systems may goods relationships take forms spreadsheets even data,datasets metadata structured often scale enterprises increasingly run files databases,use order rely businesses variety provide access reside formats present,,211,12,"Halevy, Alon, Flip Korn, Natalya F. Noy, Christopher Olston, Neoklis Polyzotis, Sudip Roy, Steven Euijong Whang",2016,"Enterprises increasingly rely on structured datasets to run their businesses These datasets take a variety of forms, such as structured files, databases, spreadsheets, or even services that provide access to the data The datasets often reside in dierent storage systems, may vary in their formats, may change every day In this paper, we present Goods, a project to rethink how we organize structured datasets at scale, in a setting where teams use diverse and often idiosyncratic ways to produce the datasets and where there is no centralized system for storing and querying them Goods extracts metadata ranging from salient information about each dataset (owners, timestamps, schema) to relationships among datasets, such as similarity and provenance It then exposes this metadata through services that allow engineers to find datasets within the company, to monitor datasets, to annotate them in order to enable others to use their datasets, and to analyze relationships between them We discuss the technical challenges that we had to overcome in order to crawl and infer the metadata for billions of datasets, to maintain the consistency of our metadata catalog at scale, and to expose the metadata to users We believe that many of the lessons that we learned are applicable to building large-scale enterprise-level datamanagement systems in general"
Information Organization Introduction,information organization,IO,slides,organization week course literature maps organize logistics principles practice tools, goals organizing science applying individual canvas usage introduction uva theory,information points systems enables data apply project exam material mind,,569,26,Paul Groth,2020,"Information Organization Introduction Paul Groth UvA   • Course goals and literature • Logistics • Why we organize? Today  Who am I? Professor Algorithmic Data Science Lead the Intelligent Data Engineering Lab (INDElaborg) at the UvA  Course Goals • Introduce you to theory and principles of information organization Practice applying these mental tools  The Objectives • Explain key principles and practices of information organization • Develop information / knowledge organization systems • Critically analyze existing information organization systems  Course Goals: Thesis • determine the essence of a topic or problem • summarize that essence • apply organization systems • think beyond individual topics • engage directly with sources  Abstract Course Goals Remembering Applying Analysing Evaluating Understanding Applying Analysing Evaluating Creating Complex Simple Abstract Concrete Image based on: http://wwwucdenveredu/faculty_staff/faculty/center-for-facultydevelopment/ Documents/Tutorials/ Assessment/module/indexhtm  Content Skill Course Context Course Topics Date Topic Week  Why We Organize The Process of Organizing Week  Introduction to Information Architecture Logical Organization and Navigation Week  Metadata Ontologies and Controlling Vocabulary Week  Relationships Categories Week  Classification Annotation Week  Information Organization in the Enterprise Week  Finalize Project & Review Week  Digital Exam  Logistics • QA on the the material in the last week • Wednesdays: : - : on Zoom • Practicals • Wednesdays: : - :  Material & Discussions • Lecture Videos • Literature - Scientific articles () posted to Canvas • Discussion on Canvas - post or answer a question about the material / literature for the upcoming Q/A session At least  weeks   Individual Assignments • Done individually • Due Monday Night • Submit via canvas  The Project • A website that organizes a collection of content • Apply the theory and practices we discuss in class • Build a training set and train a machine learning algorithm to apply your organization • Deliverables: • The website • The annotated data • The formal organization system •  page report   Exam • Concepts! • Reflection • Literature + Lectures  Grading • Individual Assignments ( points each / Mindmap  points) • Participation ( points) • Project ( points) • Exam ( points - at least  points)  Opportunity  Information Organization • “Concepts, methods, and technologies for organizing information” (Glushko) • Age old practice  Organization enables usage  Organization enables usage  Organization enables usage Information Retrieval and Information Organization are intertwined  Organization enables reuse  Organizing System: an intentionally arranged collection of resources and the interactions they support — The Discipline of Organizing  Mind maps Image source: http://wwwmind-mappingcouk/_images/_Images/ADVICE-AND-INFORMATION/How-to-MindMap-imindmapjpg  Mind maps • Examples: http://wwwargumentenfabrieknl/ de-argumentenkaart (in Dutch) • Tools: http://freemindsourceforgenet/ (mind maps), http://cmapihmcus/ (concept maps)  What’s next • Reading: • Buckland, MK (), Information as thing J Am Soc Inf Sci, : - • K E Oh, “Types of personal information categorization: Rigid, fuzzy, and flexible,” Journal of the Association for Information Science and Technology, vol , no , pp –,  • A Voida, E Harmon, and B Al-Ani, “Homebrew databases: complexities of everyday information management in nonprofit organizations,” presented at the Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, , pp – • J Rowley, “The wisdom hierarchy: representations of the DIKW hierarchy,” Journal of Information Science, vol , no , pp –, Apr  • Assignment: Organize Yourself mind-map due Monday "
The Process of Organizing,information organization,IO,slides,organized organizing systems organize groth system resources discipline data way,organization things collection properties uva different semantic descriptions object clear,information description much modeling process paul support task relationship memory,,764,47,Paul Groth,2020,"The Process of Organizing Information Organization UvA Paul Groth Organizing System: an intentionally arranged collection of resources and the interactions they support — The Discipline of Organizing  Designing Organization Systems • What is being organized? • Why its is being organized? • How much is being organized? • When is it being organized? • How or by whom is it being organized? What do we organize? What do we organize? What do we organize? • Things • Information about things • Information Affordances • Affordance: “a quality of an object, or an environment, which allows an individual to perform an action” (Wikipedia) • Different objects enable different organization paradigms • Clear connection in physical space • Not as clear in information space Why do we organize? Why do we organize? • Bringing things together - collection • To achieve a task • eg customer relationship management, HR, entertainment, science • To support task performance • Preservation & Archival • Broader institutional goals? • To support automation Memory Institutions How much do you have? • Personal information management • eg Filers vs Pilers (Malone, ) How much is being organized? • How much description is necessary to retrieve the right set of data? • facebook timeline vs investigative reporting • More data - more organization? • Supporting automated techniques Who is doing the organization? • Professionals (you) • Creators • Non-creator users (ie community) • Crowdsourcing • Computers • A combination When is it being organized? • Where is the investment? • On the way out? • On the way in? • Emergent organization - pay as you go Designing Organization Systems • What is being organized? • Why its is being organized? • How much is being organized? • When is it being organized? • How or by whom is it being organized? Organizing Principles Information Organization UvA  Paul Groth Organizing Principles “directives for the design or arrangement of a collection of resources that are ideally expressed in a way that does not assume any particular implementation or realization” - The Discipline of Organizing Knowledge Organization System • Systems for information organization • Traditionally, from memory institutions • Provide semantics and specify relationships between symbols and concepts to help with disambiguation, grouping, and usage Desiderata • The KOS imposes a particular view of the world on a collection and the items in it • The same entity can be characterized in different ways, depending on the KOS that is used • There must be sufficient commonality between the concept expressed in a KOS and the real-world object to which that concept refers that a knowledgeable person could apply/use the system with reasonable reliability Examples • Term Lists • eg glossaries, dictionaries, authority files (preferred terms), gazetteer, controlled vocabularies, tags • Classifications • eg taxonomy, subject headings in controlled vocabularies , folksonomys • Relationship lists • eg thesaurus, semantic networks, ontologies, mind maps, citation networks Term List Classifications Summary • Information Organization facilitates use and reuse • Information organization systems require design decisions • Knowledge organization systems have a point of view • There are many kinds of information organization The Process of Organizing Information Organization UvA Paul Groth Describing things Description is central • Organization systems enable description • Organization systems limit how things can be described Description Depends on • Goal • Context • Size • Audience • What is being organized? • Why its is being organized? • How much is being organized? • When is it being organized? • How or by whom is it being organized? Description Depends on • Goal • Context • Size • Audience • What is being organized? • Why its is being organized? • How much is being organized? • When is it being organized? • How or by whom is it being organized? Trade-offs Properties of Description • Name • Physical Properties • Cultural & Contextual properties • Structural Properties Google Structured Data Testing Tool Process of Description Good Descriptions from: The Intellectual Foundations of Information Organization Good Descriptions • User convenience • Common usage • Representation • Accuracy • Sufficiency and necessity • Significance • Standardization • Integration from: The Intellectual Foundations of Information Organization Modeling & Description • Modeling is a key task in information organization • Modeling is about making choices • Modeling provides the framework for descriptions • Modeling can have multiple notations The Problem of Naming The Vocabulary Problem Semantic Gap Semantic Gap “The difference in perspective in naming and description when resources are described by automated processes rather than by people” - The Discipline of Organizing"
Information Architecture,information organization,IO,slides,information design site development users discovery testing organized stakeholder competitive,content architecture navigation definition research goals task map systems labeling,user organization usability sketching understand needs time develop used review,,1146,45,Paul Groth,2020,"Information Architecture Information Organization UvA Paul Groth Designing Organization Systems • What is being organized? • Why is it being organized? • How much is being organized? • When is it being organized? • How or by whom is it being organized? Architecture: the practice of designing buildings Computer Architecture Network Architecture https://uploadwikimediaorg/wikipedia/commons/a/af/Android-System-Architecturesvg Software Architecture Information Architecture •The structural design of shared information environments • The synthesis of organization, labeling, search, and navigation systems within digital, physical, and cross-channel ecosystems • The art and science of shaping information products and experience to support usability and findability, and understanding • An emerging discipline and community of practice focused on bringing principles of design and architecture to the digital landscape (Rosenfeld, Morville) “Information architecture is about organizing and simplifying information for its intended users; designing, integrating, and aggregating information spaces to create usable systems or interfaces; creating ways for people to find, understand, exchange and manage information; and, therefore, stay on top of information and make the right decisions “ (Ding et al ) An Information Architect Information Organization UvA Paul Groth An Information Architect • Clarifies the mission and vision for the site, balancing the needs of its sponsoring organization and the needs of its audiences • Determines what content and functionality the site will contain • Specifies how users will find information in the site by defining its organization, navigation, labeling, and searching systems • Maps out how the site will accommodate change and growth over time Focus Areas Navigation - The combination of organization, labeling, and navigation schemes within an information system Interaction - The structural design of an information space to facilitate task completion and intuitive access to content Information Architecture Design Information Organization UvA Paul Groth Discovery Definition Design Development Discovery Definition Design Development • Stakeholder Interviews • Business Requirements • Competitive & Comparative Audits • User Research • Site Inventory Research Background research - understand the problems and challenges as well as the goals and objectives; develop short-term and long-term goals; learn existing technical infrastructure; develop schedules and budgets, etc Communication channels - research interview and develop working relationships with stakeholders, management team, technical team, intended users, etc Content research - learn who creates, maintains or updates content, who owns what content, how content is used currently, etc User needs, tasks and their environment - understand culture, workflow, and sophisticated levels of users Benchmarking - include both competitive benchmarking and before and after benchmarkingThe best time to start benchmarking surveys or research is before the project starts User Research • Goals • Identify patterns and trends in user behavior, tasks, preferences, obstacles • Methodology • Focus Groups • Surveys • Interviews Competitive / Comparative Review “This type of assessment helps set an industry ‘marker’ by looking at what the competition is up to, what features and functionalities are standard, and how others have solved the same problems you might be tasked with” – Dorelle Rabinowitz Heuristic Evaluation involves evaluators examining the interface and judging its compliance with recognized usability principles (the ‘heuristics’) - Wikipedia Usability Criteria • Home Page • Are home page elements appropriately weighted and distributed? • Is information clustered in meaningful ways • Navigation • Is the navigation structure concise and consistent? • Are paths to important information intuitive and unobstructed? • Content • Is content current? Are there visible indications of content freshness? • Is content properly adapted for the Web? Is tone of voice consistent throughout content? Is content chunked appropriately? • Are headings and titles scannable? • Design • Are colors appropriate to the Web? Is white space used appropriately? Is text readable? • Search • Are search results relevant and cleanly presented? • Functionality • Are functionality and forms efficiently designed? • Messaging • Are errors messages clear on the site? Is help readily available to users? • Are there appropriate means for user feedback? https://wwwnngroupcom/articles/ten-usability-heuristics/ Review Methodology • Review and analyze competitor sites according to particular criteria • Draw key findings, which can influence and guide IA through the design phase • Include a scorecard for high-level comparison of points across all sites Discovery Definition Design Development • Personas • Content Audit • Card Sorts • Use Cases • Sketching • Site Map • User Journeys • Conceptual Wires/Design • Creative Brief • UX Brief Content inventory Content Audit Content Grouping • What are the relationships between content? Card Sort Methodology • Grouping and labeling with index cards, post it notes • Two types: • Open – Participants sort cards with no preestablished categories Useful for new architectures • Closed – Participants sort cards into predetermined, provided groups Useful for fitting content into existing architectures • Online card sorts • WebSort, OptimalSort, Socratic Goals • Organize content more efficiently • Find names for categories based on users’ perspectives Characteristics • Lumping and splitting • Misc • Items in multiple categories • Categories vs Villers • Unique but intuitive labels Outcomes • Information grouping: Definition of usercentered relationships between content • Taxonomy development: Definition of a standardized naming convention (controlled vocabulary) to apply to all site content • Descriptive information creation: Definition of useful metadata that can be utilized to generate “Related Link” lists or other navigation components that aid discovery “A site map is a high level diagram showing the hierarchy of a system Site maps reflect the information structure, but are not necessarily indicative of the navigation structure” - Step Two Designs Credit: https://wwwslidesharenet/stribs/introduction-to-information-architecture-design- Discovery Definition Design Development • Site Map • Task Flows • Sketching • Wireframes • Stakeholder Reviews • Visual Design • Prototype • Usability Testing • Functional Specifications Navigation https://wwwnngroupcom/articles/ia-vs-navigation/ Tree Testing https://wwwnngroupcom/articles/tree-testing/ Wireframe Discovery Definition Design Development • Site Development • User Acceptance Testing (UAT) • Quality Assurance (QA) • Usability Testing ISO : Ergonomics of human-system • Effectiveness: Accuracy and completeness with which users achieve specified goals  • Efficiency: Resources expended in relation to the accuracy and completeness with which users achieve goals • Satisfaction: Freedom from discomfort, and positive attitudes toward the use of the product Measurement • Effectiveness: Task completion rates/errors • Efficiency: Time on task • Satisfaction: User ratings Discovery Definition Design Development • Stakeholder Interviews • Business Requirements • Competitive & Comparative Audits • User Research • Site Inventory • Personas • Content Audit • Card Sorts • Use Cases • Sketching • Site Map • User Journeys • Conceptual Wires/Design • Creative Brief • UX Brief • Site Map • Task Flows • Sketching • Wireframes • Stakeholder Reviews • Visual Design • Prototype • Usability Testing • Functional Specifications • Site Development • User Acceptance Testing (UAT) • Quality Assurance (QA) • Usability Testing Sources used: • Information Architecture: The Design and Integration of Information Spaces: Second Edition - Wei Ding, Xia Lin, and Michael Zarro • Robert Stribley - Introduction to Information Architecture and Design • https://wwwslidesharenet/stribs/ introduction-to-information-architecturedesign- "
Metadata,information organization,IO,slides,groth properties description data name elements structure semantics writing things,metadata identifiers indirection uva structural descriptions characters notation quantity unique,organization information paul syntax price physical cultural contextual external instances,,334,34,Paul Groth,2020,"Metadata Information Organization UvA Paul Groth Properties of Description • Name • Physical Properties • Cultural & Contextual properties • Structural Properties • Intrinsic vs Extrinsic • Embedded, encapsulated, external • Instances vs classes • Professional, social, automatic • Vocabulary Metadata • a surrogate record • a collection of descriptions associated with an entity or object • metadata elements - the parts of a description Descriptive Metadata Technical Metadata Rights Metadata Structural Metadata Structural Metadata Metadata levels • structure - the organization of metadata • syntax - the way in which metadata is encoded • semantics - the meaning of metadata elements • schema/metamodel - define semantics Machine Readable Metadata Information Organization UvA Paul Groth Writing things down • Metamodel = constraints on the structure of descriptions (eg graphs, trees, dictionaries, lists) • Syntax = the rules for how characters, words, etc can be combined • Notation = a set of characters with distinct form (ASCII, Unicode, Latin alphabet) • Writing system = notation + syntax exifinfoorg YAML https://yamlcheckercom invoice:  date : -- bill-to: &id given : Chris family : Dumars address: lines: |  Walkman Dr Suite # city : Royal Oak state : MI postal :  ship-to: *id product: - sku : BLD quantity :  description : Basketball price :  - sku : BLH quantity :  description : Super Hoop price :  tax :  total:  comments: > Late afternoon is best Backup contact is Nancy Billsmer @ - Metadata on the Web https://developersgooglecom/ search/docs/guides/introstructured- data https://wwwworg/TR/ rdf-primer/ Identifiers Information Organization UvA Paul Groth Identifiers • terms that identify things (names) • unique within a scope • The unique name assumption • persistent • has to do with the organization • global - never conflicts with any other Indirection & Identifiers https://enwikipediaorg/wiki/Digital_object_identifier Gregory, K, Groth, P, Scharnhorst, A, & Wyatt, S () Lost or Found? Discovering Data Needed for Research Harvard Data Science Review https://doiorg//feeb Indirection & Identifiers Indirection & Identifiers Indirection & Identifiers https://wwwcrossreforg/"
Controlled Vocabularies,information organization,IO,slides,controlled models ontology organization data entity represent paul problem definitions, terms entities vocabularies groth description relationships modeling graph model term,knowledge semantic representation classes information uva vocabulary meaning used panos,,480,35,Paul Groth,2020,"Controlled Vocabularies Information Organization UvA Paul Groth The Problem of Naming The Vocabulary Problem Term A term is a string of characters that can be used to lexically describe something Controlled Vocabularies • a dictionary of all possible terms that can be used in a domain for description • terms + definitions + rules • authority control http://wwwlocgov/marc/bibliographic/ Knowledge Organization Systems • Catalog - controlled vocabulary with preferred terms • Glossary - terms with definitions • Thesaurus - synonym relationships • Taxonomy - a controlled vocabulary with all the terms belong to a single parent/child hierarchical structure Schemaorg Becoming Explicit • Controlled vocabularies allow for easier communication of meaning • Limit choice • Increasing structure conveys additional meaning Semantic Models & Ontology Information Organization UvA Paul Groth Semantic Models “Representation of data whose goal is make the meaning of data explicit and commonly understood among humans and machines” Panos Alexopoulos Semantic Modeling for Data O'Reilly Media  • Resource Description Framework (Schema) aka RDF(S) • Description Logics (eg OWL) a family of formal knowledge representation languages that provide the logical formalisms for Semantic Web languages • Standards for controlled vocabularies, thesauri, and taxonomies like ANSI/NISO Z, ISO, SKOS • Lexical databases like WordNet, VerbNet • Conceptual models for database design (E-R models labeled property graph model for graph databases) Modeling Frameworks Alexopoulos, Panos Semantic Modeling for Data O'Reilly Media Entities A thing with distinct and independent existence (ie resource) Should be unambiguous and distinct within a model Concrete Entity - a thing at a particular place and time Abstract Entity - eg idea, categories, concepts Relations • Relations express the way that two or more entities can be related to one another Classes • Classes are abstract entities that represent kinds of things in the world • Serve as types for other entities • See Later Discussion of Categorization Attributes • Represent a characteristic of an entity that we cannot (or choose not to) represent as a relation with another entity • literal values (ie, numbers, strings, dates, etc) Axioms, Constraints, Rules Features that enable reasoning: Derivation of facts that are not explicitly in the model http://wwwfewvunl/~schlobac/esslli_daypdf Ontology • “An ontology is a specification of a conceptualization” - Gruber The Ontology Spectrum Basic Ontology • Finite controlled (extensible) vocabulary • Unambiguous interpretation of classes and term relationships • Strict hierarchical subclass relationships between classes What is Knowledge Representation? What is Knowledge Representation? • a surrogate • a set of ontological commitments • theory of reasoning • medium for efficient computation • medium for human expression Semantic Models • Help us represent knowledge • Machines People • Frameworks give us different capabilities for expression • They share common features Semantic Models In the Wild Information Organization UvA Paul Groth https://developerspotifycom/ documentation/web-api/reference-beta/ #object-albumobject https://labsmlssoccercom/implementing-graphql-at-major-league-soccer-ffabca https://engubercom/uber-eats-query-understanding/ Knowledge Graphs • Massive collections of metadata about entities in the world • Graph structured knowledge base"
Relationships,information organization,IO,slides,resources one bart terms meaning words marge lexical architectural slides,relationship perspective simpson structure document inclusion class properties among link,relationships homer semantic language component information binary maggie used paul,,2781,65,Paul Groth,2020,"Relationships Paul Groth Information Organization UvA Slides from Robert J Glushko glushko@berkeleyedu @rjglushko Defining ""Relationship“  • Relations express the way that two or more entities can be related to one another • “An association among several things, with that association having a particular significance” (Kent) • The reason is an important part of the relationship • Multiple relationships can exist among the same objects, so the order of the objects matters Five Perspectives on Relationships ()  • SEMANTIC: the meaning of the association • LEXICAL: how the conceptual description of a relationship is expressed using words in a specific language • STRUCTURAL: analyzes the patterns of association, arrangement, proximity, or connection between resources Five Perspectives on Relationships ()  • ARCHITECTURAL: emphasizes the number and abstraction level of the components of a relationship, which together characterize its complexity • IMPLEMENTATION: how the relationship is implemented in a particular notation and syntax and the manner in which relationships are arranged and stored in some technology environment Relationships: The Semantic Perspective Paul Groth Information Organization UvA Slides from Robert J Glushko glushko@berkeleyedu @rjglushko The Semantic Perspective on Relationships  • Critical to understand what relations mean • Conveys: why the resources are related • Relies on or expresses information that is not directly available from perception • “Homer is married to Marge"" is a semantic assertion; ""Homer is standing next to Marge"" is not • It is difficult to define what a relationship means in a non-circular way that doesn't rely on lots of undefined relationships… Making Sense () Making Sense ()  • Draw a diagram that represents these assertions: • ""Mary and Sue are sisters“ • ""Sally and Martha are mothers” Mother Mary Sue Mary and Sue are sisters… …of each other Mary’s Mother Mary Mary’s Sister Sue’s Mother Sue Sue’s Sister Mary and Sue are sisters… …of other people Sally Sally’s Child Sally and Martha are mothers …of different children Martha Martha’s Child Sally and Martha are mothers …Sally is Martha’s mother Sally Martha (Sally’s Child) Martha’s Child Language and Meaning  • Words and sentence structure only hint at meaning • Meaning is constructed not just from the literal language used, but from all the clues or cues in the context of use -- common knowledge, assumptions, previous discourse, the present situation, and inferences from all of these The Semantics of ""Inclusion""  • One class or type of resources logically contains another; predicate is usually is-a or ""is-a-type-of"" • A set of interconnected class inclusion relationships defines a HIERARCHY or TAXONOMY • An inclusion relationship between an instance and a class -- assigning an instance to a class - is CLASSIFICATION TDO Figure  Part-Whole (Meronymic) Inclusion  • A resource class or instance can contain another as physical or logical parts • Expressed using “is-part-of,” “is-partly,” or with other similar predicate expressions • The engine is part of the car • The book is part of the library • Wine is partly alcohol • The Napa Valley is part of Northern California • Extra time is part of a football match Part-Whole Relationship (Component-Object) Topological, Locative, and Temporal Inclusion  • A relationship between container, area, or time period and something it surrounds or contains (expressed using “is in”) • But unlike part-whole inclusion, the contained thing is not part of the container • The Vatican City is in Italy • The meeting is in the afternoon The Semantics of “Attribution”  • In contrast to inclusion expressions that state relationships between resources, attribution relationships assert or assign values to properties for a particular resource • Attribution can be thought of as a binary relationship whose predicate has a single argument: Martin the Gecko => is-small ""Title"" : ""Moby Dick“ • Or with two arguments: Martin => has-size => small The Semantics of “Attribution”  • There are often many different ways of expressing equivalent attribute values • Martin → has-size →  inches • Martin → has size →  mm • Do these mean the same thing? Properties of Semantic Relationships: Symmetry ()  • SYMMETRY: the order in which the arguments of a binary relationship doesn't matter Homer Simpson => is-married-to => Marge Simpson Marge Simpson => is-married-to => Homer Simpson Properties of Semantic Relationships: Symmetry ()  • Relationships that are symmetric are also called BI-DIRECTIONAL • ASYMMETRIC relationships are those where the order of the arguments matters Homer Simpson => is-parent-of => Bart Simpson Properties of Semantic Relationships: Inverse  • For asymmetric relationships, it is often useful to be explicit about the meaning of the relationship when the order of the arguments in the relationship is reversed • The resulting relationship is called the INVERSE or the converse of the first relationship Properties of Semantic Relationships: Transitivity ()  • TRANSITIVITY: if X and Y have a relationship, and Y and Z have the same relationship, then X also has the relationship with Z Homer Simpson => is-taller-than => Bart Simpson Bart Simpson => is-taller-than => Maggie Simpson IMPLIES THAT Homer Simpson => is-taller-than => Maggie Simpson Properties of Semantic Relationships: Transitivity ()  • Any relationship based on inclusion or ordering is transitive • Ordering includes numerical, alphabetic, and chronological relationships as well as those that imply qualitative or quantitative measurement Properties of Semantic Relationships: Equivalence  • Any relationship that is both symmetric and transitive is an EQUIVALENCE relationship • We often need to assert that a particular class or property has the same meaning as another class or property or that it is generally substitutable for it Ontology  • An ontology defines the concepts used to describe and represent some domain or area of knowledge • To fully understand a domain you need to know class and part-whole inclusion, attribution, equivalence, and many other relationships about and between the resources it contains • Class inclusion relationships form a kind of backbone or framework to which other kinds of relationships attach, creating a network of relationships Analyzing the Definition of Ontology  • The word ontology has been used to describe artifacts with different degrees of structure that differ: • according to how precisely the terms are defined • according to how precisely the relationships among them are expressed • So the simplest ontology is a dictionary • A thesaurus is a somewhat more complex ontology • More complete ontologies are expressed using formal logic-based language TDO Figure  Relationships: The Lexical Perspective Paul Groth Information Organization UvA Slides from Robert J Glushko glushko@berkeleyedu @rjglushko The Lexical Perspective on Relationships  • A prototypical word is the minimal ""meaning bearing"" element of language • Words express concepts and relationships, but not all concepts and relationships have words for them (are ""lexicalized"") • These ""lexical gaps"" differ from language to language • Whereas ""conceptual gaps"" -- the things we can't think of -- may be innate and universal Relationships Among Words / Relationships Among Concepts  • Representing the relationships among words is a task for linguistics and cognitive science • In contrast, representing the relationships among concepts is a task for computer science • Understanding both kinds of relationships is essential for designers of organizing systems Relationships Among Words  • Hyponymy/Hyperonymy • Metonymy • Synonymy • Polysemy • Antonymy Hyponymy/Hyperonymy  • When words encode IS-A or inclusion relationships, the word for the more specific class is the HYPONYM and the other is the HYPERNYM • So there can be a ""lexical hierarchy"" that represents the ""semantic hierarchy"" • Often used to situate ""basic categories"" with respect to superordinate and subordinate categories Hyponymy/Hyperonymy  • A is a hyponym of B if A is a type of B • A robin is a hyponym of bird • A is a hypernym of B if B is a type of A • An animal is a hypernym of bird A Formula for Definitions  hyponym = {adjective+} hypernym {distinguishing clause+} • Robin = Migratory BIRD with clear melodious song, a reddish breast, gray or black upper plumage • Doesn't mention every characteristic of hyponym, only those needed to distinguish from other hyponyms Metonymy  • The lexical representation of or analogy to part-whole or meronymic relationships • Using one aspect of something to stand for the whole • ""Bank"" as building stands for the institution of the ""bank"" • ""The White House released new budget figures today"" Synonymy  • Synonyms are different word forms that can express the same concept • cat, feline, Siamese cat • Absolute synonyms that can always substitute for each other probably don't exist • Propositional synonyms are those where substituting one for another isn’t likely to change the truth value of the statement Polysemy  • Many ""word forms"" (particular spelling patterns) are polysemous with multiple senses established in the language -- they are semantically ambiguous - That dog has floppy ears - She has a good ear for jazz • ""bank"" (financial) has related senses: - a building (the bank on Shattuck) - specific financial firm (Wells Fargo) - where money is kept (abstract notion) - where anything of value is kept (more abstract) Polysemy vs Homonymy  • A HOMOGRAPH is a word with multiple senses that are not conceptually related - bank (financial sense) - bank (river sense) • But what looks like homography may be polysemy from a historical perspective - mole (an animal that digs holes; an infiltrating spy) • HOMOPHONES are words with the same pronunciation but different spellings and meanings - to, two, too; might, mite; clawed, claude Antonymy  • Antonyms are lexical opposites • Some are inherently binary “true antonyms” • dead / alive, true / false, on / off • Others are ""graded"" • long / short, hot / cold • Markedness: if one member of a pair is more restricted in its contexts it can stand out psychologically • long is unmarked, short is marked Thesauri  • A THESAURUS is a tool for finding the ""right"" or ""good"" terms of a controlled vocabulary • It is a collection of vocabulary terms annotated with lexical relationships to indicate terms that are: • Preferred (UF ""used for"") • Broader (BT ""broader term"") • Narrower (NT ""narrower term"") • Related (RT ""related term"" or ""see also"") • USE in a thesaurus refers the reader from a variant term to a preferred term; the inverse of UF Implications for Vocabulary Design  • Choosing vocabulary terms, and precisely defining their semantics, is essential but impossible to do perfectly • Your vocabulary must express what YOU intend, so you ""look inward"" -- analyze how you think about a domain • You want others to understand what you mean, so you need to ""look outward"" -- analyze the terms used by your users, competitors, or subject matter experts • You should reuse other vocabularies or thesauri if they exist, especially for any ""horizontal"" components, to improve transformability and interoperability • But these three approaches may suggest different terms https://wordnetprincetonedu Relationships: The Structural Perspective Paul Groth Information Organization UvA Slides from Robert J Glushko glushko@berkeleyedu @rjglushko The Structural Perspective on Relationships  • Analyzing the association, arrangement, proximity, or connection between resources without primary concern for their meaning or the origin of these relationships • Sometimes structure is all we knowand sometimes we ignore what we know about relationship semantics to focus on the generic aspect of structural connectivity Internal and External Structure ()  • Resources can have INTERNAL structure as well as the EXTERNAL structure that connects them to other resources • We often make arbitrary decisions about the granularity with which we describe the internal structure of a resource • The boundaries we impose to identify resources determines whether some structure is internal or external with respect to them Internal and External Structure ()  • Example: How do we describe the relationships between companies? • In terms of the company-to-company transactions they carry out • In terms of the business components/ services that are involved • In terms of the interactions between their employees Making Implicit Structure Explicit  • Some organizing principles impose very little structure and the structure might be implicit • co-location / pile of resources used together • arrangement by frequency of use • Coordinate systems enable absolute description of location, or we can use relative descriptions • in, on, above, below, in front of, behind… • Statistical techniques can make explicit the structure implied by frequency distribution or correlations between properties of resources “Between-Resource” Structures  • Links between printed or digital documents • Links between web pages • Links – communication and information flows - between people, organizations, any other kind of interacting “actors” or resources – social networks • We can analyze all of these with some common concepts and abstractions Between-Document Links From document to document From document component to document component From document component to document From document to document component Within-Document Links From one component to another From document component to contained document component (call out) From document component to document component, pop-up (two-way, mandatory return link) Link Network – Graphical View Link Network – Matrix View Link Properties  • Starting point of the link (the ANCHOR) • The end point of the link (the DESTINATION) • The reason for the link (the LINK TYPE) • The number of resource types (ARITY or DEGREE) or resource instances (CARDINALITY) linked to (- or -many) • The DIRECTIONALITY Meaning from Graph Structure  • Centrality measures • Number of incoming edges • Related nodes • … Relationships: The Architectural Perspective Paul Groth Information Organization UvA Slides from Robert J Glushko glushko@berkeleyedu @rjglushko The Architectural Perspective {and, or, vs} the Structural Perspective  • The architectural perspective is abstract and prescriptive • It defines what kinds of relationships can be created given the design of an organizing system • The structural perspective is concrete and descriptive • It says ""this is what exists"" and describes the actual patterns of association, arrangements, proximity, or connection between resources in a particular context The Architectural Perspective: Degree or Arity  • The architectural perspective embodies Kent’s definition of “relation” as “A sequence of categories, that includes one thing from each category” • The DEGREE or ARITY of a relationship is the number of different ""entity types"" or ""resource categories"" in the relationship • Husband is-married-to Wife is BINARY • Person is-married-to Person is UNARY The Architectural Perspective: Cardinality  • The CARDINALITY is the number of instances that can be associated with each entity type • Husband is-married-to Wife is ONE-TOONE, because husbands have only one wife and vice versa (in monogamous societies) • Father is-parent-of Child is ONE-TO-MANY • Homer is-parent-of Bart AND Lisa AND Maggie is one-to-three Modeling Relationships as Binary Ones  • Relationships can always be modeled as binary ones, but this makes some relationships implicit that were explicit • Binary relationships are relationship ""triples"" with a ""subject"", ""predicate,"" and ""object"" • With binary relationships the reason for the relationship can often be interpreted in both directions (one is the inverse of the other) • With triples we can combine relationships into a graph and ""reason"" over the set of relationships when they have common components The Architectural Perspective: Directionality  • The DIRECTIONALITY of a relationship defines the order in which the arguments of the relationship are connected • A ONE-WAY or UNI-DIRECTIONAL relationship can be followed in only one direction • A BI-DIRECTIONAL one can be followed in both directions • All symmetric relationships are bi-directional, but not all bi-directional relationships are symmetric Relationships: The Implementation Perspective Paul Groth Information Organization UvA Slides from Robert J Glushko glushko@berkeleyedu @rjglushko Choosing an Implementation  • To communicate or process or otherwise use a relationship it needs to be represented in some implementation • The choice of implementation determines how easy it is to represent the relationship, and how easy it then is to understand and process • This means you need to consider the capabilities and biases of the intended users when choosing the implementation Contrasting Implementations (TDO )  • Natural Language • The Simpson family includes a man named Homer and a woman named Marge, the married parents of three sibling children, a boy named Bart and two girls, Lisa and Maggie • Subject-Predicate Syntax • Homer Simpson → is-married-to → Marge Simpson • Homer Simpson → is-parent-of → Bart • XML Syntax <Family name=""Simpson""> <Parents children=""Bart Lisa Maggie""> <Father name=""Homer"" spouse=""Marge"" /> <Mother name=""Marge"" spouse=""Homer"" /> </Parents> <Children parents=""Homer Marge"" > <Boy name=""Bart"" siblings=""Lisa Maggie"" /> <Girl name=""Lisa"" siblings=""Bart Maggie"" /> <Girl name=""Maggie"" siblings=""Bart Lisa"" /> </Children> </Family>"
Classification,information organization,IO,slides,ontology university class nlp population organization instance data document person,annotation sheffield nlp classification concepts based cambridge strike gate annotate,semantic instances text information sheffield knowledge john inclusion bias system,,1744,42,Paul Groth,2020,"Classification Paul Groth Information Organization UvA Class • Central grouping construct • AKA: entity type, concept, entity type • Dictionary Definition • a set or category of things having some property or attribute in common and differentiated from others by kind, type, or quality Class Inclusion • Inclusion relationship: contains or comprised • Class inclusion: where one class is contained in another and thus more specific than the other more generic one • CLASSES: super class, subclasses • Substitution principle • Class Inclusion (UML) Java warning: not every subclass in a programming language is class inclusion Taxonomy • Captures class inclusion in some domain Individual • One thing not a group • AKA: instance, member, entity, occurrence • In OWL, classes have members • Rembrant is an individual and is a member of the class Artist • Enumerated Classes: a class for which all individuals can be listed Classification • Assertion that an individual is a member of a class • The lowest level of our taxonomy • Class inclusion relationship between an instance and a class Classification as an Activity • Classification is the task of identifying the labeling for a single entity from a set of data • Semantic typing - A word or phrase in the sentence is labeled with a type identifier (from a reserved vocabulary or ontology), indicating what it denotes Classification as a System Paul Groth Information Organization UvA Classification • Assertion that an individual is a member of a class • The systematic assignment of resources to a system of intentional categories, often institutional ones • By classifying we “arrange resources to support discovery, selection, combination, integration, analysis, and other purposeful activity in every organizing system” Shared Understanding • Institutional taxonomies are classifications designed to make it more likely that people or computational agents will organize and interact with resources in the same way • Institutional semantics offer precisely defined abstraction needed to ensure that information can be efficiently exchanged and used • Standards vs Defacto standards Justifications in Classification • The warrant principle - the justification for the choice of categories and their names • Literary, Scientific , User • A one time decision (?) Maintenance • Flexibility, extensibility, and hospitality are synonyms for the degree to which the classification can accommodate new resource Concept  Concept  Concept  KOS Professional Curators Literature Software Non-professional contributors  dealing with changing cultural and societal norms, specifically to address or correct bias;  political influence  new concepts and terminology arising from discoveries or change in perspective within a technical/scientific community  gardening  incremental contributorship  progressive formalization  software and automation  integration of large numbers of data sources  variance in algorithm training data Data ⚐ Society & Politics (, , ) (, , ) () (, ) Lauruhn, Michael, and Paul Groth ""Sources of Change for Modern Knowledge Organization Systems"" Knowledge Organization , no  () Bias in Classification • Pre-existing bias - embodies personal or societal biases • Technical bias - arises from limitations and constraints of technical systems that result in unfairness • Emergent bias - from the interplay between users and systems Friedman and Nissenbaum’s Bias In Computer Systems How to Classify Paul Groth Information Organization UvA How to classify? University of Sheffield, NLP Module : Ontologies and Semantic Annotation © The University of Sheffield, - This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike Licence University of Sheffield, NLP Why ontologies in GATE? ● Semantic annotation: rather than just annotating the word “Cambridge” as a location, link it to an ontology instance ● Differentiate between Cambridge, UK and Cambridge, Mass ● Semantic search via reasoning ● So we can infer that this document mentions a city in Europe ● Ontologies tell us that this particular Cambridge is part of the country called the UK, which is part of the continent Europe ● Knowledge source ● If I want to annotate strikes in baseball reports, the ontology will tell me that a strike involves a batter who is a person ● In the text “BA went on strike”, using the knowledge that BA is a company and not a person, the IE system can conclude that this is not the kind of strike it is interested in University of Sheffield, NLP Information Extraction for the Semantic Web  Traditional IE is based on a flat structure, eg recognising Person, Location, Organisation, Date, Time etc  For the Semantic Web, we need information in a hierarchical structure  Idea is that we attach semantic metadata to the documents, pointing to concepts in an ontology  Information can be exported as an ontology annotated with instances, or as text annotated with links to the ontology University of Sheffield, NLP Information Extraction for the Semantic Web  Traditional IE is based on a flat structure, eg recognising Person, Location, Organisation, Date, Time etc  For the Semantic Web, we need information in a hierarchical structure  Idea is that we attach semantic metadata to the documents, pointing to concepts in an ontology  Information can be exported as an ontology annotated with instances, or as text annotated with links to the ontology University of Sheffield, NLP John lives in London He works there for Polar Bear Design PERSON LOCATION ORGANISATION Traditional NE Recognition University of Sheffield, NLP Richer NE Tagging  Attachment of instances in the text to concepts in the domain ontology  Disambiguation of instances, eg Cambridge, MA vs Cambridge, UK University of Sheffield, NLP Ontology-based IE John lives in London He works there for Polar Bear Design University of Sheffield, NLP Ontology-based IE () John lives in London He works there for Polar Bear Design University of Sheffield, NLP How does ontology-based IE help with IE? • We can make inferences about all kinds of things once we have the annotations linked to an ontology • We now know that cities can have airports, and people have phone numbers • Since John is a man, we know he can have a wife • If we know that the London, where John lives, is in England, we know that Polar Bear Design is also in England and not Ontario University of Sheffield, NLP Ontologies are useful for encoding the information found • Enable us to define the concepts we're trying to find in texts – eg, aircraft accident, industrial action • As well as particular instances of these – eg, Qantas flight XYZ crashed on , BA cabin crew were on strike between March -,  • And the relationships between them – eg, the plane that crashed belonged to Qantas and crashed on a specific date University of Sheffield, NLP Using knowledge from the ontology • The ontology tells us that – Industrial action involves airport or airline staff and has a start and end date • It gives a clearly defined schema to annotate against – if you annotate an instance of a strike, then you know this also requires you to annotate the airport/airline affected and the staff on strike • Extra knowledge about the different kinds of properties and the actors involved can help to improve system performance • Backbone for other processes, for example visualising results on a timeline University of Sheffield, NLP Text mining and semantic annotation • Extract structured data from text by – Linking references to entities – Linking entities to their semantic descriptions • Automatic semantic annotation based on IE technology • Attaches metadata to documents, which makes them more useful and more easily processable • They can then be used for searching and hyperlinking, categorising, and monitoring • Adds value to content of libraries, enabling user interaction with content • Enhanced capability for cross-referencing and dynamic document classification University of Sheffield, NLP Some Terminology  Semantic annotation – annotate in the texts all mentions of instances relating to concepts in the ontology  Ontology learning – automatically derive an ontology from texts  Ontology population – given an ontology, populate the concepts with instances derived automatically from a text University of Sheffield, NLP Semantic Annotation vs Ontology Population  Semantic Annotation  Mentions of instances in the text are annotated wrt concepts (classes) in the ontology  Requires that instances are disambiguated  It is the document which is modified  Ontology Population  Generates new instances in an ontology from a text  Links unique mentions of instances in the text to instances of concepts in the ontology  Instances must be not only disambiguated but also coreference between them must be established  It is the ontology which is modified University of Sheffield, NLP Semantic Annotation University of Sheffield, NLP Ontology Population University of Sheffield, NLP How to do Semantic Annotation • Manually: ontology based annotation using OAT (Ontology Annotation Tool) • Automatically ● Gazetteer/rule/pattern based ● Classifier (ML) based ● Combination of the two University of Sheffield, NLP Manual semantic annotation: OAT • Shows document and ontology class hierarchy side-by-side • Interactive creation of annotations that link to the ontology class/instance • Allows on-the-fly instance creation • Used to create evaluation or training corpus University of Sheffield, NLP OAT University of Sheffield, NLP Automatic Semantic Annotation in GATE  GATE supports ontologies as part of IE applications - Ontology-Based IE (OBIE)  Supports semantic annotation and ontology population  GATE has its own ontology API based on Sesame  and OWLIM  Semantic annotation can combine learning and rule-based methods  Enables use of large-scale linguistic resources for IE, such as WordNet University of Sheffield, NLP Semantic IE in GATE University of Sheffield, NLP Typical Semantic Annotation pipeline Analyse document structure Linguistic Pre-processing Ontology Lookup Ontology-based IE Populate ontology NE recognition Corpus Export as RDF RDF The Annotation Development Cycle MATTER methodology, as detailed here and shown in Figure - (Pustejovsky ): University of Sheffield, NLP Further materials Ontology design principles: http://lsdiscsugaedu/SemWebCourse/OntologyDesignppt BDM: http://gateacuk/userguide/sec:eval:bdmplugin Semantic Annotation: K Bontcheva, B Davis, A Funk, Y Li and T Wang Human Language Technologies Semantic Knowledge Management, John Davies, Marko Grobelnik, and Dunja Mladenic (Eds), Springer, -,  KBontcheva, HCunningham, AKiryakov and VTablan Semantic Annotation and Human Language Technology Semantic Web Technology: Trends and Research John Wiley and Sons Ltd  D Maynard, Y Li and W Peters NLP Techniques for Term Extraction and Ontology Population Bridging the Gap between Text and Knowledge - Selected Contributions to Ontology Learning and Population from Text, P Buitelaar and P Cimiano (editors) IOS Press, "
"SSO, Lecture 1",stastics simulation and optimization,SSO,slides,data parameters lecture course sso sample density discrete random function,summarizing finish qq-plots normal variable values p(x mean summaries continuous,distributions distribution qq-plots probability population example expensescrime parameter relation scatter,,2771,43,Eduard Belitser,2019,"SSO, Lecture  SSO, Lecture  Eduard Belitser VU Amsterdam  course parameters distributions summarizing data QQ-plots to finish Overview      course parameters (meetings, assignments, etc) probability distribution (kansverdeling) summarizing data QQ-plots and exploring distributions (verdelingsonderzoek) demonstration RStudio (?) SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish course parameters SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Organisation Lecturers: Eduard Belitser (lectures -), Joost Berkhout (lectures -) Teaching assistants: ? Communication: all information is on Canvas Meetings: during  weeks, lectures twice a week, once a week laptop session (weeks , , , : homework exercises, weeks , , : assignments) Literature: lecture slides (available on Canvas), books for statistics part: Elementary Statistics, by M Triola, R-manual on Canvas Home work exercises: statistics exercises are from the Triola book Assignments:  assignments to be started in weeks ,  and , to be submitted via Canvas  one week later Software: R (wwwr-projectorg) and Rstudio (wwwrstudiocom) Grading:  assignments (%, to hand in via Canvas) + written exam (%) SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Overview of the statistics module        recap distributions, summarizing data, QQ-plots sampling distributions, estimating (and testing for) mean and proportion testing in two independent and paired samples, sign and Wilcoxon tests categorical data, contingency tables, chi-squared test linear regression (simple, multiple), problems with linear regression ANOVA/ANCOVA logistic regression SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish distributions SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Probability distribution: continuous or discrete A probability distribution P determines the probability of different outcomes of a random variable (stochastische variabele of stochast) Probability distributions come in two varieties: A discrete random variable has a finite or countable set of possible outcome values (eg, dice, coins, birthdays) A continuous random variable has an infinite set of possible outcome values (eg, temperature, length) SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Probability distribution A probability distribution P determines the probability of different outcomes of a random variable A distribution can be continuous or discrete, or a mix of both Examples of the probability density p (kansdichtheid of dichtheid) of some continuous distributions SSO −uniform(densityGamma(   /   course parameters distributions summarizing data QQ-plots to finish Probability of an outcome – continuous distribution The probability to have an outcome in some interval I is the area under the 􀀀 R density function p over that interval PI = I p(x)dx In the normal (,) situation: probability that observation is between - and  is: Z  􀀀 PX  (−, ) = p(x)dx = − −−−normal(SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Probabilities in R – continuous distribution R functions for distribution dist with parameters par: ddist(x,par) equals p(x) 􀀀 R u pdist(u,par) equals PX  u = p(x)dx − qdist(a,par) equals the value u such that pdist(u,par)=a for some  < a <  rdist(size,par) yields a random sample (steekproef) from dist with parameter par of size size Examples > pnorm(,mean=,sd=)-pnorm(-,mean=,sd=) []  > pnorm()-pnorm(-) []  > rnorm() []  - -  The default for norm (normal distribution) is μ mean=,  standard deviation sd= SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Probability density functions Continuous distributions in R with default parameter values: normal distribution norm with parameters μ mean= and  sd= (x−μ)  −   p(x)= p e   exponential distribution exp with parameter  lambda= p(x)= e−x x >  uniform distribution unif with parameters minimum min=a and maximum max=b of the support interval  p(x)= a  x  b b − a Gamma distribution gamma with parameters shape shape and rate rate= densityGamma(SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Probability of an outcome – discrete distribution For discrete distributions we have a probability mass function p p(x)= P(X = x) The probability to have an outcome in some set A is the sum 􀀀 X PX  A = p(x) xA Examples of discrete distributions are binomial and Poisson binomial(,) prob massbinomial(,) prob SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Probabilities in R – discrete distribution R functions for distribution dist with parameters par: ddist(x,par) equals p(x) 􀀀P pdist(v,par) equals PX  v)= xv p(x) qdist(a,par) equals approximately the value v such that pdist(v,par)=a for some  < a <  rdist(size,par) yields a random sample from dist with parameter par of size size Examples > dbinom(,size=,prob=) []  > pbinom(,size=,prob=) []  > dbinom(,size=,prob=)+dbinom(,size=,prob=) []  > rpois(,lambda=) []    SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Probability mass functions Discrete distributions in R: binomial distribution binom with parameters n size and p prob n! p(x)= x!(n − x)! px ( − p)n−x Poisson distribution pois with parameter  lambda p(x)=  x x ! e− SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Cumulative probability function The cumulative probability function of a distribution is R u continuous: P(u)= P(X  u)= p(x)dx = pdist(u,par) 􀀀P − discrete: P(u)= PX  u)= xu p(x)= pdist(u,par) cumnprobcumnprobcumnprob−−cumprob normal(,) xprob−cumprob uniform(,) xprobcumSSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Expectation The expectation or mean of a probability distribution is a location parameter For discrete random variable: X E(X)= xp(x) For continuous random variable: Z E(X)= xp(x)dx Examples P  =  Throwing dies: E(X)= xp(x)= ×  + + ×  RR (x−μ)  Normal distribution: E(X)= xp(x)dx = x p  e−   dx = = μ  SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Variance and standard deviation The variance of a probability distribution is a scale parameter For discrete random variable: X Var(X)= (x − E(X))p(x) For continuous random variable: Z Var(X)= (x − E(X))p(x)dx Examples Throwing dies: Var(X)= P (x −   )p(x) = ( −   ) ×  + + ( −   ) ×  = Normal distribution: RR (x−μ)   Var(X)= (x − μ)p(x)dx =(x − μ) p e−  dx = =   The standard deviation is the square root of the variance SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Expectation and variance – standard distributions expectation variance binomial (n,p) np np( − p) poisson () uniform (a,b) normal (μ,) exponential () a+b  μ (b−a)     SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish summarizing data SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Population and sample A population can be an actual population, eg the heights of all men in the Netherlands It can also be the (imaginary) infinite number of outcomes obtained by repeating an experiment over and over, eg, throwing a die many times A sample is a set of values (randomly) selected from a population The population has a certain distribution, called the population distribution From the sample we want to gain information about this unknown population distribution SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Types of data summaries A good summary of a data set shows the relevant information in a data set It can exist of numerical summaries (what it estimates/investigates) sample mean (population mean) sample median (population median) sample standard deviation (population standard deviation) sample variance (population variance) sample correlation(s) (population correlation(s)) graphical summaries histogram (probability density or probability mass) empirical distribution function (cumulative probability function) boxplot (assess symmetry) scatter plot(s) (assess relations) SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Numerical summaries Numerical summaries sample size n location mean x¯= n− Pn i= xi  x((n+)/), if n odd median med(x)=   (x(n/) + x(n/+)), if n even s Pn x) scale variance = n− i=(xi − ¯ p standard deviation s = s Interpretation of location measures: mean – average value median – middle value in sorted values Interpretation of location measures: variance – average squared deviation from mean standard deviation – square root of variance SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Example — incomes () Example data incomes of  white families and  colored families in US white     · · ·  color     · · ·  This concerns two univariate data sets (one measurement per ‘object’) — not one bivariate data set (two measurements per ‘object’) What are the interesting questions? SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Example — incomes () sample size mean median sd var min max white families        Numerical summaries sample size mean median sd var min max colored families        SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Histogram The histogram of a sample of observed values is a barplot, where the area of the bar over an interval (a, b) corresponds to the fraction number of observations in (a, b) sample size > x=rnorm() > par(mfrow=c(,)) > hist(x) > hist(x,prob=T) SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Histogram versus density () The histogram of a sample from distribution P varies around the density p The smaller the sample the bigger this variation SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Histogram versus density () For continuous distributions, the population density function is the smoothed histogram of the population values The resemblance between the true normal(,) density and the histogram of a sample of size  The population exists of infinitely many values xxDensity−− SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Example — incomes () Graphical summaries () >hist(incomes$white,main=""incomeswhitefamilies"",xlab=""income"") >hist(incomes$colored,main=""incomescoloredfamilies"",xlab=""income"") familiesincomeFrequency familiesincomeFrequency SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Boxplot and empirical density functions The boxplot of a sample of observed values is a box with whiskers and (possibly) extremes From a boxplot you can see the scale of the data, its symmetry and whether there are extremes In R: boxplot(x) The empirical distribution function of a sample of observed values is the cumulative histogram It resembles the cumulative distribution function of the underlying population In R: plot(ecdf(x)) SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Example — incomes () Graphical summaries () > boxplot(income,main=""boxplots for white and color"") > plot(ecdf(income[,]),col=""red"",xlab=""income"",main=""empirical dist functions"") > lines(ecdf(income[,]),col=""blue"") >legend(,,c(""white"",""color""),col=c(""red"",""blue""),lwd=c(,)) whitecolore+e+e+e+e+e+e+e+e+e+e+e+Empirical Distribution functionsincomewhitecolorSSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Example — expensescrime () An example of a multivariate data set, multiple measurements per ‘object’ The data expensescrime were obtained to determine factors related to state expenditures on fighting criminality (courts, police, etc) The variables are: state (indicating the state in the USA), expend (state expenditures on fighting criminality in $), bad (number of persons under judicial supervision), crime (crime rate per ), lawyers (number of lawyers in the state), employ (number of persons employed in the state) and pop (population of the state in ) > head(expensescrime) state expend bad crime lawyers employ pop  AK        AL        AR        AZ        CA        CO       Apart from numerical and graphical summaries of the columns separately, we can consider bivariate summaries to see the relation between pairs of columns SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Correlation and scatter plot () The correlation between two variables quantifies the linear relation between the two variables −−−−cor = variable variable −−−−−cor = −variable variable −Correlation values: +: perfect linear relation (straight line) with positive slope –: perfect linear relation (straight line) with negative slope : no linear relation (but maybe some other relation?!) SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Correlation and scatter plot () Example of two variables that have correlation close to , but a clear relation: −−cor variable  Such a figure is called a scatter plot of variable  (vertical) versus variable  (horizontal) Complement graphical summaries with numerical summaries and vice versa SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Example — expensescrime () The correlation between all pairs of variables, excluding the first column of state names: > round(cor(expensescrime[,-]),) expend bad crime lawyers employ pop expend       bad       crime       lawyers       employ       pop       Ingredients of R-code: expensescrime[,-] removes column  from expensescrime cor(expensescrime[,-]) produces pairwise correlations between remaining columns round(cor(expensescrime[,-]),) rounds the numbers to  decimals SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Example — expensescrime () The scatter plots of all pairs of variables, excluding the first column of state names: > pairs(expensescrime[,-]) expendbadcrimelawyersemploypop SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Example — expensescrime () The scatter plots of only the variables expend, crime, employ, pop: SSO Lecture   /  >pairs(expensescrime[,c(,,,)]) expendcrimeemploypopexpensescrime[,c(,,,)]selects columns , ,  and   course parameters distributions summarizing data QQ-plots to finish QQ-plots and exploring distributions SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Goal of exploring distributions and scale If we know or may assume that a sample comes from some (standard) distribution, perhaps with unknown parameter(s), then the summary could be, eg, the data follow a normal distribution, and we then estimate/test the parameters μ and  we know whether statistical methods that require a normal distribution of the data are applicable We will only discuss investigating normality, ie, whether the population distribution is normal SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish All normal distributions are similar Two normal distributions with different μ (mean) and  (standard deviation) are similar in the following way −normal(,) and normal(,) density(,) (,) −−−normal(density−All normal distributions have the same bell shape The normal density function is  e−  (x−μ)/ pμ,(x)= p  SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish QQ-plots A normal QQ-plot (or normal probability plot) can reveal whether data (approximately) follows a normal distribution For a sample of size n it plots the ordered data x(), x(),, x(n) with x()  x()   x(n) versus the values n,, n,,, n,n that are typical for ordered values from a normal population A fraction of i/n of the population is smaller than the i/n-quantile n,i If the points are approximately on a straight line, then the data can be assumed to be sampled from a normal population with some values μ and , which need to be estimated (Pay special attention to the corners!) SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish Examples Some QQ-plots (qqnorm) of the income data and the expensescrime data: whiteFrequencye+expendFrequencyexpensescrime$crimeFrequency−−e+e+e+qqnorm(income$white) Theoretical QuantilesSample Quantiles−−qqnorm(expensescrime$Theoretical QuantilesSample Quantiles−SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish to finish SSO Lecture   /   course parameters distributions summarizing data QQ-plots to finish To wrap up Today we discussed probability distributions summarizing data QQ-plots In the first assignment you will practice these topics Make neat and concise reports of the assignments Next time estimation and statistical tests SSO Lecture   /  "
"SSO, Lecture 2",stastics simulation and optimization,SSO,slides,sample lecture confidence interval estimation p-value level trains normal versus,mean testing sso parameters test data amsterdam example statistic proportion,distribution hypothesis finish estimation women size population value deviation probability,,2650,31,Eduard Belitser,2019,"SSO, Lecture  SSO, Lecture  Eduard Belitser VU Amsterdam  distribution sample mean parameters estimation hypothesis testing to finish Overview    distribution of sample mean estimation mean proportion hypothesis testing mean proportion SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish distribution of the sample mean SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish The sample mean and its distribution The sample mean of a sample X,, Xn of sample size n is X = ¯ X  n Xi n i= We can consider the sampling distribution of the sample mean When the sample is taken from the normal(μ,  X has exactly the normal(μ, /n) distribution ¯ ) distribution, then the sample  mean ¯X hasapproximately thenormal(μ, When the sample is taken from some other distribution with expectation μ and variance   , then the sample mean  /n) distribution because of the Central Limit Theorem In both cases the mean varies less than the individual observations: the p standard deviation  is replaced by / n SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Examples of sample mean Examples of distributions of X (black) and distribution of sample mean for sample sizes n = , n =  and n =  −−−normal(exp(poisson(The larger the sample size, the lower the variance in the distribution of the sample mean SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Standardizing the mean A normally distributed random variable X with mean μ and standard deviation  can be standardized into a standard N(, ) variable by X − μ Z =  If X  N(μ, ) then Z  N(, ) This is just applying the right scaling and shifting as in slide  of last week (“All normal distributions are similar”) Standardizing the sample mean yields p X¯ − μ n(X¯ − μ) Z = p =  N(, ) / n  SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish The t-distribution In a real data set X,, Xn the population standard deviation  is unknown and needs to be estimated by the sample standard deviation s This uncertainty influences the distribution of the sample mean The random variable ¯ X − μ p s/ n does not have the normal N(, ) distribution Instead, it has a t-distribution with n −  degrees of freedom SSO / Lecture   distribution sample mean parameters estimation hypothesis testing to finish estimation SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Estimation – the concept Suppose we assume that our population of interest has a certain distribution with an unknown parameter, eg its mean μ or a fraction p A point estimate (puntschatting) for the unknown parameter is a function of (only) the observed data (x,, xn) We denote estimators by a hat: μˆ, pˆ, etc Examples: ¯ μˆ= X pˆ is the sample proportion A confidence interval (betrouwbaarheidsinterval) for the unknown parameter is an interval based on (only) the observed data (x,, xn) that contains the true value of the parameter with a certain level of confidence SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Estimating the mean () For given random variables X,, Xn from the normal(μ, ) distribution, we have that X¯  N(μ, /n) Define the following (upper) quantile of the standard normal distribution: z =qnorm(-alpha), such that P(Z  z ) = for Z  N(, ) Then we have the following probability: 􀀀   − = P μ − z / p X¯  μ + z / p nn 􀀀   = P − z / p X¯ − μ  z / p nn 􀀀  = P − z / p μ − X¯  z / p nn 􀀀¯   = PX − z / p μ  X¯ + z / p nn SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Estimating the mean () We ended up with X − z /    μ  X¯ + z /   − = P p n p n In other words: the probability that the true mean μ lies in the interval  pn [X¯ − z /  ¯ X + z / ] p n , equals  − We denote such a confidence interval as: ¯ X ± z /  p n The confidence level of this interval is  − SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Estimating the mean () If the standard deviation  is unknown, we estimate it by s and the confidence interval is based on a t-distribution and t-quantile t =qt(-alpha,df=n-) The t confidence interval for μ is: s ¯ X ± t / p n The confidence level of this interval is  − Remark In real data sets this interval is (nearly) always used, since it does not make the assumption of known  SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – heights of women () Consider the data of heights (in cm) of  women womenwomenFrequency−−−Normal QuantilesSample Quantiles > women=scan(""lengthswomentxt"") Read  items > par(mfrow=c(,)) > hist(women) > qqnorm(women) > n=length(women) >n []  > m=mean(women) >m []  > s=sd(women) >s []  > t=qt(,df=n-) >t []  > c(m-t*s/sqrt(n),m+t*s/sqrt(n)) []   SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – heights of women () We used /=, so the confidence level is  − = −  =  We derived the following % confidence interval for the mean height of women: [, ] cm SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Margin of error for the mean The ( − )-confidence interval for μ ¯X ± z / pn or s¯X ± t / p n The margin of error is  z / pn or st / p n Note  If we take larger n, the confidence interval will be smaller (shorter) In other words: we gain more accuracy at the same confidence level Note  If  (or s) is smaller, the confidence interval will be smaller (shorter) In other words: this yields more accuracy at the same confidence level Note  If we take bigger , the confidence interval will be smaller (shorter) Be careful: more accuracy at the cost of a lower confidence level SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Minimal sample size Question How big should the sample size be in order to obtain a margin of error of E? Answer We need to solve n from  s z / p = E or t / p = E nn This yields p z / p t /s n = or n = EE (z /) (t /)s n = or n = E E Remark For large n we have t /  z / and s   SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – heights of women () Question How big should the sample size in the women heights data be to obtain E= mm at a confidence level of %? Answer We have E =cm,   , z / = which yields ()() n = =   In words: we should include at least  women to have a confidence interval of length at most cm (note that the interval length is  × E) SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Estimating a proportion Suppose we want to estimate a population proportion p, based on a sample The point estimate for p will be the sample proportion pˆ Write q = − p and qˆ= − pˆ The confidence interval for p with confidence level  − is given by r pˆqˆ pˆ ± z / n (This is based on the normal approximation of the binomial distribution, Sec ) The margin of error and minimal sample size are therefore r pˆqˆ z /pˆqˆ E = z / and n = nE SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – trains in time Question Suppose we want to take a sample amongst trains of NS to estimate the fraction p of trains that arrive in time In august  this fraction was  (according to wwwnsnl) We want to set up a % confidence interval for p with length at most % How many trains should we include in the sample? Answer A length of % means  × E = and E = Furthermore pˆ=, so qˆ= For a % interval we have z / = qnorm()= The minimal sample size is z /pˆqˆ () ×  ×  n = = =  E () which is found in R by qnorm()ˆ**/()ˆ In words: we should include at least  trains to have a % confidence interval of length at most % SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish hypothesis testing SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Testing – the concept () In a hypothesis test or test of significance (hypothese toets), we have two claims, the null hypothesis H and the alternative hypothesis H, which do not overlap The goal is to draw some conclusion about the correctness of the two claims The claim of interest is usually represented by H A test has two possible outcomes: the strong outcome: H is rejected, and H is assumed to be true the weak outcome: H is not rejected, (H isn’t either!) Example H: The mean income of inhabitants of Amsterdam is lower than e (the mean income of the Netherlands) H: the mean income of inhabitants of Amsterdam is higher than e SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Testing – the concept () In order to perform the test, one needs a test statistic, which summarizes the data in a relevant way The H is rejected if the value of the test statistic is too far away from what is expected under the H Example (incomes Amsterdam) Given a sample of incomes in Amsterdam a sensible test statistic would be the sample mean The value of the test statistic is converted into a p-value The p-value expresses the probability of getting a value of the test statistic at least as extreme as in the data, assuming that H is true When this p-value is below the chosen significance level (eg = ) the outcome is: reject H (strong outcome) If the p-value is bigger, then we do not reject H (weak outcome) SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – Amsterdam incomes () (Fictive) data on  incomes in Amsterdam amsterdamamsterdamFrequency−QuantilesSample Quantiles Assuming that the underlying distribution of incomes is normal (μ,) seems ok for this dataset We take  unknown We test H : μ  μ =  versus H : μ>μ =  The value of X¯ is relevant for H, so base test statistic on X¯ SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – Amsterdam incomes () The test statistic is ¯ X − μ T = p s/ n which has the t-distribution with n −  degrees of freedom, at the border of H and H, ie for μ = μ (slide ) > m=mean(amsterdam) > s=sd(amsterdam) > n=length(amsterdam) > t=(m-)/(s/sqrt(n)) >t []  > -pt(,df=n-) []  The p-value is  Conclusion? −−−p−SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish The t-test The t-test is for testing the population mean μ of a normal population i H : μ  μ versus H : μ>μ (ttest(data,mu=μ,alt=""g"")) ii H : μ  μ versus H : μ<μ (ttest(data,mu=μ,alt=""l"")) iii H : μ = μ versus H : μ = μ (ttest(data,mu=μ)) In all  cases the test statistic is ¯ X − μ T = p s/ n which has the t-distribution with n −  degrees of freedom, at the border of H and H (ie for μ = μ) The p-value for observed value t of test statistic is i p = P(T  t) under H ii p = P(T  t) under H 􀀀 iii p = × min P(T  t), P(T  t) under H SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – Amsterdam incomes () The t-test on the Amsterdam data in R, using ttest: > ttest(amsterdam,mu=,alt=""g"") One Sample t-test data: amsterdam t = , df = , p-value =  alternative hypothesis: true mean is greater than   percent confidence interval:  Inf sample estimates: mean of x  Confidence interval [, +) is also given in the above output But why is Inf in it? SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – trains in Amsterdam () In a (fictive) sample amongst  trains arriving at Amsterdam Central station we measure a sample proportion of pˆ= (/) trains arriving in time We want to test whether this is significantly lower than the reported % for The Netherlands: H : p   versus H : p <  This is a binomial sample with n =  and p unknown One can find the exact p-value, assuming p =, from the binomial(,) Alternatively, one can use the normal approximation to the binomial distribution Here the p-value is for finding at most  out of  > pbinom(,size=,p=) []  Conclusion? SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish The binomial test The binomial test is for testing the population proportion p i H : p  p versus H : p > p (binomtest(number,n,p=p,alt=""g"")) ii H : p  p versus H : p < p (binomtest(number,n,p=p,alt=""l"")) iii H : p = p versus H : p = p (binomtest(number,n,p=p)) In all  cases the test statistic is the number of “successes” in the sample, which has a binomial(n, p) distribution at the border of H and H (ie for p = p) As for the t-test, the p-value is calculated one-sided or two-sided SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish Example – trains in Amsterdam () > binomtest(,n=,p=,alt=""l"") Exact binomial test data:  and  number of successes = , number of trials = , p-value =  alternative hypothesis: true probability of success is less than   percent confidence interval:   sample estimates: probability of success  SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish to finish SSO Lecture   /   distribution sample mean parameters estimation hypothesis testing to finish To wrap up Today we discussed distribution of sample mean estimation hypothesis testing Next time more on statistical tests (two sample tests, nonparametric tests) SSO Lecture   /  "
"SSO, Lecture 3",stastics simulation and optimization,SSO,slides,two test means one shapiro-wilk sso p-value distribution true statistic,"tests sample data wrap hypothesis amsterdam conclusion confidence estimates trains",proportions nonparametric lecture wrap example samples t-test mean differences normal,,3459,40,Eduard Belitser,2019,"SSO, Lecture  SSO, Lecture  Eduard Belitser VU Amsterdam  one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Overview     recap one sample tests Shapiro-Wilk test two samples tests two means (independent samples) two means (matched pairs) two proportions nonparametric tests sign test Wilcoxon signed-rank test Wilcoxon rank-sum test SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up recap: t-test and binomial test SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Recap tests and p-values A test of significance yields a conclusion about the null hypothesis H versus the alternative hypothesis H The H is either rejected or not The conclusion of the test is based on the p-value, expressing the likelihood of the observed data under the H If the p-value is lower than a certain threshold (the significance level of the test), then H is rejected The p-value is calculated from a test statistic T, which summarizes the data in a (for the test) relevant way In order to compute the p-value, one needs to know the distribution of T under H The p-value can be either one-sided or two-sided: pright = P(T  t) under H pleft = P(T  t) under H 􀀀 ptwo−sided = × min pleft, pright under H SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Type I and type II error A hypothesis test has two possible outcomes: reject H or do not reject H Therefore, one can make two types of errors: Type I error reject H while it is true Type II error not reject H while it is false Which error is worse? In a type I error the conclusion is really wrong In a type II error there is no conclusion, whereas we could have drawn one The significance level of a test limits the probability of a type I error to A test has high power if the probability of a type II error is small The sample size influences the power: higher sample size yields higher power Asymmetric treatment of the errors: rejecting H is a strong conclusion, so the claim of interest is usually represented by H SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Recap t-test for the mean of one sample Setting: a sample X,, Xn  N(μ, ), test about the mean μ () () = = Hypotheses: H : μ  μ versus H : μ>μ  < Test statistic: T = Xs/ − pμ n  Distribution of T under H: t-distribution with n −  degrees of freedom In R: ttest(data,mu=μ,alt=) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Recap binomial and (appr) normal tests for a proportion Setting: X  Bin(n, p), eg, the number of successes in n trials, p is the success proportion (or the probability of success) We want to test about p () () = = Hypotheses: H : p  p versus H : p > p  < pˆ−p Test statistic: X or T = p , where pˆ= X/n p(−p)/n Distribution under H: X  Bin(n, p) (exactly) or T  N(, ) (approx) In R: binomtest(number,n,p=p,alt=) proptest(number,n,p=p,alt=) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – trains on time () We test whether the “on-time fraction” amongst trains arriving in Amsterdam is % In our (fictive) sample amongst  trains  trains are on time The exact binomial test: > binomtest(,,p=) Exact binomial test data:  and  number of successes = , number of trials = , p-value =  alternative hypothesis: true probability of success is not equal to   percent confidence interval:   sample estimates: probability of success  SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – trains on time () The approximate test: > proptest(,,p=) -sample proportions test with continuity correction data:  out of , null probability  X-squared = , df = , p-value =  alternative hypothesis: true p is not equal to   percent confidence interval:   sample estimates: p  The p-values in both tests are smaller than  (but different), and the conclusion is the same: reject H SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – trains on time () The influence of the sample size: if we had found  trains arriving in time amongst  trains: > binomtest(,,p=) Exact binomial test data:  and  number of successes = , number of trials = , p-value = e- > proptest(,,p=) -sample proportions test with continuity correction data:  out of , null probability  X-squared = , df = , p-value < e- e- = − =, e-= Finding the same deviation from H in more data is more convincing, ie yields a lower p-value SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Shapiro-Wilk test for normality SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Shapiro Wilk test for normality Setting: A sample X,, Xn from an unkown distribution P Hypotheses: H : P is a normal distribution versus H : P is not a normal Test statistic: with certain constants a,, an, 􀀀Pn  i= ai X(i) W = Pn  (, ] i=(Xi − X) Distribution of W under H: known, but complicated to write down H is rejected for “small” values of W It is always the left-sided test In R: shapirotest(x) Note: this test complements the graphical check by a normal QQ-plot SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up two means: independent or matched samples SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes of Amsterdam and Rotterdam () (Fictive) data on  incomes in Amsterdam and  incomes in Rotterdam amsterdamrotterdam−−QQplot amsterdamTheoretical QuantilesSample Quantiles−Question: is the mean income the same in Amsterdam and Rotterdam? Remark This is a fictive data set, real incomes are not symmetrically distributed SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes of Amsterdam and Rotterdam () Compare sample means and standard deviations: > mean(amsterdam) []  > sd(amsterdam) []  > length(amsterdam) []  > mean(rotterdam) []  > sd(rotterdam) []  > length(rotterdam) []  > shapirotest(amsterdam) Shapiro-Wilk normality test data: amsterdam W = , p-value =  > shapirotest(rotterdam) Shapiro-Wilk normality test data: rotterdam W = , p-value =  We will use the t-test for testing the difference in means for two independent samples SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up t-test for two means of two independent samples Setting: Two samples: X,, Xn  N(μ,) and Y,, Yn  N(μ,) We want to test about the difference in mean μ − μ () () = = Hypotheses: H : μ − μ   versus H : μ − μ >   < X−Y¯ Test statistic: T = q ¯ s s  + n n Distribution of T under H: approx t-distribution with df (R computes df approximately) degrees of freedom In R: ttest(data,data,mu=,alt=) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes of Amsterdam and Rotterdam () The t-test for two independent samples (Amsterdam and Rotterdam): > ttest(amsterdam,rotterdam) Welch Two Sample t-test data: amsterdam and rotterdam t = , df = , p-value =  alternative hypothesis: true difference in means is not equal to   percent confidence interval:   sample estimates: mean ofxmeanof y   Conclusion? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up t-test for two means of two independent samples, = Setting: Two samples: X,, Xn N(μ,) and Y,, Yn  N(μ,) We want to test about the difference in mean μ − μ Assumption:  =  () () = = Hypotheses: H : μ − μ   versus H : μ − μ >   < PP n n (Xi −X¯ )+ (Yj−Y¯ ) X−Y¯ i= j= Test statistic: T = p ¯ , where s = is the + sn+n− n n pooled sample variance Distribution of T under H: t-distribution with n + n −  df (exactly) In R: ttest(data,data,mu=,alt=,varequal=TRUE) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes of Amsterdam and Rotterdam () The t-test for two independent samples (Amsterdam and Rotterdam), assuming equal variances: > sd(amsterdam) []  > sd(rotterdam) []  > ttest(amsterdam,rotterdam,varequal=TRUE) Two Sample t-test data: amsterdam and rotterdam t = , df = , p-value =  alternative hypothesis: true difference in means is not equal to   percent confidence interval:   sample estimates: mean ofxmeanof y   Conclusion? For large samples there is usually no big difference between these two tests (with unequal or equal variances) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes of tax couples () (Fictive) data on incomes of  tax couples in Utrecht (couple=man+woman, tax partners) manwomanmanwoman Question: is there a difference in mean income for men and women within tax couples? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes of tax couples () We need to look at differences within pairs > d=man-woman > plot(man,woman,ylim=c(,),xlim=c(,)) > abline(,,col=""red"",lwd=) > hist(d,main=""man-woman within couples"",xlab=""difference income"") manwomanman−incomeFrequency−SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up t-test for means of matched pairs Setting: One sample (X, Y),, (Xn, Yn) of n matched pairs The differences Xi − Yi are assumed to come from N(μd,) distribution We want to test about the mean of the differences μd () () = = Hypotheses: H : μd   versus H : μd >   < Test statistic: T = d p , where d the sample mean of differences, and sd the sd / n sample sd of differences Distribution of T under H: t-distribution with n −  df (exactly) In R: ttest(data,data,mu=,alt=,paired=TRUE) or ttest(data-data,mu=,alt=) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes of tax couples () Investigate normality of the differences within pairs and apply t-test to differences −−−Normal QuantilesSample Quantiles > ttest(man,woman,paired=TRUE) Paired t-test data: man and woman t = , df = , p-value = e- alternative hypothesis: true difference in  percent confidence interval:   sample estimates: mean of the differences  > ttest(d) One Sample t-test Conclusion? data: d t = , df = , p-value = e- means is not equal to  alternative hypothesis: true mean is not equal to   percent confidence interval:   sample estimates: mean of x  SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up two proportions SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – fraud fractions Utrecht and Den Haag () We test whether the fraud fraction amongst bijstand (welfare) clients is the same in Utrecht and Den Haag In a (fictive) sample amongst  bijstand clients in Utrecht we find  fraud cases and amonst  bijstand clients in Den Haag we find  fraud cases The sample fractions are pˆutrecht = / =  and pˆdenhaag = / =  Question: is there a significant difference in fraud proportion between Utrecht and Den Haag? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Testing two proportions Setting: X successes in a sample of size n taken from population  and X successes in a sample of size n from population  We want to test about the difference in population success proportion p and p () () = = Hypotheses: H : p − p   versus H : p − p >   < p−pˆ X X x+x Test statistic: T = q ˆ􀀀, where pˆ = n , pˆ = n , p¯= n+n is the +  p¯q¯ n n pooled sample fraction and q¯= − p¯ Distribution of T under H: N(, ) (approximately) In R: proptest(c(x,x),c(n,n),alt=) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – fraud fractions Utrecht and Den Haag () We apply the approximate proportion test: > proptest(c(,),c(,)) -sample test for equality of proportions with continuity correction data: c(, ) out of c(, ) X-squared = , df = , p-value =  alternative hypothesis: twosided  percent confidence interval: -  sample estimates: prop  prop    Conclusion? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – fraud fractions Utrecht and Den Haag () Suppose we found the same sample proportions in larger samples per city: > proptest(c(,),c(,)) -sample test for equality of proportions with continuity correction data: c(, ) out of c(, ) X-squared = , df = , p-value = e- alternative hypothesis: twosided  percent confidence interval:   sample estimates: prop  prop    Now we do reject H : putrecht = pdenhaag Why? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up nonparametric tests SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Nonparametric tests – the concept So far, in all tests we looked at the sample average which is approximately normally distributed, for, say, n >  Question: what if the data and/or test statistic are not (approx) normally distributed? Then we need a test that does not assume normality (or even any other particular distribution) of the data Nonparametric tests are valid (ie, yield reliable p-values) for a broad class of distributions of the data SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – parcels () PostNL delivered  million parcels in  Assume we are given the (fictive) dataset on total daily weights of parcels handled by PostNL on Mondays and Saturdays for all  weeks in , and we want to investigate whether there is a difference between these two week days > head(parcels) monday saturday                   > tail(parcels) monday saturday                   Histogram of saturdayweightFrequencye+e+e+ SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – parcels () The distribution of the weekly differences (monday-saturday): differencesdifferenceFrequency−−QuantilesSample Quantiles It seems to deviate a bit from normal, the Shapiro-Wilk test yields p-value =  (reject H of normality) We will not use the t-test, but test for the median of the differences instead SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up The sign test () Setting: A sample X,, Xn from some population We want to test about the population median m Hypotheses: H : m = m versus H : m = m 􀀀 Test statistic: T =# i : Xi < m , where “#” means “the number of” Distribution of T under H: Bin(n,  ) distribution (exactly), an approximation by a normal distribution is possible In R: binomtest(t,n,p=,alt=) Setting: A sample (X, Y) , (Xn, Yn) of matched pairs from some population We want to test whether the median m of the differences is  Hypotheses: H : m =  versus H : m = 􀀀 Test statistic: T =# i : Xi < Yi Distribution of T under H: Bin(n,  ) (exactly), an approximation by a normal distribution is possible In R: binomtest(t,n,p=,alt=) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – parcels () The sign test on the matched pairs of the parcel data: > d=monday-saturday > t=sum(d<) >t []  > length(d) []  > binomtest(,) Exact binomial test data:  and  number of successes = , number of trials = , p-value =  alternative hypothesis: true probability of success is not equal to   percent confidence interval:   sample estimates: probability of success  Conclusion? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up The signed rank test Setting: A sample X,, Xn from some symmetric population We want to test about the population median m Hypotheses: H : m = m versus H : m = m P Test statistic: T = Ri , which is the sum of the ranks of |Xi − m| of i:Xi >mthe observations Xi > m Large values of T indicate that m > m Distribution of T under H: is known in R For larger n an approximation by a normal distribution is used Depending on H, one-sided or two-sided test In R: wilcoxtest(data,mu=m,alt=) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – grades Given a (real!) dataset on statistics grades of  randomly chosen students > grades=c(,,,,,,,,,,,,) > sort(grades) []              Question: are the grades symmetrically distributed around m = ? > wilcoxtest(grades,mu=) Wilcoxon signed rank test data: grades V = , p-value =  alternative hypothesis: true location is not equal to  Conclusion? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes white and colored () whitecolore+e+e+e+e+e+ Are the locations of the two income populations significantly different? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up The rank sum test or Mann-Whitney test Setting: Two samples X,, Xn and Y,, Yn from two populations We want to test whether the populations are “identical” (actually, we only test med(X) = med(Y )) Hypotheses: H : med(X) = med(Y ) versus H : med(X) = med(Y ) Pn Test statistic: T = i= Ri , which is the sum of the ranks of the X’s in the combined sample Distribution of T under H: is known in R For larger n an approximation by a normal distribution is used In R: wilcoxtest(data,data,alt=) SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up Example – incomes white and colored () The Mann-Whitney U test or Wilcoxon two sample test applied on the income data: > wilcoxtest(income[,],income[,]) Wilcoxon rank sum test with continuity correction data: income[, ] and income[, ] W = , p-value =  alternative hypothesis: true location shift is not equal to  Conclusion? SSO Lecture   /   one sample tests Shapiro-Wilk two means two proportions nonparametric tests To wrap up To wrap up Today we discussed Shapiro-Wilk test two samples tests two means (independent samples) two means (matched pairs) two proportions nonparametric tests sign test Wilcoxon signed-rank test Wilcoxon rank-sum test Next time categorical data, linear regression SSO Lecture   /  "
"SSO, Lecture 4",stastics simulation and optimization,SSO,slides,contingency sso finish variable p-value exact degrees least correlation model,linear lecture test rate row teen statistic teenage week categories,distribution regression tables data hours mortality column sample example eij,,2904,37,Eduard Belitser,2019,"SSO, Lecture  SSO, Lecture  Eduard Belitser VU Amsterdam  -distribution contingency tables linear regression to finish Overview     distribution contingency tables,  test, Fisher’s test simple linear regression SSO Lecture   /   -distribution contingency tables linear regression to finish chisquare distribution SSO Lecture   /   -distribution contingency tables linear regression to finish The -distribution Suppose that Z,, Zn  N(, ), and are independent Then the sum n X Z Y = i  n, i= ie, Y has a -distribution (chi-kwadraat verdeling) with n degrees of freedom some chi^ densitiesdf=df=df=some chi^SSO Lecture   /   -distribution contingency tables linear regression to finish Properties of -distributions  k distributions are asymmetric “live” only on positive values have different shapes for each value of k QQ-plots can not be used in the same way as they are used to check normality For each k, a different QQ-plot would be necessary  If Y   (ie, a random variable Y has  -distribution), then k k E(Y )= k and Var(Y )=k (see previous slide) Note : the  distribution is the exponential distribution with  =   Withincreasing k the k distribution moves to the right and becomes wider Note : the Central Limit Theorem applies: for large k the  k distribution can be approximated by the N(k, k) distribution SSO Lecture   /   -distribution contingency tables linear regression to finish The -distribution in R In R we have the following functions for the k  distribution: dchisq(x,df=k) pchisq(x,df=k) qchisq(a,df=k) rchisq(size,df=k) > pchisq(,df=) []  > qchisq(,df=) []  SSO Lecture   /   -distribution contingency tables linear regression to finish contingency tables SSO Lecture   /   -distribution contingency tables linear regression to finish Study and gender (); success rate in statistics () ) Consider the following (fictive) data of a sample amongst  VU-students (numbers given are counts): exact arts total men    women    total    Question: are kind of study and gender independent? ) Consider the following (fictive) data on success in statistics courses amongst three subpopulations of students (numbers given are counts): passed failed total  hours a week     hours a week     hours a week    total    Question: is passing rate the same for each subpopulation? SSO Lecture   /   -distribution contingency tables linear regression to finish General contingency table The general form of a contingency table, with row variable with I categories and column variable with J categories: o, o, · · · o,J o,· o, o, · · · o,J o,· oI, oI, · · · oI,J oI,· o·, o·, · · · o·,J o·,· SSO Lecture   /   -distribution contingency tables linear regression to finish Independence versus homogeneity Testing independence Take one large sample (cf students data) and test the null hypothesis: H : row variable and column variable are independent Rejecting H means there is a dependence between row and column variable Testing homogeneity Take samples from c populations, that is, one sample per column, (cf succes rate statistics data) and test the null hypothesis: H : the c distributions over row factors are equal Rejecting H means that the distribution over rows varies from column to column Note: homogeneity between rows can also be tested (swap rows and columns) SSO Lecture   /   -distribution contingency tables linear regression to finish The test statistic The test statistic is based on the difference between what is expected count under H (E) and what is observed count (O or N) in each cell of the table Expectation in the example data sets: exact arts total men ? ?  women ? ?  total    passed failed total  hours a week ? ?   hours a week ? ?   hours a week ? ?  total    For each cell (i, j) in row i and column j, the corresponding term in the test statistic is (Oij − Eij) Eij SSO Lecture   /   -distribution contingency tables linear regression to finish The  test for independence Setting one sample, categorized into I categories of a row variable and J categories of a column variable Hypotheses: H : the row variable and column variable are independent versus H : the row variable and column variable are dependent Test statistic: IJ XX (Oij − Eij) X = , Eij i= j= Oi· O·j Oi·O·j where Eij = npij = npi·p·j = n = is the expected count in cell ij nn n Distribution of X under H: -distribution with (I − )(J − ) degrees of freedom (approximately) Condition: At least % of the Eij ’s should be at least  p-value: The p-value is always right-sided: pright = P(X > x) Why? In R: chisqtest(data) SSO Lecture   /   -distribution contingency tables linear regression to finish Example – Study and gender () Performing the test in R using chisqtest Have a close look at how to set up the table in R (it should be a dataframe) > x=asdataframe(matrix(c(,,,),ncol=,nrow=)) > dimnames(x)=list(c(""men"",""women""),c(""exact"",""arts"")) >x exact arts men   women   > chisqtest(x) Pearson’s Chi-squared test with Yates’ continuity correction data: x X-squared = , df = , p-value =  Conclusion? SSO Lecture   /   -distribution contingency tables linear regression to finish The  test for homogeneity Setting: samples from J different populations, categorized into I categories of some row variable Hypotheses: H : the distribution amongst categories of row variable is the same for each column versus H : the distribution amongst categories of row variable is not the same for each column Test statistic: IJ X XX (Oij − Eij) Oi·O·j = , with Eij = Eij n i= j= Distribution of X under H: -distribution with (I − )(J − ) degrees of freedom (approximately) Condition: At least % of the Eij ’s should be at least  p-value: The p-value is always right-sided: pright = P(X > x) Why? In R: chisqtest(data) SSO Lecture   /   -distribution contingency tables linear regression to finish Example – Succes rate in statistics () Performing the test in R using chisqtest Have a close look at how to set up the table in R (it should be a dataframe) > x=asdataframe(matrix(c(,,,,,),ncol=,nrow=, byrow=TRUE)) > dimnames(x)=list(c("" hours"","" hours"","" hours""),c(""passed"",""failed"")) >x passed failed  hours    hours    hours   > z=chisqtest(x) > z Pearson’s Chi-squared test data: x X-squared = , df = , p-value =  Conclusion? SSO Lecture   /   -distribution contingency tables linear regression to finish Example – Succes rate in statistics () Checking the condition and more information from the ouput of chisqtest > attributes(z) $names [] ""statistic"" ""parameter"" ""pvalue"" ""method"" ""dataname"" ""observed"" [] ""expected"" ""residuals"" ""stdres"" > z$expected passed failed  hours    hours    hours   > z$observed passed failed  hours    hours    hours   > X=sum((z$observed-z$expected)^/z$expected) > X []  > -pchisq(X,df=(-)*(-)) []  SSO Lecture   /   -distribution contingency tables linear regression to finish After rejecting H Suppose you have rejected H (independence of homogeneity) Where do the numbers deviate from what is expected under H? We can look at the standardized residuals using chisqtest(data)$stdres to determine which observed values deviate most from the expected under H > z$stdres passed failed  hours -   hours -   hours  - > qnorm() []  So, the biggest contribution to rejecting H is due to the cells pass/fail in the group  hours Alternatively, one can look at the (square root) contributions of each cell to the chi-squared statistics > (z$observed-z$expected)/sqrt(z$expected) SSO Lecture   /   -distribution contingency tables linear regression to finish What if the condition is not fulfilled? () If the condition At least % of the Eij’s should be at least  does not hold, R yields a warning One should not ignore this warning! Example > x=asdataframe(matrix(c(,,,,,),ncol=,nrow=)) > dimnames(x)=list(c(""A"",""A""),c(""B"",""B"",""B"")) >x BB B A    A    > chisqtest(x) Pearson’s Chi-squared test data: x X-squared = , df = , p-value =  Warning message: In chisqtest(x): chi-squared approximation may be incorrect SSO Lecture   /   -distribution contingency tables linear regression to finish What if the condition is not fulfilled? () In such a case one can use chisqtest(data,simulatepvalue=TRUE) Then R computes a p-value in a bootstrap fashion (a simulated p-value), that does not rely on the -approximation > z=chisqtest(x) > z$expected B B B A    A    > chisqtest(x,simulatepvalue=TRUE) Pearson’s Chi-squared test with simulated p-value (based on  replicates) data: x X-squared = , df = NA, p-value =  This may yield a very different p value SSO Lecture   /   -distribution contingency tables linear regression to finish Fisher’s exact test for x-tables For x-tables it is possible to compute an exact p-value, that does not use approximation or simulation This is called Fisher’s exact test Data on right-and left-handed people, classified according to gender > handed=matrix(c(,,,),nrow=,ncol=,byrow=TRUE, + dimnames=list(c(""right-handed"",""other""),c(""men"",""women""))) > handed men women right-handed   left-handed   We can compare this to picking without replacement  balls from a vase which contains  balls,  white and  red The number of white balls amongst the picked  balls is n, =  n,      =) The number n, determines all other numbers Fisher’s exact test is based on this number Under the null hypothesis of no dependence between the two factors it has a hypergeometric distribution n,  − n,  − n,  − ( − n,) SSO Lecture   /   -distribution contingency tables linear regression to finish Fisher’s test — testing in R > fishertest(handed) Fisher’s Exact Test for Count Data data: handed p-value =  alternative hypothesis: true odds ratio is not equal to   percent confidence interval:   sample estimates: odds ratio  > chisqtest(handed) Pearson’s Chi-squared test with Yates’ continuity correction data: handed X-squared = , df = , p-value =  (The chisquare approximation is also fine for these data) SSO Lecture   /   -distribution contingency tables linear regression to finish linear regression SSO Lecture   /   -distribution contingency tables linear regression to finish Example -Mortality and teenage mothers () The data on teenage birth rate per  and the infant mortality rate per  live births of the US (see assignment ) mortality rate teenagersmortality rate > teen=mortality$teen > mort=mortality$mort > cor(teen,mort) []  Question Does mortality rate significantly increase with teenager birth rate? SSO Lecture   /   -distribution contingency tables linear regression to finish The correlation test Setting: A bivariate sample (X, Y),, (Xn, Yn) from some population We want to test about the correlation  between the variables X and Y Hypotheses: H :  =  versus H :  = Test statistic: r T = q , −r n− where r is the sample correlation coefficient Distribution of T under H: t-distribution with n −  degrees of freedom (approximately) Usually the two-sided test is considered In R: cortest(x,y) SSO Lecture   /   -distribution contingency tables linear regression to finish Example -Mortality and teenage mothers () The correlation test on the mortality data: > cortest(teen,mort) Pearson’s product-moment correlation data: teen and mort t = , df = , p-value = e- alternative hypothesis: true correlation is not equal to   percent confidence interval:   sample estimates: cor  > r=cor(teen,mort) > r/sqrt((-r^)/(length(teen)-)) []  Conclusion? SSO Lecture   /   -distribution contingency tables linear regression to finish Example -Mortality and teenage mothers () Question: which line fits best? > plot(teen,mort,xlab="""",ylab="""",main="""") > abline(,,lwd=,col=""blue"") > abline(,,lwd=,col=""red"") SSO Lecture   /   -distribution contingency tables linear regression to finish The simple linear regression model The simple linear regression model (enkelvoudig lineair regressiemodel) is: Y =  + x + e with Y the dependent variable (response variable) x the independent variable (explanatory variable, predictor variable) ,  unknown population parameters e the stochastic error (fluctuation) Assumption: the error e has a N(,) distribution with unknown variance  SSO Lecture   /   -distribution contingency tables linear regression to finish Estimating parameters, SSE To find the best line we minimize the sum of squared vertical distances between the observations yi’s and the line  − x: nn min X (yi −  − xi ) = X (yi − ˆ − ˆxi ) = SSE, ,  i= i= the minimum is called the Sum of Squared Errors (SSE) The values for  and  that minimize this sum are called the least squares estimates (kleinste kwadraten schatters) These are Pn sxi=(xi − x¯)(yi − y¯)ˆ = y¯ − ˆx¯, ˆ = r = Pn , syi=(xi − x¯) where sx and sy are the sample standard deviations of x and y samples respectively (in Triola: b =ˆ en b =ˆ)  = s SSE The estimated variance of the errors ei isˆ = n− In R: lm(yx, data = ) SSO Lecture   /   -distribution contingency tables linear regression to finish Example -Mortality and teenage mothers () Estimating the regression parameters and variance > mymodel=lm(mort~teen,data=mortality); summary(mymodel) Call: lm(formula = mort ~ teen, data = mortality) Residuals: Min Q Median Q Max - - -   Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)    e- *** teen    e- *** --- Residual standard error:  on  degrees of freedom Multiple R-squared: ,Adjusted R-squared:  F-statistic:  on  and  DF, p-value: e- From the output: ˆ =and ˆ = (this corresponds to the blue line), ˆ=, so ˆ = SSO Lecture   /   -distribution contingency tables linear regression to finish Is it a good model? The residual (residu) is the estimated error of the ith observation is ˆei = yi − ˆ − ˆxi, In R: residuals(mymodel) mortality rate teenagersmortality rate−residualsIndexresiduals(mymodel) Two methods to check model quality (more next week): coefficient of determination test H :  = SSO Lecture   /   -distribution contingency tables linear regression to finish Coefficient of determination The coefficient of determination (determinatiecoëfficiënt) r compares the models Y =  + e and Y =  + x + e Left we get ˆ = y with sum of squares (Triola: total variation) n X SSyy =(yi − y) i= The coefficient of determination r is  SSyy − SSE  r = (  r  ) SSyy which is equal to the squared sample correlation coefficient This is the proportion of explained variance The higher r the more the model explains SSO Lecture   /   -distribution contingency tables linear regression to finish Example -Mortality and teenage mothers () > summary(mymodel) Residual standard error:  on  degrees of freedom Multiple R-squared: ,Adjusted R-squared:  > SSE=sum(residuals(mymodel)^) > SSYY=sum((mortality$mort-mean(mortality$mort))^) > (SSYY-SSE)/SSYY []  > cor(teen,mort) []  > ^ []  For this data set the simple linear regression model explains % of the variation It is clear that the mortality rate is influenced by other factors than teenage birth rate only SSO Lecture   /   -distribution contingency tables linear regression to finish Test for H : = Setting: a bivariate sample (X, Y),, (Xn, Yn) from some population We test about  in the simple linear regression model: Y =  + x + e Hypotheses: H :  =  versus H :  =  Test statistic: ˆ T = , s ˆ where s ˆ is the estimated standard deviation of ˆ Distribution of T under H: t-distribution with n −  degrees of freedom (exact) Usually the two-sided test is considered Assumption: the errors follow a normal distribution In R: The p-values are in the column Pr(>|t|) in the output of summary(lm(yx)) SSO Lecture   /   -distribution contingency tables linear regression to finish Example -Mortality and teenage mothers () > summary(mymodel) Call: lm(formula = mort ~ teen, data = mortality) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)    e- *** teen    e- *** -- −−−Normal QuantilesSample Quantiles SSO Lecture   /   -distribution contingency tables linear regression to finish Confidence intervals for  The ( − ) confidence interval for  is  = ˆ ± t /s ˆ In R use confint(lm(yx)) > confint(mymodel) % % (Intercept)   teen   > confint(mymodel,level=) % % (Intercept)   teen   SSO Lecture   /   -distribution contingency tables linear regression to finish to finish SSO Lecture   /   -distribution contingency tables linear regression to finish To wrap up Today we discussed  distribution contingency tables,  test, Fisher’s test simple linear regression Next time multiple linear regression SSO Lecture   /  "
"SSO, Lecture 5",stastics simulation and optimization,SSO,slides,model validation sso lecture thigh error explanatory bodyfat fat estimate,strategies variables linear regression r-squared test value intercept step interval,prediction parameters good finish multiple data triceps midarm variable coefficients,,2950,37,Eduard Belitser,2019,"SSO, Lecture  SSO, Lecture  Eduard Belitser VU Amsterdam  model and parameters good model strategies prediction validation to finish Overview      multiple linear regression model a good model strategies prediction validation SSO Lecture   /   model and parameters good model strategies prediction validation to finish regression model and parameters SSO Lecture   /   model and parameters good model strategies prediction validation to finish Example -Bodyfat data () Data of  females between  and  years old on amount of body fat, triceps skinfold thickness, thigh circumference and midarm circumference > bodyfat Fat Triceps Thigh Midarm                               Body fat is hard to measure, while the other  variables are easy to measure Question: can we predict Fat from the other  variables? SSO Lecture   /   model and parameters good model strategies prediction validation to finish Example -Bodyfat data () Scatter plots of all pairs of two variables: > pairs(bodyfat) Question Can we predict Fat from the other  variables? FatTricepsThighMidarm SSO Lecture   /   model and parameters good model strategies prediction validation to finish The multiple linear regression model The multiple linear regression model (meervoudig lineair regressiemodel) is: Y =  + x + + kxk + e, where Y is the dependent variable (response variable), x,, xk are the independent variables (explanatory variables, predictor variables), ,, k are unknown population parameters, e is the stochastic error Assumption: the error e  N(,), with unknown variance  Note: simple linear regression is a special case of multiple linear regression with k =  SSO Lecture   /   model and parameters good model strategies prediction validation to finish Examples of explanatory variables Possible explanatory variables (prediction variables): all xi different Y =  + x + + kxk + e, powers of xi ’s y =  + x + x + x + e, interactions between xi’s y =  + x + x + xx + e Essential: all models are linear in the i ’s, but not necessarily in the xi ’s SSO Lecture   /   model and parameters good model strategies prediction validation to finish Estimating parameters, SSE To find the best parameters we minimize the sum of squared differences between the observations and the model: nn min X (yi − − xi,−− k xi,k) = X (yi− ˆ− ˆxi,−− ˆkxi,k ) = SSE, , k i= i= where ˆ,, ˆk are the least squares estimates (kleinste kwadraten schatters) for the ’s (In Triola: bi =ˆi ), the Sum of Squared Errors (SSE) is nn  SSE = X (yi − ˆ − ˆxi, − − ˆkxi,k) = X eˆi , i= i= ˆ= yi − ˆ − ˆxi, − − ˆkxi,k is the ith residual (residu) ei The estimated variance of the errors ei is  SSE ˆ= s = n − k −  In R: lm(yx++xk, data = ) SSO Lecture   /   model and parameters good model strategies prediction validation to finish Example -Bodyfat data () Estimating the regression parameters: > bodyfatlm=lm(Fat~Triceps+Thigh+Midarm,data=bodyfat) > summary(bodyfatlm) Call: lm(formula = Fat ~ Triceps + Thigh + Midarm, data = bodyfat) Residuals: Min Q Median Q Max - -    Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)     Triceps     Thigh -  -  Midarm -  -  From the output we can find ˆ,ˆ,ˆ and ˆ SSO Lecture   /   model and parameters good model strategies prediction validation to finish Example -Bodyfat data () The estimated variance of the bodyfat data: > summary(bodyfatlm) Call: lm(formula = Fat ~ Triceps + Thigh + Midarm, data = bodyfat) Residuals: Min Q Median Q Max - -    Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)     Triceps     Thigh -  -  Midarm -  -  Residual standard error:  on  degrees of freedom Multiple R-squared: , Adjusted R-squared:  F-statistic:  on  and  DF, p-value: e- From this output: ˆ=, so ˆ = SSO Lecture   /   model and parameters good model strategies prediction validation to finish a good model SSO Lecture   /   model and parameters good model strategies prediction validation to finish When is a model good? Not all available explanatory variables have explanatory power The goal is to find the best possible model with the smallest number of explanatory variables There exists no standard strategy to find the optimal model The practical context also plays a role We consider several ways of comparing two models SSO Lecture   /   model and parameters good model strategies prediction validation to finish Coefficient of determination The multiple coefficient of determination (meervoudige determinatiecoëfficiënt) R compares the models Y =  + e and Y =  + x + + kxk + e Left we get ˆ = y with sum of squares n X SSyy =(yi − y) i= The coefficient of determination R is SSyy − SSE R = (  R  ) SSyy This is the proportion of explained variance R yields a global check on the multiple linear regression model The higher R the more variation the model explains Note: if k = , we have R = r SSO Lecture   /   model and parameters good model strategies prediction validation to finish Example -Bodyfat data () > summary(bodyfatlm) Residual standard error:  on  degrees of freedom Multiple R-squared: , Adjusted R-squared:  F-statistic:  on  and  DF, p-value: e- > SSE=sum(residuals(bodyfatlm)^) > SSYY=sum((bodyfat$Fat-mean(bodyfat$Fat))^) > (SSYY-SSE)/SSYY []  For this data set the multiple linear regression model explains % of the variation That is quite a lot Question: when is R high (enough)? SSO Lecture   /   model and parameters good model strategies prediction validation to finish Testing the full multiple linear regression model () In simple linear regression we compare Y =  + e and Y =  + x + e If H :  =  is rejected (see t-test of last week) a simple linear regression model is useful, since x has significant explanatory power in a linear model In multiple linear regression we compare Y =  + e and Y =  + x + + kxk + e Now we test H :  = = k =  If this H is rejected, multiple linear regression is useful, since x,, xk together have significant explanatory power in a linear model SSO Lecture   /   model and parameters good model strategies prediction validation to finish Test for H : = =k= Setting: a multivariate data set with response variable Y and explanatory variables X,, Xk We test the i ’s in the multiple linear regression model: Y =  + x + + kxk + e Hypotheses: H :  = = k =  versus H : at least one i  =  Test statistic: R/k T = ( − R)/(n − (k + )) The larger R , the larger T Distribution of T under H: F -distribution with k and n − (k + ) degrees of freedom (exact) The test is always right-sided: pright = P(T > t) We only reject H if R is large, ie if T is large Assumption: the errors follow a normal distribution In R: The p-value is in the last line of summary(lm(yx)) SSO Lecture   /   model and parameters good model strategies prediction validation to finish Example -Bodyfat data () The output of the overall F -test of the bodyfat data: > summary(bodyfatlm) Residual standard error:  on  degrees of freedom Multiple R-squared: , Adjusted R-squared:  F-statistic:  on  and  DF, p-value: e- The p-value in the overall test is  Hence for this data the F -test rejects H :  = = k =  At least one of the i’s is not equal to  SSO Lecture   /   model and parameters good model strategies prediction validation to finish Testing individual explanatory variables Not all available explanatory variables have explanatory power From all explanatory variables, we need to find relevant explanatory variables Therefore we test H : i =  for all i in the model SSO Lecture   /   model and parameters good model strategies prediction validation to finish Test for H :i= Setting: a multivariate data set with response variable Y and explanatory variables X,, Xk We test H : i =  in the multiple linear regression model: Y =  + x + + kxk + e Hypotheses: H : i =  versus H : i =  Test statistic: ˆiT = T =  tn−(k+) si (s= sii , where the matrix [ij]=(XTX)−) i Distribution of T under H: t-distribution with n − (k + ) degrees of freedom (exact) Usually the two-sided test is considered Assumption: the errors follow a normal distribution In R: the p-value is in the column Pr(>|t|) in the output of summary(lm(yx)) SSO Lecture   /   model and parameters good model strategies prediction validation to finish Example -Bodyfat data () The p-values of the individual explanatory variables in the bodyfat data: > summary(bodyfatlm) Call: lm(formula = Fat ~ Triceps + Thigh + Midarm, data = bodyfat) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)     Triceps     Thigh -  -  Midarm -  -  Residual standard error:  on  degrees of freedom Multiple R-squared: , Adjusted R-squared:  F-statistic:  on  and  DF, p-value: e- From this output: none of the i ’s is significant So none of the explanatory variables separately explains a signficant part, but all together they explain %! SSO Lecture   /   model and parameters good model strategies prediction validation to finish Confidence intervals for i’s The ( − ) confidence interval for the i’s are i =ˆi ± t /s ˆi In R use confint(lm(yx)) > confint(bodyfatlm) % % (Intercept) -  Triceps -  Thigh -  Midarm -  > confint(bodyfatlm,level=) % % (Intercept) -  Triceps -  Thigh -  Midarm -  SSO Lecture   /   model and parameters good model strategies prediction validation to finish strategies SSO Lecture   /   model and parameters good model strategies prediction validation to finish How to find the relevant predictors? The goal is to find the best possible model (high R) with the smallest number of explanatory variables Since more explanatory variables always explain more, we can consider the R adjusted for the number k of explanatory variables: n −  R adjusted = − ( − R) n − (k + ) The goal is to maximize R with as few as possible explanatory variables, and R adjusted helps to choose between models with different amounts of variables Note that the interpretation of R adjusted is not fraction of explained variance anymore We consider two strategies to find the optimal model SSO Lecture   /   model and parameters good model strategies prediction validation to finish Two strategies for finding a good model In practice we need a strategy for building a model The step up method:  start with the background model Y =  + e  take the variable (that is not in the model) that yields the maximum increase in R  if this variable is significant (t-test) add it to the model and go to step ; otherwise stop The step down method:  start with the full model Y =  + x + + kxk + e  test all variables by using the t-test  if the largest p-value is larger than , remove the corresponding variable and go back to step  SSO Lecture   /   model and parameters good model strategies prediction validation to finish Step up () We apply the step up strategy to the bodyfat data: > summary(lm(Fat~Triceps)) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept) -  -  Triceps    e- *** Multiple R-squared:  > summary(lm(Fat~Thigh)) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept) -  -  *** Thigh    e- *** Multiple R-squared:  Residual standard error:  on  degrees of freedom > summary(lm(Fat~Midarm)) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)     Midarm     Multiple R-squared:  The first variable to add is Thigh SSO Lecture   /   model and parameters good model strategies prediction validation to finish Step up () The second step: > summary(lm(Fat~Thigh+Triceps)) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept) -  -  * Thigh     * Triceps     Multiple R-squared:  > summary(lm(Fat~Thigh+Midarm)) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept) -  -  ** Thigh    e- *** Midarm     Multiple R-squared:  Resulting model: Fat =- + *Thigh + error, with R = and ˆ= SSO Lecture   /   model and parameters good model strategies prediction validation to finish Step down () We now apply the step down strategy to the bodyfat data: > summary(lm(Fat~Triceps+Thigh+Midarm)) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)     Triceps     Thigh -  -  Midarm -  -  Multiple R-squared:  We see that none of the variables is significant The first variable to remove is Thigh, which has the highest p-value SSO Lecture   /   model and parameters good model strategies prediction validation to finish Step down () The second step: > summary(lm(Fat~Triceps+Midarm)) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)     Triceps    e- *** Midarm -  -  * Residual standard error:  on  degrees of freedom Multiple R-squared:  All remaining variables are significant Resulting model: Fat =  + *Triceps -*Midarm + error with R = and ˆ= SSO Lecture   /   model and parameters good model strategies prediction validation to finish Up or down? Now we are left with two different models Model : (R =,ˆ=) Fat = - + *Thigh + error Model : (R =,ˆ=) Fat =  + *Triceps -*Midarm + error Question: which one do we prefer, and why? Model  is preferred, because it has less variables, a comparable estimate of error variance, and a comparable value of R SSO Lecture   /   model and parameters good model strategies prediction validation to finish prediction SSO Lecture   /   model and parameters good model strategies prediction validation to finish The predicted value Once all ˆi’s are known, one can predict the y-value for a (new) measurement of the k explanatory variables: xnew =(xnew,j, j =,, k): Yˆ new =ˆ +ˆxnew, + ˆkxnew,k For the x-values in the data set, these yˆ-values are found by > fitted(bodyfatlm)                 SSO Lecture   /   model and parameters good model strategies prediction validation to finish Confidence and prediction intervals Two types of intervals for ynew for given xnew -values: confidence interval for Ynew : an interval for the mean Ynew -value for given xnew -values p (This is interval xT s(xT new ˆ ± t /,n−(k+) new (XTX)−xnew )) prediction interval for Ynew : an interval for an individual Ynew -observation for given xnew -values (this interval is larger!) p (This is interval xT ˆ ± t /,n−(k+) s( + xT (XTX)−xnew )) new new Confidence: is for the population mean, whereas prediction is for an individual observation In R: predict(lm(yx++xk),newxdata,interval=,level=) SSO Lecture   /   model and parameters good model strategies prediction validation to finish Example -Bodyfat data () Prediction intervals for the body fat data for new data can be found by designing a dataframe with the new x-values applying predict to this dataframe > newxdata=dataframe(Triceps=,Thigh=,Midarm=) > predict(bodyfatlm,newxdata,interval=""prediction"") fit lwr upr     > predict(bodyfatlm,newxdata,interval=""prediction"",level=) fit lwr upr     > predict(bodyfatlm,newxdata,interval=""confidence"",level=) fit lwr upr     The prediction interval is indeed larger! SSO Lecture   /   model and parameters good model strategies prediction validation to finish validating the model SSO Lecture   /   model and parameters good model strategies prediction validation to finish Validation of normality As in the case of simple linear regression, one needs to check the normality assumption in a QQ-plot of the residuals > qqnorm(residuals(bodyfatlm)) −−−−Normal QuantilesSample Quantiles Next time we will investigate more scatter plots for validating the model SSO Lecture   /   model and parameters good model strategies prediction validation to finish to finish SSO Lecture   /   model and parameters good model strategies prediction validation to finish To wrap up Today we discussed multiple linear regression model and parameters a good model strategies prediction validation Next time several problems in multiple linear regression and ANOVA SSO Lecture   /  "
"SSO, Lecture 6",stastics simulation and optimization,SSO,slides,diagnostics model city linear mean bodyfat cities error value triceps,anova problems sso finish plot variable variance variables incomes income,lecture data residuals example test point explanatory group scatter assumption,,3673,49,Eduard Belitser,2019,"SSO, Lecture  SSO, Lecture  Eduard Belitser VU Amsterdam  diagnostics problems anova to finish Overview   multiple linear regression diagnostics problems outliers influence points collinearity ANalysis Of VAriance (ANOVA) one-way ANOVA Kruskal-Wallis test two-way ANOVA ANCOVA SSO Lecture   /   diagnostics problems anova to finish diagnostics SSO Lecture   /   diagnostics problems anova to finish Diagnostics Recall the multiple linear regression model: Y =  + x + + kxk + e Checking the fit by looking at the (adjusted) R is not sufficient One needs to check the model assumptions: the linearity of the relation and the normality of the errors We consider both graphical and numerical tools In the following  examples of artificial data, the fitted model is y =  + *x + error, and ˆ = and R = The differences between the  situations illustrate the need for a diagnostic tool, apart from looking at R and ˆ SSO Lecture   /   diagnostics problems anova to finish Plots of the  examples     The first looks ok No linear relation between X and Y Outlying point in Y All X’s are the same except one SSO Lecture   /   diagnostics problems anova to finish Diagnostic plots To check the model quality look at  scatter plot: plot Y against each Xj separately (this yields overall picture, and shows outlying values)  added variable plot: plot residuals of Xj against residuals of Y for each Xj (this shows how much Xj contributes additional to the other variables)  scatter plot: plot residuals against each Xj in the model separately (look at pattern (curved?) and spread)  scatter plot: plot residuals against each Xj not in the model separately (look at pattern — linear? then include!)  scatter plot: plot residuals against Y (look at spread)  normal QQ-plot of the residuals (check normality assumption) SSO Lecture   /   diagnostics problems anova to finish Example -Bodyfat data ()  scatter plot of Y againsteach Xj separately (this yields overall picture, and shows outlying values) > pairs(bodyfat) FatTricepsThighMidarm SSO Lecture   /   diagnostics problems anova to finish Example -Bodyfat data ()  added variable plot of residuals of Xj againstresiduals of Y for each Xj(this shows how much Xjcontributes additive to the other variables) > x=residuals(lm(Triceps~Thigh)) > y=residuals(lm(Fat~Thigh)) > plot(x,y,main=""Added variable plot for Triceps"") > x=residuals(lm(Midarm~Thigh)) > y=residuals(lm(Fat~Thigh)) > plot(x,y,main=""Added variable plot for Midarm"") > x=residuals(lm(Thigh~Midarm+Triceps)) > y=residuals(lm(Fat~Midarm+Triceps)) >plot(x,y,main=""AddedvariableplotforThigh"") −−−Added variable plot for Tricepsresidual of Tricepsresidual of Fat−−−Added variable Midarmresidual of Fat−SSO Lecture   /   diagnostics problems anova to finish Example -Bodyfat data ()  scatter plot of residuals against each Xj in the model separately (look at pattern (curved?) and spread) > par(mfrow=c(,)) > plot(Thigh,residuals(bodyfatlm)) Remedy: if a curved pattern is visible, then include Xj  or transform Xj (eg, −−Residuals ThighThighresiduals(bodyfatlm) p take log(Xj ) or Xj) SSO Lecture   /   diagnostics problems anova to finish Example -Bodyfat data ()  scatter plot of residuals against each Xj not in the model separately (look at pattern — linear? then include!) > par(mfrow=c(,)) > plot(Triceps,residuals(bodyfatlm),main=""Residuals against Triceps"") > plot(Midarm,residuals(bodyfatlm),main=""Residuals against Midarm"") −−Residuals TricepsTricepsresiduals(bodyfatlm) MidarmMidarmresiduals(bodyfatlm) Remedy: if a linear pattern is visible, then include that Xj! SSO Lecture   /   diagnostics problems anova to finish Example -Bodyfat data ()  scatter plot of residuals against Y (look at spread) > par(mfrow=c(,)) > plot(Fat,residuals(bodyfatlm),main=""Residuals against Fat"") −−Residuals FatFatresiduals(bodyfatlm) p Remedy: if residuals grow with Y , then transform Y , (eg, log(Y ) or Y ) SSO Lecture   /   diagnostics problems anova to finish Example -Bodyfat data ()  normal QQ-plot of the residuals (check normality assumption) > qqnorm(residuals(bodyfatlm)) −−−−Normal QuantilesSample Quantiles Remedy: if residuals are not normally distributed, go back to scatter plots () and start with different model Options: transform Y , take different X’s, split dataset in multiple parts SSO Lecture   /   diagnostics problems anova to finish Conclusion of example on Bodyfat data None of the plots shows outlying values, specific patterns or anything else that indicates that our assumptions are wrong Therefore, we stay with the model Fat = - + *Thigh + error, with R = and ˆ= SSO Lecture   /   diagnostics problems anova to finish problems in linear regression SSO Lecture   /   diagnostics problems anova to finish Outlier – Forbes’ data () An outlier is an observation with an extremely high or low response value, compared to what is expected under the model pointresidualsresiduals vs Ypressureresiduals−−QQ−QuantilesSample Quantiles Forbes’ data on the relation between boiling point of water and pressure Residuals are for the univariate linear regression model SSO Lecture   /   diagnostics problems anova to finish Test for outliers () The mean shift outlier model can be applied to test whether the kth point significantly deviates from the other points in a linear regression setting For Forbes’ data: > x=forbes[,] > y=forbes[,] > forbeslm=lm(y~x) > round(residuals(forbeslm),)     - - - -    - - -       - - - - - The th data point is the outlier > order(abs(residuals(forbeslm))) []       The command order(abs(residuals())) gives the indices of the ordered absolute values of residuals from smallest to largest The last one(s) corresponds to the outlier(s) SSO Lecture   /   diagnostics problems anova to finish Test for outliers () > u=c(rep(,),,rep(,)) > u=rep(,) > u[]= > u []      > forbeslm=lm(y~x+u) > summary(forbeslm) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept) -  - e- *** x    < e- *** u    e- *** Since the coefficienct ( ) for explanatory variable u is significantly different from , the outlier is significant It is common to apply a one-sided version of this test — we know whether the Y-value is very small or very big SSO Lecture   /   diagnostics problems anova to finish Example – Huber’s data () A leverage point or potential point is an observation with an outlying value in the explanatory variable −−−Huber's datahuberxhubery Huber’s fictive data Question: what is the influence of the observation with x=? SSO Lecture   /   diagnostics problems anova to finish Definition influence point To study the effect of a leverage point one can fit the model with and without that data point If the estimated parameters change drastically by deleting the leverage point, the observation is called an influence point This is not necessarily the case, it depends on the Y -value of the leverage point The Cook’s distance for the ith data point is X  n (ˆ Yj) Di =  Y(i),j − ˆ , (k + )ˆ j= with Yˆ (i),j the predicted jth response based on the model without the ith data point The Cook’s distance Di quantifies the influence of observation i on the predictions Rule of thumb: if the Cook’s distance for some data point is close to or larger than , it is considered an influence point SSO Lecture   /   diagnostics problems anova to finish Example – Huber’s data () We compute the Cook’s distances for Huber’s data set: > round(cooksdistance(huberlm),)        > plot(:,cooksdistance(huberlm),type=""b"") Here we clearly have encountered an influence point: the Cook’s distance is  for the leverage point A plot of Cook’s distances is usually insightful SSO Lecture   /   diagnostics problems anova to finish Collinearity Collinearity is the problem of linear relations between explanatory variables If two explanatory variables show a straight line in a scatter plot, then they explain the same Example on weight and length: weight =  +  × lengthm +  × lengthcm + error For a person of  cm and  kilo:  = + ×   = + ×  +  ×   = + ×  +  ×  The ’s can not be uniquely determined SSO Lecture   /   diagnostics problems anova to finish Ways to investigate collinearity Graphical ways to investigate collinearity: scatter plot of Xj against Xl for all combinations j, l (check pairwise collinearity) Numerical way to investigate collinearity: pairwise linear correlation of Xj and Xl for all combinations j, l (check whether these are far from ), variance inflation factor of j for all j (check whether these are high) There exist more advanced numerical ways to investigate collinearity, eg, condition indices variance decomposition These diagnostics (and others) can be produced using special packages for R, eg, package car SSO Lecture   /   diagnostics problems anova to finish Variance inflation factor To see which predictor variables are involved in collinearities we can look at the residuals of Xj regressed on the other explanatory variables (cf added variable plot) If these residuals are very small, then Xj is (nearly) a linear combination of other X’s This is quantified in the variance inflation factor  VIFj = , j =,, k,  −R j with R j the determination coefficient of the mentioned regression Rule of thumb: VIFj’s larger than  indicate that ˆj is unreliable Remark: these values do not give information about which variables are in the same collinear group of variables SSO Lecture   /   diagnostics problems anova to finish Example -Bodyfat data () Apply these checks to the bodyfat data: > round(cor(bodyfat),) Fat Triceps Thigh Midarm Fat     Triceps     Thigh     Midarm     Clearly Triceps and Thigh are collinear, both from the plot and from the correlation value of  FatTricepsThighMidarm SSO Lecture   /   diagnostics problems anova to finish Example -Bodyfat data () We compute the VIF-values for the bodyfat data > bodyfatlm=lm(Fat~Thigh+Triceps+Midarm, data=bodyfat) > vif(bodyfatlm) Thigh Triceps Midarm    > bodyfatlm=lm(Fat~Triceps+Midarm, data=bodyfat) > vif(bodyfatlm) Triceps Midarm   > bodyfatlm=lm(Fat~Thigh, data=bodyfat) > vif(bodyfatlm) Error in vifdefault(bodyfatlm) : model contains fewer than  terms If we fit the full model all  VIF’s are large, so there is a collinearity problem (as we saw in the scatter plots) The other  models are ok with respect to collinearity problems SSO Lecture   /   diagnostics problems anova to finish analysis of variance (ANOVA) SSO Lecture   /   diagnostics problems anova to finish ANOVA and ANCOVA In linear regression all variables (explanatory and response) are numerical An explanatory variable can be categorical a income  city, with variable city indicating, eg, Amsterdam or Rotterdam (cf two sample t-test), b income  city + nationality, c weight  length + gender, with variable gender indicating man/woman Models for these three situations: a one-way ANOVA one categorical explanatory variable, b two-way ANOVA two categorical explanatory variables, c ANCOVA a mix of categorical and numerical explanatory variables SSO Lecture   /   diagnostics problems anova to finish one-way ANOVA A categorical explanatory variable with k different categories corresponds to k groups/populations The one-way ANOVA model (-weg anova model) is: Yij = μ + i + eij, with Yij the jth response measured in group i, μ the mean of group i = , i the deviation from the mean for group i, for i =,, k, eij the stochastic error (fluctuation) Note: default parametrization in R is that Group  is the reference class and  =  Assumption: the errors eij  N(,), with unknown variance  Note: if k =  this is the setting for the two sample t-test, assuming equal variances SSO Lecture   /   diagnostics problems anova to finish Test for H : = =k= Setting: a one-way ANOVA model: Yij = μ + i + eij Hypotheses: H :  = = k =  versus H : at least one i =  Test statistic: Pk explained variance i= ni(Y¯ i − Y¯ )/(k − ) F == Pk Pni unexplained variance j=(Yij − Y¯ i )/(N − k) i= Distribution of F under H: F-distribution with k −  and N − k degrees of freedom (exact) Assumption: the errors follow a normal distribution In R: The p-value is in anova(lm(yx)) with x the categorical variable SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of Amsterdam and Rotterdam () (Fictive) data on  incomes in Amsterdam and  incomes in Rotterdam amsterdamrotterdam−−QQplot amsterdamTheoretical QuantilesSample Quantiles−Question: is the (population) mean income the same in Amsterdam and Rotterdam? SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of Amsterdam and Rotterdam () > amsrotdam income city   a   a   a > incomelm=lm(income~city,data=amsrotdam) > anova(incomelm) Analysis of Variance Table Response: income Df Sum Sq Mean Sq F value Pr(>F) city  e+    ** Residuals  e+  > ttest(a,r,varequal=TRUE) Two Sample t-test data: a and r t = , df = , p-value =  The p-values of the anova model and the two sample t-test are equal SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of  cities () We add (fictive) data on  incomes in Utrecht and  incomes in Leeuwarden amsterdamleeuwardenrotterdamutrechtincomes Question: is the (population) mean income the same for all  cities? boxplot(incomecity,main=""incomes in  cities"",data=cityincomes) SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of  cities () > cityincomes income city   a   a   a   l   l   l > citylm=lm(income~city,data=cityincomes) > anova(citylm) Analysis of Variance Table Response: income Df Sum Sq Mean Sq F value Pr(>F) city  e+   e- *** Residuals  e+  The variable city is significant with p-value of  SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of  cities () Estimating the i ’s for all the cities: > summary(citylm) Call: lm(formula = income ~ city, data = cityincomes) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)    < e- *** cityleeuwarden -  - e- *** cityrotterdam -  -  ** cityutrecht     Residual standard error:  on  degrees of freedom Multiple R-squared: , Adjusted R-squared:  F-statistic:  on  and  DF, p-value: e- Note: R orders the citynames in alphabetical order, so a (for amsterdam) will always be the reference class SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of  cities () Check the normality assumption in a QQ-plot of the residuals: −−−−Normal QuantilesSample Quantiles SSO Lecture   /   diagnostics problems anova to finish A nonparametric alternative: Kruskal-Wallis test Question: what if the normality assumption fails? The Kruskal-Wallis test is a nonparametric alternative to one-way ANOVA This test is a generalization of the rank sum test (Mann-Whitney test) for two samples It is based on ranks Setting: measurements Yij for i =,, k and j =,, ni from k different populations Yij follows distribution Pi of population i Hypotheses: H : P = = Pk versus H : at least two distrib are different Test statistic: K = N(N +) P ik = niR¯ i  − (N + ), where N = n + + nk and Pni ¯ Ri = j= Rij/ni is the average pooled rank of the observations in sample i, Rij are the pooled ranks Distribution of K under H:  k− (approximately), the test is one sided Assumption: all ni >  In R: kruskaltest(y,x,data=) SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of  cities () Apply the Kruskal-Wallis test to the income data: > kruskaltest(income~city,data=cityincomes) Kruskal-Wallis rank sum test data: income by city Kruskal-Wallis chi-squared = , df = , p-value = e- Also in the (less powerfull) nonparametric Kruskal-Wallis test the variable city is significant with slightly higher p-value than in ANOVA SSO Lecture   /   diagnostics problems anova to finish two-way ANOVA Consider two categorical explanatory variables with resp I and J categories The additive two-way ANOVA model (additief -weg anova model) is: Yijk = μ + i + j + eijk, with Yijk the kth response measured in group (i, j), μ the mean of group i = , j = , i the deviation from the mean for group i, for i =,, I, j the deviation from the mean for group j, for j =,, J, eijk the stochastic error (fluctuation) Note: In R, Group (,) is the reference class,  =  and  =  Assumption: the errors eijk  N(,), with unknown variance  Note: one can also model interactions ij SSO Lecture   /   diagnostics problems anova to finish Test for H : = =I= and H : = =J= Setting: a two-way ANOVA model: Yijk = μ + i + j + eijk Hypotheses: H :  = = I =  versus H : at least one i = , and H :  = = J =  versus H : at least one i =  Test statistic: (balanced design: equal group size r for each i & j, thus N = rIJ) PI explained variance i= rJ(Y¯ i − Y¯ )/(I − ) F == Pr PI PJ unexplained variance j=(Yijk − Y¯ ij)/(N − IJ) k= i= Similarly for F Distribution of F’s under H: F-distributions with I − (J −  for F ) and N − IJ degrees of freedom (exact) Assumption: the errors follow a normal distribution In R: the p-value is in anova(lm(yx+x)) with x and x the categorical variables SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of cities and nationalities () We add fictive nationalities to the  incomes: nl, som, else elsenlsomincomes Question: is the (population) mean income the same for the cities and the nationalities? boxplot(incomenationality,main=""incomes in  cities"",data=incomescitynat) SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of cities and nationalities () > incomescitynat income city nationality   amsterdam nl   amsterdam else   amsterdam som > citynatlm=lm(income~city+nationality,data=incomescitynat) > anova(citynatlm) Analysis of Variance Table Response: income Df Sum Sq Mean Sq F value Pr(>F) city  e+   e- *** nationality  e+   e- *** Residuals  e+  Both variables city and nationality are of significant influence on income SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of cities and nationalities () Estimating the i ’s and j’s for all the cities and nationalities: > summary(citynatlm) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)    < e- *** cityleeuwarden -  - e- *** cityrotterdam -  -  ** cityutrecht     nationalitynl     nationalitysom -  - e- *** Note: the reference class is city=amsterdam combined with nationality=else Note: not all i and j are significant when tested in a separate t-test However, overall the variables city and nationality are significant (see ouput of anova) SSO Lecture   /   diagnostics problems anova to finish Example – Incomes of cities and nationalities () Check the normality assumption in a QQ-plot of the residuals: −−−−Normal QuantilesSample Quantiles SSO Lecture   /   diagnostics problems anova to finish ANCOVA A mix of numerical and categorical explanatory variables: The ANCOVA model is: Yi = μ + i + + x + + ei, with Yi the response measured in group i, μ the mean of group i = , i the deviation from the mean for group i, for i =,, I,  the linear influence of explanatory variable X, ei the stochastic error (fluctuation) Note: Group (,) is the reference class,  =  Assumption: the ind errors ei  N(,), with unknown variance  Note: one can have an arbitrary amount of explanatory variables, both numerical and categorical SSO Lecture   /   diagnostics problems anova to finish Example – weight () Consider data on weight, length and sex of  people > weightdata weight sex length   man    man    woman  sexlengthweightmanwomanlength~sexHistogram weightweightFrequency plot(length,weight,col=factor(sex,labels=c(""blue"",""red""))) SSO Lecture   /   diagnostics problems anova to finish Example – weight () > weightlm=lm(weight~sex+length) > anova(weightlm) Analysis of Variance Table Response: weight Df Sum Sq Mean Sq F value Pr(>F) sex     e- *** length      * Residuals    Both variables are significant The estimated parameters, with reference class man: > summary(weightlm) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)     ** sexwoman -  -  ** length     * Estimates: categorical ˆ = − for woman compared to man; numerical ˆ = for additional unit in length SSO Lecture   /   diagnostics problems anova to finish Example – weight () Check the normality assumption in a QQ-plot of the residuals: −−−Normal QuantilesSample Quantiles SSO Lecture   /   diagnostics problems anova to finish to finish SSO Lecture   /   diagnostics problems anova to finish To wrap up Today we discussed multiple linear regression diagnosticsproblems outliers influence points collinearity ANalysis Of VAriance (ANOVA) one-way ANOVA Kruskal-Wallis test two-way ANOVA ANCOVA Next time logistic regression variable transformation overview of module and applications to your own context(s) SSO Lecture   /  "
"SSO, Lecture 7",stastics simulation and optimization,SSO,slides,variable transformation fraude example test transform categorical two difference brains,regression logistic overview sso lecture friesland data anova linear residuals,finish model explanatory testing method population transformation scatter module way,,1755,29,Eduard Belitser,2019,"SSO, Lecture  SSO, Lecture  Eduard Belitser VU Amsterdam  variable transformation logistic regression overview to finish Overview  variable transformation  logistic regression  overview of all techniques SSO Lecture   /   variable transformation logistic regression overview to finish variable transformation in linear regression SSO Lecture   /   variable transformation logistic regression overview to finish types of variable transformation Sometimes a linear regression model Y =  + x + + kxk + e does not fit because the model does not agree with the data In such case data transformation can help We consider two types of data transformation: transform X (or add some f(X)) transform Y SSO Lecture   /   variable transformation logistic regression overview to finish Transform explanatory variable () If a scatter plot of Y against an explanatory variable X shows a relation other than linear transform X p add f (X) to the model (eg X or X) Y~sqrt(X) xyY~exp^XxyY~SSO Lecture   /   variable transformation logistic regression overview to finish Transform explanatory variable () The difference between the linear model lm(yx) the transformed model lm(ylog(x)) −residuals yresiduals−yresiduals Plots of residuals against Y Left we see a curvature, right not really curved SSO Lecture   /   variable transformation logistic regression overview to finish Transform response variable () If a scatter plot of residuals against Y shows an increase or decrease in error size then transform Y , the response variable Example scatter (x,y) xy−−scatter xy−Problem: the residuals grow with increasing Y value (right picture) SSO Lecture   /   variable transformation logistic regression overview to finish Transform response variable () The difference between the linear model lm(yx+x) the transformed model lm(log(y)x+x) The residuals of the log(y) model do not show a specific pattern (bottom-right picture) SSO −residuals yresiduals(mylm) residualsLecture   /   variable transformation logistic regression overview to finish Example – brains () In a methodological study on mapping brain areas we compare  methods (FIRST,FS,Man),  disease groups (CTRL,AD,MCIN,MCIP), and  hemispheres (Left,Right) This yields -way ANOVA SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () Applying (additive) -way ANOVA to the raw data is problematic Yijkl = μ + i + j + k + eijkl > rawlm=lm(Response~Disease+Method+Hemisphere,data=) > qqnorm(residuals(rawlm)) −−−−−Normal QuantilesSample Quantiles SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () We look for a transformation of Y Shown: QQ-plot and p-values of Shapiro-Wilk test on residuals SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () FItting the additive -way ANOVA model on the Y  data: > mylm=lm(Response^~Method+Disease+Hemisphere,data=) > anova(mylm) Analysis of Variance Table Response: Y^ Df Sum Sq Mean Sq F value Pr(>F) Method     < e- *** Disease      ** Hemisphere      Residuals    Conclusion: Disease and Method matter significantly, Hemisphere does not SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () The researchers expect a possible interaction between Method and Disease Interactions can be modeled by Method*Disease or by Method+Disease+Method:Disease > mylm=lm(Response^~Method*Disease+Hemisphere,data=) > anova(mylm) Analysis of Variance Table Response: Y^ Df Sum Sq Mean Sq F value Pr(>F) Method     < e- *** Disease      ** Hemisphere      Method:Disease      Residuals    The interaction is not significant (p=) SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () We stay with the additive model, without interactions > mylm=lm(Response^~Method+Disease+Hemisphere,data=) > summary(mylm) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)    < e- *** MethodFS -  - < e- *** MethodMan -  - < e- *** DiseaseCTRL     ** DiseaseMCIN     DiseaseMCIP     HemisphereRight     FS is worse than Man is worse than reference Method (FIRST) CTRL is best Disease group Hemisphere is not significant Remark All pairwise comparisons between different levels of all factors can be made using Tukey’s method SSO Lecture   /   variable transformation logistic regression overview to finish logistic regression SSO Lecture   /   variable transformation logistic regression overview to finish Logistic regression If the response variable Y is categorical (-) then we cannot use linear regression or anova/ancova For a - response variable we can model the probability P(Y =) as a function of explanatory variables The explanatory variables can be either numerical or categorical or a mix The logistic regression model (logistische regressie) is  P(Y = ) = P(Y =)=  − P(Y = ), + e−function , with function = μ + i + + x + , the mix of numerical and categorical explanatory variables In R: glm(yx+x+,family=binomial,data=mydata) SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () Consider (fictive) data on Toeslagfraude in  suspicious cases: > toeslagfraude fraude hoogtetoeslag friesland                             Question: does the probability of fraude depend on the height of the Toeslag and whether the person lives in Friesland? SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () Exploring the dependence on the numerical variable hoogtetoeslag: hoogtefraudehoogtefraudeFrequencyHistogram hoogtenietfraudehoogtenietfraudeFrequency>hoogtefraude=hoogtetoeslag[fraude==] >hoogtenietfraude=hoogtetoeslag[fraude==] >wilcoxtest(hoogtefraude,hoogtenietfraude) Wilcoxon rank sum test with continuity correction data: hoogtefraude and hoogtenietfraude W = , p-value =  alternative hypothesis: true location shift is not equal to  SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () Exploring the dependence on the categorical variable friesland: > xtabs(~fraude+friesland,data=toeslagfraude) friesland fraude        > chisqtest(xtabs(~fraude+friesland,data=toeslagfraude),simulatepvalue=TRUE) Pearson’s Chi-squared test with simulated p-value (based on  replicates) data: xtabs(~fraude + friesland, data = toeslagfraude) X-squared = , df = NA, p-value =  Fraude occurs significantly less often in Friesland SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () Fitting a logistic regression model with explanatory variables hoogtetoeslagand friesland: > fraudeglm=glm(fraude~friesland+hoogtetoeslag,family=binomial) > summary(fraudeglm) Call: glm(formula = fraude ~ friesland + hoogtetoeslag, family = binomial) Coefficients: Estimate Std Error z value Pr(>|z|) (Intercept) -  -  friesland -  -  ** hoogtetoeslag     * Both variables are significant The estimated coefficient for friesland is negative, which means friesland= yields lower probability on fraude than friesland= The estimated coefficient for hoogtetoeslag is positive, which means that probability on fraude increases with hoogtetoeslag SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () If the categorical explanatory variables has more than  categories the command drop is helpful: > fraudeglm=glm(fraude~friesland+hoogtetoeslag,family=binomial) > drop(fraudeglm,test=""Chisq"") Single term deletions Model: fraude ~ friesland + hoogtetoeslag Df Deviance AIC LRT Pr(>Chi) <none>   friesland      ** hoogtetoeslag      * It works the same as anova in ANOVA models SSO Lecture   /   variable transformation logistic regression overview to finish More advanced logistic regression If the categorical response variable has more than  categories the usual logistic regression model does not apply In that case one can use multinomial logistic regression For that you need special R packages SSO Lecture   /   variable transformation logistic regression overview to finish overview of module SSO Lecture   /   variable transformation logistic regression overview to finish Module overview () In this module we have discussed and practiced: probability distributions population — the underlying huge (infinite?) population sample — the sample measured summarizing data numerical ways — mean, median, sd, variance, range, etc graphical ways — histogram, boxplot, scatter plot, etc testing normality QQ-plots — graphical way to test normality Shapiro-Wilk test — numerical way to test normality estimation population mean — point estimate or confidence interval population proportion — point estimate or confidence interval SSO Lecture   /   variable transformation logistic regression overview to finish Module overview () hypothesis testing formal set up — eg H, H, test statistic, p-value one-sample tests t-test — testing population mean in normal setting binomial test — testing population proportion two-samples tests t-test for matched pairs — testing difference between population means t-test for independent pairs — testing difference between population means testing two proportions — testing difference between two population proportions non-parametric tests sign test — testing population median Wilcoxon one sample test — testing point of symmetry Wilcoxon two samples test — testing difference in two populations SSO Lecture   /   variable transformation logistic regression overview to finish Module overview () contingency tables chisquare test — testing independence or homogeneity between variables simple and multiple linear regression model — Y =  + x + + kxk + e estimation — using R testing — tests for H : i =  and H :  = = k = challenges © in linear regression diagnostics — several scatter plots to validate the model fit outliers — extreme in response variable leverage points — extreme in explanatory variable influence points — extreme in explanatory variable with influence collinearity — explanatory variables that explain the same SSO Lecture   /   variable transformation logistic regression overview to finish Module overview () anova one-way anova — one categorical explanatory variable Kruskal-Wallis — nonparametric alternative to one-way anova two-way anova — two categorical explanatory variables ancova — mix of anova and linear regression variable transformation transformation of X — if relation between Y and X is not linear transformation of Y — if residuals not ok, ie model does not fit logistic regression logistic regression — a - response variable SSO Lecture   /   variable transformation logistic regression overview to finish to finish SSO Lecture   /   variable transformation logistic regression overview to finish To wrap up Today we discussed two-way anova logistic regression variable transformation overview of all techniques ENJOY ANALYZING YOUR DATA SSO Lecture   /  "
"Statistics, Simulation & Optimization Lecture 8",stastics simulation and optimization,SSO,slides,solution feasible integer lecture example afbeeldingsresultaat value view problems linear,problem region ilo mix relaxation modeling decision voor variables function,optimization objective product step max data excel model simulation branch,,2675,43,Joost Berkhout,2019,"SSO, Lecture  SSO, Lecture  Eduard Belitser VU Amsterdam  variable transformation logistic regression overview to finish Overview  variable transformation  logistic regression  overview of all techniques SSO Lecture   /   variable transformation logistic regression overview to finish variable transformation in linear regression SSO Lecture   /   variable transformation logistic regression overview to finish types of variable transformation Sometimes a linear regression model Y =  + x + + kxk + e does not fit because the model does not agree with the data In such case data transformation can help We consider two types of data transformation: transform X (or add some f(X)) transform Y SSO Lecture   /   variable transformation logistic regression overview to finish Transform explanatory variable () If a scatter plot of Y against an explanatory variable X shows a relation other than linear transform X p add f (X) to the model (eg X or X) Y~sqrt(X) xyY~exp^XxyY~SSO Lecture   /   variable transformation logistic regression overview to finish Transform explanatory variable () The difference between the linear model lm(yx) the transformed model lm(ylog(x)) −residuals yresiduals−yresiduals Plots of residuals against Y Left we see a curvature, right not really curved SSO Lecture   /   variable transformation logistic regression overview to finish Transform response variable () If a scatter plot of residuals against Y shows an increase or decrease in error size then transform Y , the response variable Example scatter (x,y) xy−−scatter xy−Problem: the residuals grow with increasing Y value (right picture) SSO Lecture   /   variable transformation logistic regression overview to finish Transform response variable () The difference between the linear model lm(yx+x) the transformed model lm(log(y)x+x) The residuals of the log(y) model do not show a specific pattern (bottom-right picture) SSO −residuals yresiduals(mylm) residualsLecture   /   variable transformation logistic regression overview to finish Example – brains () In a methodological study on mapping brain areas we compare  methods (FIRST,FS,Man),  disease groups (CTRL,AD,MCIN,MCIP), and  hemispheres (Left,Right) This yields -way ANOVA SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () Applying (additive) -way ANOVA to the raw data is problematic Yijkl = μ + i + j + k + eijkl > rawlm=lm(Response~Disease+Method+Hemisphere,data=) > qqnorm(residuals(rawlm)) −−−−−Normal QuantilesSample Quantiles SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () We look for a transformation of Y Shown: QQ-plot and p-values of Shapiro-Wilk test on residuals SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () FItting the additive -way ANOVA model on the Y  data: > mylm=lm(Response^~Method+Disease+Hemisphere,data=) > anova(mylm) Analysis of Variance Table Response: Y^ Df Sum Sq Mean Sq F value Pr(>F) Method     < e- *** Disease      ** Hemisphere      Residuals    Conclusion: Disease and Method matter significantly, Hemisphere does not SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () The researchers expect a possible interaction between Method and Disease Interactions can be modeled by Method*Disease or by Method+Disease+Method:Disease > mylm=lm(Response^~Method*Disease+Hemisphere,data=) > anova(mylm) Analysis of Variance Table Response: Y^ Df Sum Sq Mean Sq F value Pr(>F) Method     < e- *** Disease      ** Hemisphere      Method:Disease      Residuals    The interaction is not significant (p=) SSO Lecture   /   variable transformation logistic regression overview to finish Example – brains () We stay with the additive model, without interactions > mylm=lm(Response^~Method+Disease+Hemisphere,data=) > summary(mylm) Coefficients: Estimate Std Error t value Pr(>|t|) (Intercept)    < e- *** MethodFS -  - < e- *** MethodMan -  - < e- *** DiseaseCTRL     ** DiseaseMCIN     DiseaseMCIP     HemisphereRight     FS is worse than Man is worse than reference Method (FIRST) CTRL is best Disease group Hemisphere is not significant Remark All pairwise comparisons between different levels of all factors can be made using Tukey’s method SSO Lecture   /   variable transformation logistic regression overview to finish logistic regression SSO Lecture   /   variable transformation logistic regression overview to finish Logistic regression If the response variable Y is categorical (-) then we cannot use linear regression or anova/ancova For a - response variable we can model the probability P(Y =) as a function of explanatory variables The explanatory variables can be either numerical or categorical or a mix The logistic regression model (logistische regressie) is  P(Y = ) = P(Y =)=  − P(Y = ), + e−function , with function = μ + i + + x + , the mix of numerical and categorical explanatory variables In R: glm(yx+x+,family=binomial,data=mydata) SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () Consider (fictive) data on Toeslagfraude in  suspicious cases: > toeslagfraude fraude hoogtetoeslag friesland                             Question: does the probability of fraude depend on the height of the Toeslag and whether the person lives in Friesland? SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () Exploring the dependence on the numerical variable hoogtetoeslag: hoogtefraudehoogtefraudeFrequencyHistogram hoogtenietfraudehoogtenietfraudeFrequency>hoogtefraude=hoogtetoeslag[fraude==] >hoogtenietfraude=hoogtetoeslag[fraude==] >wilcoxtest(hoogtefraude,hoogtenietfraude) Wilcoxon rank sum test with continuity correction data: hoogtefraude and hoogtenietfraude W = , p-value =  alternative hypothesis: true location shift is not equal to  SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () Exploring the dependence on the categorical variable friesland: > xtabs(~fraude+friesland,data=toeslagfraude) friesland fraude        > chisqtest(xtabs(~fraude+friesland,data=toeslagfraude),simulatepvalue=TRUE) Pearson’s Chi-squared test with simulated p-value (based on  replicates) data: xtabs(~fraude + friesland, data = toeslagfraude) X-squared = , df = NA, p-value =  Fraude occurs significantly less often in Friesland SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () Fitting a logistic regression model with explanatory variables hoogtetoeslagand friesland: > fraudeglm=glm(fraude~friesland+hoogtetoeslag,family=binomial) > summary(fraudeglm) Call: glm(formula = fraude ~ friesland + hoogtetoeslag, family = binomial) Coefficients: Estimate Std Error z value Pr(>|z|) (Intercept) -  -  friesland -  -  ** hoogtetoeslag     * Both variables are significant The estimated coefficient for friesland is negative, which means friesland= yields lower probability on fraude than friesland= The estimated coefficient for hoogtetoeslag is positive, which means that probability on fraude increases with hoogtetoeslag SSO Lecture   /   variable transformation logistic regression overview to finish Example – Toeslag fraude () If the categorical explanatory variables has more than  categories the command drop is helpful: > fraudeglm=glm(fraude~friesland+hoogtetoeslag,family=binomial) > drop(fraudeglm,test=""Chisq"") Single term deletions Model: fraude ~ friesland + hoogtetoeslag Df Deviance AIC LRT Pr(>Chi) <none>   friesland      ** hoogtetoeslag      * It works the same as anova in ANOVA models SSO Lecture   /   variable transformation logistic regression overview to finish More advanced logistic regression If the categorical response variable has more than  categories the usual logistic regression model does not apply In that case one can use multinomial logistic regression For that you need special R packages SSO Lecture   /   variable transformation logistic regression overview to finish overview of module SSO Lecture   /   variable transformation logistic regression overview to finish Module overview () In this module we have discussed and practiced: probability distributions population — the underlying huge (infinite?) population sample — the sample measured summarizing data numerical ways — mean, median, sd, variance, range, etc graphical ways — histogram, boxplot, scatter plot, etc testing normality QQ-plots — graphical way to test normality Shapiro-Wilk test — numerical way to test normality estimation population mean — point estimate or confidence interval population proportion — point estimate or confidence interval SSO Lecture   /   variable transformation logistic regression overview to finish Module overview () hypothesis testing formal set up — eg H, H, test statistic, p-value one-sample tests t-test — testing population mean in normal setting binomial test — testing population proportion two-samples tests t-test for matched pairs — testing difference between population means t-test for independent pairs — testing difference between population means testing two proportions — testing difference between two population proportions non-parametric tests sign test — testing population median Wilcoxon one sample test — testing point of symmetry Wilcoxon two samples test — testing difference in two populations SSO Lecture   /   variable transformation logistic regression overview to finish Module overview () contingency tables chisquare test — testing independence or homogeneity between variables simple and multiple linear regression model — Y =  + x + + kxk + e estimation — using R testing — tests for H : i =  and H :  = = k = challenges © in linear regression diagnostics — several scatter plots to validate the model fit outliers — extreme in response variable leverage points — extreme in explanatory variable influence points — extreme in explanatory variable with influence collinearity — explanatory variables that explain the same SSO Lecture   /   variable transformation logistic regression overview to finish Module overview () anova one-way anova — one categorical explanatory variable Kruskal-Wallis — nonparametric alternative to one-way anova two-way anova — two categorical explanatory variables ancova — mix of anova and linear regression variable transformation transformation of X — if relation between Y and X is not linear transformation of Y — if residuals not ok, ie model does not fit logistic regression logistic regression — a - response variable SSO Lecture   /   variable transformation logistic regression overview to finish to finish SSO Lecture   /   variable transformation logistic regression overview to finish To wrap up Today we discussed two-way anova logistic regression variable transformation overview of all techniques ENJOY ANALYZING YOUR DATA SSO Lecture   /  "
"Statistics, Simulation & Optimization Lecture 9",stastics simulation and optimization,SSO,slides,machine scheduling modeling set time transportation afbeeldingsresultaat voor costs tardiness,min model regression robust shift decision covering variables period constraints,problem example cover job jobs lecture applications single schedule sources,,3780,46,Joost Berkhout,2019,"Statistics, Simulation & Optimization Lecture  Joost Berkhout, VU Course design by Prof Ger Koole (VU)  About Me • Background: Operations research • Current positions : • Main research project : • Other research interests: Theory and application of Markov chains ( , social networks) C:\Users\MQ\Downloads\Presentatie Techport (--)\CWI_Logo_Croppedjpg Gerelateerde afbeelding Afbeeldingsresultaat voor google Afbeeldingsresultaat voor fabriek mengvoer Production scheduling at industrial plants Gerelateerde afbeelding Gerelateerde afbeelding Biomaterials (eg, animal feed) How to schedule the production orders?  Using data and predictions: What should we do? More value, but less used* Statistics part * Source: renowned consultancy company Gartner, see here and here Simulation & optimization part Less value, but more used  Optimization ≈ prescriptive analytics Typical problems are: • What is the best way to schedule production orders on a machine? • How should we efficiently deliver the goods in time? • What price to ask for airplane seats over time to get the most profit? Afbeeldingsresultaat voor delivery Afbeeldingsresultaat voor airplane klm  Course objectives • Introduction to the process of optimization: – Modeling (simulation) – Optimization techniques for: • Deterministic problems • Stochastic problems (addressing uncertainty) – Optimization and simulation software • Able to simulate and optimize problems and interpret results • Recognize optimization opportunities  Program • Lecture : Linear optimization • Lecture : Advanced modeling • Lecture : Solvers, heuristics, complexity • Lecture : Simulation • Lecture : Simulation optimization • Lecture : Dynamic optimization • Lecture : W rap - up  Organization Organization in same line as statistics part ( see Canvas for details) Some specific notes simulation & optimization part: • Slides contain all information • Book developed for this course: • Main tools for assignments: Excel & AMPL An Introduction to Business Analytics  Background literature • Guenin et al (pretty technical) • Foreman (broader, Excel) • Winston or any other Introduction to Operations Research/ Management Science • AIMMS Optimization Modeling (book) • Most books are technical – meant for Bachelor students IE/ maths or more advanced vemNGbL_SX_BO,,,_jpg xOAiu+wL_SX_BO,,,_jpg Screen Shot -- at  png  Program • Lecture : Linear optimization • Lecture : Advanced modeling • Lecture : Solvers, heuristics, complexity • Lecture : Simulation • Lecture : Simulation optimization • Lecture : Dynamic optimization • Lecture : W rap - up  About real - life problems Afbeeldingsresultaat voor delivery Afbeeldingsresultaat voor airplane klm In general: Decision System Outcome Real - life problem : Which feasible decision gives the best outcome? “decision variables” “constraints” “objective function”  About optimization Today : • Linear optimization (LO) • Integer linear optimization (ILO) Universal modeling “language” = mathematics Real - life problem Model* Modeling Decision Optimization/solving Data Data analysis Aka (integer) linear programming * Accurate but not too complex to be optimizable (capture the problem essence)  Motivating LO problem: optimal product mix •  products composed of  resources • The product info per quantity is: • Availability:  units of resource ,  of  • Problem: What is the optimal product mix with maximum profit? Profit in € Units resource  required Units resource  required Product     Product      Modeling approach  Study problem (setting) in detail  Define decision variables  Define objective  Define the restrictions/constraints Real - life problem Model Modeling Decision Optimization/solving Data Data analysis  Product mix model Model structure : • Decision variables: x i = quantity of i produced , i = ,  • Objective : “ maximize profit ” = max x  + x  • Constraints : – “units resource  used ” ≤  ⟺ x  ≤  – “units resource  used ” ≤  ⟺ x  + x  ≤  – x i ≥ , i = ,  All equations are of a linear nature: linear optimization  Solving the model For example using solvers in: • Excel • R • Python • … Real - life problem Model Modeling Decision Optimization/solving Data Data analysis  Excel solver  Aside: on errors in Excel • Excel does not enforce structure • Excel sheets often used repeatedly • Add structure and readability by: – separating formulas and data – colors and variables – delimitating input ranges – documentation • Little theory, huge impact Screen Shot -- at  png Powell et al   R Install LO package once: installpackages ("" lpSolve "") Screen Shot -- at  png  Python  Optimization What is happening when optimizing the product mix problem? • A graphical view • An algebraic view  A graphical view             x  x  Constraint : x  ≤  Constraint : x  + x  ≤           Constraint : x  ≥   Constraint : x  ≥  Feasible region Objective function: x  + x  =  No solutions on this line lie in the feasible region … Idea: slide red line in parallel towards feasible region “Direction of increasing objective value”  A graphical view            x  x            Feasible region Objective function: x  + x  =  All the solution on this line - part are feasible and have objective value of , but we can do better …   A graphical view            x  x            Feasible region Objective function: x  + x  =  Optimal solution = (, ) with value   Important insight: an optimal solution to LO is always a corner because only linear/straight lines  An algebraic view max x  + x  st x  ≤  st x  + x  ≤  st x i ≥ , i = ,  ⟺ max x  + x  st x  + y  =  st x  + x  + y  =  st x i , y i ≥ , i = ,  Variable y i indicates the amount of slack in the i - th constraint •  equations,  variables ⇒  variables can be  • Every feasible region corner corresponds to a solution for which  variables are zero …  An algebraic view            x  x            Feasible region  x  x  y  y                  max x  + x  st x  + y  =  st x  + x  + y  =  st x i , y i ≥ , i = ,   An algebraic view            x  x            Feasible region  Simplex method ( Dantzig ) • Solution method: go from corner to corner improving every time • Linearity: local optimum = global optimum dantzig “simplex step” “No improvement: local optimum found”  Possible outcomes LO Optimal solution can be found x  x  Feasible region Unbounded… x  x  Feasible region direction of increasing objective value Eg: Product mix problem with option to buy resource  at /unit No solutions x  x  Eg: Product mix problem with a client who needs  units of product   LO example : project planning Activities, precedence constraints, durations: What is earliest finish time of project?  Can be found using LO Screen Shot -- at  png   A  B D  C  E F  G  Graph model: Problem: Minimize project finish time by setting the activity starting times  • Decision variables: x i = start time activity i , ∀i • Objective: min “project finish time” = min max (x i + d i ) • Constraints: • Optimization model: A constraint: x A + d A ≤ x B min z st z ≥ x i + d i , ∀i st x i + d i ≤ x j , if i  j st x i ≥ , ∀i min max (x i + d i ) st x i + d i ≤ x j , if i  j st x i ≥ , ∀i LO model: d A B (for all arcs) Nonlinear objective, so no LO… ( ∀ means “for all”) LO example: project planning  Equalities and linearity • LO matrix notation: max { p T x | Ax≤b , x≥ } • Covers also “min”, “≥”, “=”, x unrestricted • Proof:  min p T x = - max - p T x  Ax ≥ b ⟺ - Ax ≤ - b  Ax = b ⟺ Ax ≤ b & Ax ≥ b  x free ⟺ x = x + - x - with x + , x - ≥  • Essential for LO: objective and constraints linear • Otherwise: nonlinear optimization = different, less efficient algorithms • Important class: integer linear optimization (ILO)  ILO • LO with extra constraints of the form: x i ∈ {,,,} • Often: x i binary, ie, x i ∈ {,} • Excel solver: • R: lp (""max"", fobj, fcon , fdir , frhs , allint=TRUE) or allbin =TRUE Screen Shot -- at  png (“ ∈ ” means “element in”)  Integer product mix problem            x  x            Feasible region  max x  + x  st x  ≤  st x  + x  ≤  st x i ∈ {,,,} , i = ,  Integer infeasible solution Optimal LO solution = (, ) but non - integer … Objective function: x  + x  =  Integer feasible solution  Integer product mix problem            x  x            Feasible region  max x  + x  st x  ≤  st x  + x  ≤  st x i ∈ {,,,} , i = ,  Optimal I LO solution = (, ) Objective function: x  + x  =   Branch & Bound method • Method to find the optimal ILO solution • Key concepts: – LO relaxation ignores integer constraints in ILO: • Replace x i  {,} by  ≤ x i ≤  • Replace x i  {,,,…} by x i ≥  – Branching on a non - integer LO relaxation solution • Example: x  =  → Consider  subproblems/subtrees with extra restrictions x  ≤  and x  ≥ , resp – Bounds in case of maximization : – Eliminate subproblem if its UB ≤ (best) LB The objective value of the LO relaxation is an upper bound (UB) for the ILO problem The objective value of a feasible ILO solution is a lower bound (LB) for the ILO problem (this make the method “intelligent”)  Branch & Bound method Idea: Iteratively solve LO relaxation of a (sub)problem, branch (sub)problem into  subproblems, and try eliminating subproblems by using the bounds Subproblem Branching If UB ≤ (best) LB : eliminate subproblem Continue till all subproblems are eliminated, with the exception of one subproblem whose LO relaxation solution is the optimal ILO solution “Original problem” Afbeeldingsresultaat voor check Stop branching at subproblem when LO relaxation solution is integer (gives a LB )  Example: Integer product mix problem            x  x            Feasible region  Solution LO relaxation= (, ) Objective function: x  + x  =  Branch on non - integer   Example: Integer product mix problem            x  x            Feasible region  Objective function: x  + x  =  Branch : x  ≤  Solution LO relaxation with branch  = (, ) Branching stops at subtree since LO relaxation solution is integer Afbeeldingsresultaat voor check  Example: Integer product mix problem            x  x            Feasible region  Solution LO relaxation with branch  = (, ) Objective function: x  + x  =  Branch : x  ≥  Branching stops at subtree since LO relaxation solution is integer Afbeeldingsresultaat voor check  Example: Integer product mix problem x  ≤  x  ≥  LB left subtree < LB right subtree , so (, ) is the ILO solution (ie, eliminate left subtree )  Example ILO: Knapsack problem • Number of items, size and reward given • Limited space • Which items to take to maximize reward? • Practical example: portfolios optimization • Example: space ,  items: Item        Size        Reward        Afbeeldingsresultaat voor knapsack  Knapsack example ILO formulation: Decision variables: x i = , i =, …,  Objective: max x  + x  + x  + x  + x  + x  + x  Constraints: x  + x  + x  + x  + x  + x  + x  ≤  x i  {,}, i = , …,  Solution according to Excel: value , items // , take item i , else As practice: verify this in Excel  Legend: UB = upper bound at branch LB = lower bound original ILO Subopt = suboptimal solution Knapsack Example Step  :  (UB) , , , , /, ,  Step :  (UB) , , /, , , ,  Step :  (UB) , / , , , , ,  Step  :  (UB) , , , , , /,  Step :  (LB) , , , , , ,  Step  :  ( LB) , , , , , ,  Step  :  (LB) , , , , , ,  x  =  x  =  x  =  x  =  x  =  x  =  Step : subopt Step : subopt Solving LO relaxation: Take items in the order of most reward/size until space is full Item        Size        Reward        Reward / size        Step :  (UB) , , , ,/, ,  Step :  (UB) /, , , , , ,  x  =  x  =  Step : UB ≤ LB Step : UB ≤ LB Step : optimum  Recap today Modeling and solving problems with: • Linear optimization (LO): o Graphical view o Simplex algorithm • Integer linear optimization (ILO): o Branch & bound Real - life problem Model Modeling Decision Optimization/solving Data Data analysis "
"Statistics, Simulation & Optimization Lecture 10",stastics simulation and optimization,SSO,slides,algorithm maximum source complexity real practical languages tools problems increase, find problem example optimization fulkerson afbeeldingsresultaat voor tour heuristics life,flow path modeling augmenting ford nodes algorithms shortest solvers tsp,,3294,60,Joost Berkhout,2019,"Statistics, Simulation & Optimization Lecture  Joost Berkhout, VU  Program • Lecture : Linear optimization • Lecture : Advanced m odeling • Lecture : Solvers, heuristics, complexity • Lecture : Simulation • Lecture : Simulation optimization • Lecture : Dynamic optimization • Lecture : W rap - up  Today Last week: (integer) linear optimization (ILO) Today : • An overview of applications (I)LO:  Demonstrating (I)LO applicability  Inspiration for modeling own real - life problems • Modeling tricks Real - life problem Model Modeling Decision Optimization/solving Data Data analysis  Approach to applications • Practical motivation • Modeling:  Problem definition and an example  Define decision variables  Define objective  Define the restrictions/constraints • Modeling of example • Possible extension(s)  Applications (I)LO • Transportation problem • Set cover problem & covering problem • Machine scheduling • Robust regression • Multi - period models  Transportation problem Motivating example Afbeeldingsresultaat voor warehouse symbol Afbeeldingsresultaat voor warehouse symbol Afbeeldingsresultaat voor customers symbol Afbeeldingsresultaat voor customers symbol Afbeeldingsresultaat voor customers symbol Warehouses Customers Gerelateerde afbeelding Gerelateerde afbeelding How to arrange the transportation efficiently?  Transportation problem definition • Transport of a single good • n sources and m destinations with connections given • Supply a i at source i • Demand b j at destination j • Transportation costs c ij per unit transported on i → j • Problem: What is the cheapest transport to fulfil demand while respecting the supply?   n Sources   m Destinations  a  a  a n b  b  b  b m c  c nm c n c  c  c  c   Transportation problem example Example with  sources and  destinations:    Sources   Destinations            Source \ dest   Supply             Demand    Transportation problem LO model ( in case no i  j, set c ij large) : • Decision variables: x ij = quantity transported from i  j , ∀i , j • Objective : “ minimize costs ” = min ∑ i,j c ij x ij • Constraints :  Max supply : ∑ j x ij ≤ a i , i = , …, n  Fulfil demand : ∑ i x ij ≥ b j , j = , …, m  x ij ≥ , ∀i , j As practice : Give the specific model for the example and solve it in Excel (answers on next slide)  Transportation problem: LO model for example    Sources   Destinations            Example: LO model: min ( x  +  x  +  x  min ( +  x  +  x  +  x  ) st st x  + x  ≤  st x  + x  ≤  st x  + x  ≤  st x  + x  + x  ≥  st x  + x  + x  ≥  st x ij ≥ , i =,,, j=, Solution with value : •  on →, → •  on →  Transportation problem Extension: • Intermediate nodes • Additional constraints for all intermediate node k: ∑ i x ik = ∑ j x kj • Known as Transshipment problem • Can be extended to network   n Sources   m Destinations  k c k c km c nk c k  Applications (I)LO • Transportation problem • Set cover problem & covering problem • Machine scheduling • Robust regression • Multi - period models  Set cover problem Motivating examples Afbeeldingsresultaat voor map amsterdam Afbeeldingsresultaat voor ambulance Afbeeldingsresultaat voor ambulance Afbeeldingsresultaat voor ambulance Find the lowest number of ambulance locations to cover a region Efficient virus scanning (IBM) Sets of virus code known with many overlap, scan for the sets from set cover Gerelateerde afbeelding From: http:// mathmitedu /~ goemans /S/ setcover - tamarapdf  Set cover problem definition Given: • Universe U = {, , , m} that has to be covered • Sets S  , S  , , S n with S i ⊆U , i = , …, n Goal: Find the smallest number of sets covering U    S  S  S  Example: U = {, , }, S  = {, }, S  = {, } and S  = {, } ( ⊆= “subset of”)  Set cover ILO model ILO model: • Decision variables: x i = , i = , …, n • Objective : “ minimize number of sets” = min ∑ i x i • Constraints :  Cover universe : let a iu = , ∀ i , u, then ∑ i a iu x i ≥  , ∀ u ∈ U  x i ∈ {,} , i = , …, n , take S i , else , if u ∈ S i , else As practice : Give the specific model for the example and solve it in Excel (answers on next slide)  Set cover problem example U = {, , }, S  = {, }, S  = {, } and S  = {, } ILO model: min x  + x  + x  st x  + x  ≥  (“cover u = ”) st x  + x  ≥  (“cover u = ”) st x  + x  ≥  (“cover u = ”) st x i ∈ {, }, i = , ,     S  S  S  Optimal solution: take any two sets (objective value ) Are integrality constraints necessary? Yes: the optimal solution of LO relaxation is x i = , ∀ i (objective value )  Covering problem Generalization of set cover problem : • Application: workforce scheduling • Each u ∈ U should be covered b u times • Set can be chosen multiply times: x i = number of S i chosen • Each chosen S i costs c i ILO model: min ∑ i c i x i st ∑ i a iu x i ≥ b u , ∀ u ∈ U st x i ∈ {,,,}, ∀ i Afbeeldingsresultaat voor workforce scheduling  Covering problem example Personnel shift scheduling • There are  shift types that can be scheduled: • Find the cheapest shift schedule from  - h such that the following requirements are met: Shift  Shift  Shift  Shift  Costs (euro)     Working hours  - h  - h  - h  - h  - h  - h  - h  - h Number of workers required     Following from predictive analytics  Covering problem example Personnel shift scheduling • U = {,,,} representing the h time intervals from  till h, resp • b u = required staffing during u ( b = [, , , ] T ) • Set i consists of working time intervals shift i: S  = {, }, S  = {, }, S  = {, }, S  = U • a iu = , if shift i works at time u,  else, eg, [ a u ] = [, , , ] • c i = costs of shift i ( c = [, , , ] T ) • x i = number of workers with shift i  Covering problem example Personnel shift scheduling ILO min  x  +  x  +  x  +  x  st x  + x  ≥  (u =  →  - h) st x  + x  + x  ≥  (u =  →  - h) st x  + x  + x  ≥  (u =  →  - h) st x  + x  ≥  (u =  →  - h) st x i ∈ {, , , …}, i = , , ,  Optimal solution: x = [, , , ], ie, schedule  times shift  and  times shift  Total costs :  euro  Applications (I)LO • Transportation problem • Set cover problem & covering problem • Machine scheduling • Robust regression • Multi - period models  Machine scheduling Motivating example Related example: project planning Screen Shot -- at  png List of sales orders Gerelateerde afbeelding Production facility Find most “efficient” schedule while respecting restrictions such as due dates and machine restrictions Afbeeldingsresultaat voor fabriek mengvoer  Machine scheduling Single - machine scheduling problem • Given n jobs with, for i = , …, n:  s i = duration job i  r i = release date job i  d i = due date job i  c i = costs due date exceedance job i • Goal: schedule that minimizes the weighted tardiness sum Example: Schedule n =  jobs with Job i s i r i d i c i                For example: Schedule      has a weighted tardiness of  “maximum of delay and ”  Machine scheduling Modeling for single - machine : • Decision variables: x i = start time job i • Objective : min ∑ i c i * max( x i + s i - d i , ) • Constraints : a) Release dates: x i ≥ r i , ∀ i b) Prevent overlap of jobs on single - machine c) Model tardiness of jobs linearly Tardiness job i (nonlinear) Next two slides, respectively  Machine scheduling b) Prevent overlap of jobs on single machine: • Prevent overlap if job i before job j: x i + s i ≤ x j • But order unknown … • Introduce decision variable for order: y ij = ( i ≠ j) with restriction y ij + y ji =  • Using y ij and a M big, avoid overlap among jobs: x i + s i ≤ x j + M y ji , for all i,j , i ≠ j Issue: many y ij variables , if i→j , if j→i Two possibilities:  If i→j ( y ji = ): x i + s i ≤ x j (“job j starts after job i is done”)  If j→i ( y ji = ): x i + s i ≤ x j + M ( not restrictive if M big)  Machine scheduling c) Model tardiness of jobs linearly: • How to cover max( x i + s i - d i , )? • Let decision variable z i be the tardiness of job i • To give z i its meaning, add restrictions: z i ≥ x i + s i - d i , for all i z i ≥ , for all i • Claim: when minimizing z i (which we do) it holds that z i = max( x i + s i - d i , ) Tardiness job i  Machine scheduling ILO model for single - machine (see prev  slides) : min ∑ i c i z i (weighted tardiness) st x i ≥ r i , ∀ i (release dates) st x i + s i ≤ x j + M y ji , ∀ i,j , i≠j , M ≫  (overlap) st y ij + y ji = , ∀ i,j , i <j (order) st y ij ∈ {,}, ∀ i,j , i≠j st z i ≥ x i + s i - d i , ∀ i (tardiness) st z i ≥ , ∀ i For n jobs: n  + n variables & n  constraints!  Machine scheduling example Example: For simplicity only jobs  &  min z  + z  st x  ≥ , x  ≥  (release dates) st x  +  ≤ x  + M y  (order) st x  +  ≤ x  + M y  st y  + y  =  st y  , y  ∈ {,} st z  ≥ x  +  -  (tardiness) st z  ≥ x  +  -  st z  ≥  , z  ≥ , M =  Job i s i r i d i c i                To practice: • Verify the above in Excel • Formulate the ILO for the complete example • Solve the complete example in Excel (answer: x  = , x  =  & x  = ) Solution x  =  and x  = , ie, schedule     Machine scheduling Other common objectives: • Flowtime = ∑ i ( x i +s i ) • Makespan * = max i ( x i +s i ) • Weighted sum of makespan & tardiness * To minimize the nonlinear makespan : min max i ( x i +s i ) ⟺ min z st x i +s i ≤ z, ∀ i Note that this is just the “project planning trick”  Applications (I)LO • Transportation problem • Set cover problem & covering problem • Machine scheduling • Robust regression • Multi - period models  Robust regression Motivating example: production time estimation  Robust regression Motivating example: production time estimation Zoomed Quantile regression with p =  is more robust regarding outliers than OLS regression  Robust regression problem • Given datapoints (x i , y i ), i = , …, n • Goal: find a & b that solve min ∑ i | y i – ( a + b x i )| • Example: Sum of absolute deviations from line         x y        ? i x i y i          Datapoints : “Which line a + b x minimizes the total length of all dotted arrows?”  Robust regression model Modeling : • Decision variables:  Coefficients a and b of line a + b x  Error e i of data point i • Objective : min ∑ i | e i | • Constraints : e i = y i – ( a + b x i ), i = , …, n Non linear objective  no LO problem But it can be casted into a LO using a modeling trick …  Robust regression modeling trick • min ∑ i | e i | st e i = y i – ( a + b x i ), ∀i (P) • LO modeling trick for (P) :  Introduce decision variables : e i + ≥  and e i - ≥ , for all i  Replace | e i | with e i + + e i -  Replace e i for e i + - e i - • LO for (P) : min ∑ i e i + + e i - st e i + - e i - = y i – ( a + b x i ), ∀i st e i + , e i - ≥ , ∀i  Robust regression modeling trick • Claim: When optimizing min ∑ i e i + + e i - st e i + - e i - = y i – ( a + b x i ), ∀i st e i + , e i - ≥ , ∀i e i + or e i - is  for each i • Consequence for optimum: – If e i + >  then e i - = , thus e i + = y i – ( a + b x i ) > : “ e i + gives distance datapoint i above the line” – If e i - >  then e i + = , thus – e i - = y i – ( a + b x i ) < : “ e i - gives distance datapoint i below the line” x i y i a b e i + “Optimal line” y j x j e j -   Robust regression example LO model: min ( e  + + e  - + e  + + e  - + min ( e  + + e  - ) st e  + - e  - =  – ( a + b ) st e  + - e  - =  – ( a + b ) st e  + - e  - =  – ( a + b ) st e i + , e i - ≥ , i = ,,   ? e  + e  - e  -        x y         i x i y i          Datapoints :   Robust regression example i x i y i          Datapoints : LO model: min ( e  + + e  - + e  + + e  - + min ( e  + + e  - ) st e  + - e  - =  – ( a + b ) st e  + - e  - =  – ( a + b ) st e  + - e  - =  – ( a + b ) st e i + , e i - ≥ , i = ,,   e  + ≈         x y         Optimal solution: a ≈ , b ≈ - , e  + ≈ , e  - =  & e  + = e  - = e  + = e  - =  y =  – x  In general: minimizing absolute values • Non linear problem (in matrix form): min { c T |x | | Ax ≥ b }, c ≥  can be casted into a LO as: min { c T (x + +x - ) | A(x + - x - ) ≥ b, x + ,x - ≥  } • Claim: either x i + or x i - is  in optimum, ∀i • Robust regression (in matrix form): min {  T |e| | e = y – ( a  + b x ) }  min { T (e + +e - ) | e + - e - = y - ( a + b x), e + ,e - ≥} • Quantile regression (weighted objective) : min p  T e + + ( - p)  T e - , < p < Possibility via p to focus more on upper or lower datapoints ( p =  is robust regression)  Applications (I)LO • Transportation problem • Set cover problem & covering problem • Machine scheduling • Robust regression • Multi - period models  Multi - period models Models where a system state is tracked over time periods Motivating example: inventory management Afbeeldingsresultaat voor inventory procurement Afbeeldingsresultaat voor inventory procurement When should we increase inventory such to minimize costs and be able to satisfy (varying) demand?  Multi - period inventory problem definition • Stock with a single product • s  = initial stock at time  • For time t = , …, T (time horizon):  d t = demand at t  h t = holding costs/product at t  c t = order costs/product at t • Goal: Find procurement strategy that minimizes total costs while satisfying demand at each time Parameters d t & c t may follow from predictive modeling !   - period inventory problem example • T =  time periods with initial stock s  =  • For the parameters it holds: • Sketch of idea: Time t Demand d t Holding costs h t Order costs c t                  Time t  “demand  & supply x  ” Stock at t = : s  =  –  + x  s  =  Et cetera …  Multi - period inventory problem LO model: • Decision variables for t = , …, T: x t = supply/production at t s t = stock at t • Objective : min ∑ t ( c t x t + h t s t ) • Constraints : s t = s t -  – d t + x t , for t = , …, T s t , x t ≥ , for t = , …, T Note many extension possible, eg: • maximum stock • production capacity constraints s t ≥  ensures that all demand is met   - period inventory problem example LO model min  x  +  x  +  x  + s  + s  + s  st s  =  –  + x  st s  = s  –  + x  st s  = s  –  + x  st x  , x  , x  ≥  st s  , s  , s  ≥  Optimal solution (costs = ): Parameters:  Recap today Today: • Applications ( I)LO • Modeling tricks Real - life problem Model Modeling Decision Optimization/solving Data Data analysis See also the book AIMMS Optimization Modeling for an overview of other modeling tricks "
"Statistics, Simulation & Optimization Lecture 11",stastics simulation and optimization,SSO,slides,simulation sample example r(x) samples lecture cdf average f(x) interval,afbeeldingsresultaat function uniform thus normal des next process optimization problem,random distribution time event discrete voor probability pdf,,2437,39,Joost Berkhout,2019,"Statistics, Simulation & Optimization Lecture  Joost Berkhout, VU  Program • Lecture : Linear optimization • Lecture : Advanced modeling • Lecture : Solvers, heuristics, complexity • Lecture : Simulation • Lecture : Simulation optimization • Lecture : Dynamic optimization • Lecture : W rap - up  Today: Chance to win  million dollar! Afbeeldingsresultaat voor money funny  Today Last lecture: Applications ILO & modeling tricks Today: Practical aspects of real - life optimization: • Solvers • Modeling tools: Algebraic modeling languages and environments • Algorithms • Complexity theory • Heuristics Real - life problem Model Modeling Decision Optimization/solving Data Data analysis  Outline Practical aspects of real - life optimization: • Solvers • Modeling tools: Algebraic modeling languages and environments • Algorithms • Complexity theory • Heuristics  Optimization software • Best known optimization tool: Excel solver (free add - in) • But: many other (much better!) open source & commerci al tools ( sometimes with Excel interface) • F ree Excel alternative: OpenSolver • Current best solvers for ILO problems: Afbeeldingsresultaat voor excel Afbeeldingsresultaat voor gurobi Afbeeldingsresultaat voor cplex IBM Afbeeldingsresultaat voor FICO XPRESS logo  Screen Shot -- at  png From https:// enwikipediaorg /wiki/ List_of_optimization_software market leader developed by academics bought by IBM license ≈ K/ yr  History optimization software • Timeline –  - : LO solvers • Simplex method • Interior point methods – Since s: major advances in ILO and NLO solvers Source: talk Linderoth at FOCAPO   About solving in practice • Many practical problems solvable by advanced solvers nowadays • However, not always: – Very large instances – Special cases (intrinsically difficult problems or evil academic problem instances) Real - life problem Model Modeling Decision Optimization/solving Data Data analysis  Outline Practical aspects of real - life optimization: • Solvers • Modeling tools: Algebraic modeling languages and environments • Algorithms • Complexity theory • Heuristics  About modeling • First, make sure you thought deeply about problem • Has a similar problem been solved before? • D edicated tooling (decision support systems) available? Afbeeldingsresultaat voor navigation problem Afbeeldingsresultaat voor do not reinvent the wheel quote For example: Navigation software  About modeling Once decided to model a problem:  Study problem in detail  Define decision variables  Define objective  Define the restrictions/constraints Possibly using: Algebraic modeling languages (AML): • Problem formulation in a “standard language” • Similar style as mathematical notation • Act as interface between problem & computer solver  Algebraic modeling languages (AML) Advantages: • Relatively quick modeling • Easy to communicate • Splitting of model & data for easy recalculations • Possibility to call different solver engines • User interface to make reports Disadvantage: • Can be expensive • Inflexible when embedding in other software • Learning curve Best known AML: • AMPL, AIMMS, GAMS, LINDO, MPL • INFORMS Impact prize  Also IDE (integrated development environment) Demonstration in this lecture  Knapsack problem (see Lecture ) • Number of items, size and reward given • Limited space • Which items to take to maximize reward? • Example: space ,  items: Afbeeldingsresultaat voor knapsack Item        Size        Reward         AMPL • AMPL (an AML) is composed of  files • For knapsack problem example: Model file (mod) Data file ( dat ) Run file Model and data separation allows for quick optimization of new instances Note that the standard modeling structure comes forward!  More on AMPL • AMPL book online: http://amplcom/resources/the - ampl - book/chapter - downloads/ • Debugging: start simple and increase complexity • Some syntax: –  - dim binary variables: var x {N, M} binary; – constraint for all i,j , i≠j : subject to example_constraint { i in N, j in M: i <>j}: x[ i,j ] <= ; Mind the closing sign!  NEOS server • Free service for solving optimization problems at wwwneos - serverorg/neos/ • Provides access to >  state - of - the - art solvers • Optimization routine: Transform o ptimization problem to suitable input Suitable for a problem type (ILO, LO, …) and the requested solver Upload Solve Output (solution) Using AMPL & CPLEX in our case  AIMMS Screenshot I Definition of the objective Complete IDE to help setup the model  AIMMS Screenshot II Afbeeldingsresultaat voor aimms Possibility to develop (online) applications for end - users  Outline Practical aspects of real - life optimization: • Solvers • Modeling tools: Algebraic modeling languages and environments • Algorithms • Complexity theory • Heuristics  Algorithms “Set of instructions to find a solution for a problem” Example problem: Get groceries from a shopping list Algorithm for groceries (without shoplifting):  Start: Go to a supermarket  While “not all products picked from shopping list”: Choose next product from list and pick it up  Go to checkout and pay Gerelateerde afbeelding  Algorithms • Mainly executed by computers • Examples: – Simplex method is an algorithm to solve linear optimization problems – Other algorithms exist to solve other (types of) optimization problems • Sometimes dedicated ( Dijkstra , Ford - Fulkerson) • Sometimes general (branch & bound, genetic algorithms, dynamic optimization) • Sometimes optimum guaranteed - sometimes best - effort ( heuristic ) • Complexity ≈ runtime of algorithm  Rest of algorithms outline For the following  classical problems: – Shortest path – Maximum flow – Traveling salesman = shortest tour we will present (resp): • an (I)LO formulation • an algorithm  Shortest path problem • Given a network with distances given by c ij ’s (≥), find a shortest path from source s to destination d • Example: Find shortest path from s = A  d = F Afbeeldingsresultaat voor navigation “Distance from D  F is c DF = ” A C F B D E           LO of shortest path • LO = special case transshipment problem with demand  at destination • Decision variables: x ij =  if i → j used,  else • Objective: min ∑ i,j c ij x ij ( c ij = ∞ if no i  j) • Constraints:  ∑ j x s j - ∑ j x j s =  for s = source  ∑ i x i d - ∑ j x d j =  for d = destination  ∑ j x ij = ∑ j x ji for all other nodes i  x ij ≥ , ∀ i,j n  variables (n nodes) n constraints Special LO: no need to force x ij  {,} (i f solution non - integer: any >  connection is optimal)  Dijkstra’s algorithm • Algorithm by Edsger Dijkstra in  • Finds the shortest paths from source to all nodes • Important: non - negative connections • Starting from the source, we will iteratively visit all nodes (once), this leads to: – Visited nodes which we put in set W – Unvisited nodes Afbeeldingsresultaat voor dijkstra Edsger Dijkstra ( – ) Once visited, Dijkstra proved that we have found a shortest path from source to this visited node  Dijkstra’s algorithm • We will construct for growing W for all x: d W (x) = shortest path length from source to x using only intermediate nodes from set W • Dijkstra’s algorithm:  Start: W = empty, d W (“source”) = , d W (“rest”) = ∞  While “ W contains not all nodes”:  Find x not in W with smallest d W (x), say x’  Visit x’ (add to W ) and thereby making it current  Update d W (x) = min{ d W \ x’ ( x’ ) + c x’ ,x ; d W \ x’ (x)}, for all x not in W • When updating: save “previous node” • Runtime in case of n nodes = n  , very fast! Screen Shot -- at  png Our aim: d “all nodes” (“ dest ”) W \ x’ = W without x’  Dijkstra’s algorithm Example: Find shortest path from A  F Step \ x A B C D E F Current W Start  ∞ ∞ ∞ ∞ ∞ A {} Step   (A)  (A) ∞ ∞ ∞ C {A} Step   (A)  (C)  (C) ∞ B {A, C} Step   (C)  (C) ∞ E {A, C, B} Step   (E)  (E) D {A, C, B, E} Step   (E) F {A, C, B, E, D} Step  {A, C, B, E, D, F} A C F B D E          A C F B D E                Distances d W (x) throughout algorithm: “The shortest path from A to D is A  C  E  D and has length ”  Maximum flow problem Motivating example Road network analysis: What is the maximum traffic flow between two locations? Any bottlenecks? Afbeeldingsresultaat voor file nederland Gerelateerde afbeelding  Maximum flow problem • Given a directed network with: – Source node s – Destination node d – Edge capacity c ij between all nodes i and j • Goal: find the maximum feasible flow from source to destination that respects: – The capacity constraints – Flow conservation principle: what comes into a node, should also come out (except at nodes s & d )  Maximum flow example Example: Find the maximum flow from s =A  d =F given the flow capacities over the edges A C F B D E          Note: eg, A  D can be thought of as capacity c AD =  c DF =  means that the maximum flow from D  F is   Maximum flow LO (n nodes): • Decision variables: x ij = flow from i  j , ∀ i , j • Objective: max ∑ j x s j ( s = source) • Constraints:  Capacity constraints: x ij ≤ c ij , ∀ i , j  Flow conservation ( d = destination): ∑ i x ij = ∑ i x ji , ∀ j: j ≠ s and j ≠ d  Non - negative flow: x ij ≥ , ∀ i , j n  variables n –  constraints n  constraints n  constraints  Maximum flow algorithm An augmenting path from s to d is only composed of: • Forward edges i  j whose capacities are not reached • Reversed edges j  i with positive flow Key idea: flow from source to destination can be increased along the augmenting path Idea Ford – Fulkerson algorithm: when no augmenting path can be found anymore, a maximum flow is reached (by increasing flow on every forward edge and decreasing the flow on the reversed edges as much as possible)  Ford – Fulkerson algorithm Ford – Fulkerson algorithm: Repeat until “no more augmenting path”  Find augmenting path from s to d  Increase flow along augmenting path and update edge capacities available (residual graph) Optimality test: – Any s - d flow is no greater than any cut value – Maximum flow = minimum cut value The total capacity of edges that when cut , disconnects s from d  Ford – Fulkerson example Find maximum flow from A  F B C F B D E           Ford – Fulkerson example Find maximum flow from A  F  Find augmenting path from A to F B C F B D E           Ford – Fulkerson example Find maximum flow from A  F  Increase flow along augmenting path and update edge capacities B C F B D E /  /      / Max flow along the augmenting path is determined by this capacity “A flow of  flows from D to F which has a capacity of ”  Ford – Fulkerson example Find maximum flow from A  F  Find augmenting path from A to F B C F B D E /  /      / Max flow along the augmenting path is determined by this capacity, ie, flow increase of   Ford – Fulkerson example Find maximum flow from A  F  Increase flow along augmenting path and update edge capacities B C F B D E / / /   /   /  Ford – Fulkerson example Find maximum flow from A  F  Find augmenting path from A to F B C F B D E / / /   /   /  Ford – Fulkerson example Find maximum flow from A  F  Increase flow along augmenting path and update edge capacities B C F B D E / / / /  / /  / Note: the capacity from C  D is reached now, but we now may push a flow of max  back from D  C if that would give us an augmenting path  Ford – Fulkerson example Find maximum flow from A  F  Find augmenting path from A to F B C F B D E / / / /  / /  /  Ford – Fulkerson example Find maximum flow from A  F  Increase flow along augmenting path and update edge capacities B C F B D E / / / / / / /  /  Ford – Fulkerson example Find maximum flow from A  F  Find augmenting path from A to F B C F B D E / / / / / / /  / Not possible anymore to find an augmenting path! Ford - Fulkerson Algorithm: a max flow has been found of value  This cut has a capacity of ++ =  Since the max flow value is  this verifies the optimality of the max flow  Ford – Fulkerson example Illustration augmenting flow with reversed edge • Goal is to find the maximum flow from A  D • Suppose after a first iteration we have the following C B / A  / D /   Find augmenting path Reversed edge C  B where we can push a flow of  backwards  Ford – Fulkerson example Illustration augmenting flow with reversed edge • Goal is to find the maximum flow from A  D • Suppose after a first iteration we have the following C B / A / / D / /  Increase flow along augmenting path Pushing back the flow of  along this edge leads to / In the next iteration: no augmenting path anymore Ford - Fulkerson Algorithm: a max flow has been found of value   Traveling salesmen problem (TSP) Afbeeldingsresultaat voor order picking Synopsis figure What is the shortest route that visits each city and returns to the origin city? Afbeeldingsresultaat voor delivery company Applications: Afbeeldingsresultaat voor microchips  TSP • Given a network of n nodes (cities) with distances given by c ij between all nodes i , j • Goal: find the shortest tour visiting all nodes and returning to start node Example Eg, tour A  B  D  F  E  C  A has a length of   A C F B D E           TSP ILO model ILO model: • Decision variables: x ij = , i, j = , …, n • Objective : min ∑ i,j c ij x ij • Constraints :  ∑ i x ij = ∑ i x ji = , ∀ j  To prevent subtours : add: ∑ i,j ∈ S x ij < |S|, for all node subsets S:  ≤ |S| ≤ n -  , use i  j , else Satisfies the first set of constraints, but no tour number of different S =  n , thus ≈ n constraints! When traveling to city j, we should also leave this city  |S| = size S  TSP Algorithm Enumeration “naïve” algorithm: Iterate over all possible tours:  Calculate total distance current tour  If better than current best  new best However: • (n - )! = (n - )*(n - )* … ** possible tours • So runtime algorithm is approx n!  TSP Algorithm Some values for polynomial and non - polynomial functions: Really non - practical for reasonable sized instances… Can’t we do better ? (also not by a quantum computer: √ of the numbers)  Outline Practical aspects of real - life optimization: • Solvers • Modeling tools: Algebraic modeling languages and environments • Algorithms • Complexity theory • Heuristics  Complexity theory Based on running time of solving algorithm, roughly two problem classes: NP - complete P  Complexity theory Roughly, two classes of problems: • P : – Algorithms known that find optimal solutions even for large instances – Eg, transportation, shortest path, product mix • NP - complete : – No fast algorithms known … – Heuristics for large instances – Eg, knapsack, machine scheduling, set covering, TSP  How to win a million? Do one of the following: a Find a fast* algorithm for solving a NP - complete problem b Proof that there does not exist a fast* algorithm for solving a NP - complete problem and call me…  (Hint: most researchers believe that b is true) The Clay Mathematics Institute awards a US$ million prize for an answer * Polynomial time algorithm Afbeeldingsresultaat voor money funny  Outline Practical aspects of real - life optimization: • Solvers • Modeling tools: Algebraic modeling languages and environments • Algorithms • Complexity theory • Heuristics  Heuristics for TSP • No fast algorithm known for solving TSP • Heuristics aim not for optimum but for good solutions in reasonable time • Example:  - opt heuristic for TSP to be discussed next • Good heuristics ( - opt,) have near - optimal performance  TSP heuristic  - opt •  - opt heuristic for TSP : – Find an initial tour (eg, randomly move to a non - visited node) – Remove  by  all couples of tour edges – Reconnect for valid tour using other  edges – If improvement: move to this new tour – Repeat until no improvements Tour Distance Swap  Local minimum where the “swapping” stops Global minimum Swap  Swap  “Swap” (see next slide) Initial tour  TSP heuristic  - opt Example  - opt swap leading to better tour A C F B D E     "
"Statistics, Simulation & Optimization Lecture 12",stastics simulation and optimization,SSO,slides,simulation local test optimization budget gradient simulate random afbeelding optimum,"solution lecture level solutions newsvendor times option π,π continuous voor", example search significance problem best gerelateerde discard afbeeldingsresultaat r&s step,,1957,23,Joost Berkhout,2019,"Statistics, Simulation & Optimization Lecture  Joost Berkhout, VU  Program • Lecture : Linear optimization • Lecture : Advanced modeling • Lecture : Solvers, heuristics, complexity • Lecture : Simulation • Lecture : Simulation optimization • Lecture : Dynamic optimization • Lecture : W rap - up  Recap Real - life problem Model r() Output r( π ) Input “Represented by” - Fixed parameters - Decision π ∈ S Problem form: max r(π) st π ∈ S • S often very large (eg, TSP: |S| = (n - )!) • Algorithms to find solution can be – “polynomial” (eg, LO, Dijkstra ) – “non - polynomial” (eg, TSP, ILO) • But r( π ) can always be computed easily Afbeeldingsresultaat voor money funny Reminder:  million dollar opportunity  Today Real - life problem Input “Represented by” - Random parameters X - Decision π ∈ S Problem form: max E[ r( X, π ) ] st π ∈ S • Next two lectures: r( X, π) can only be evaluated by simulation Simulation - based optimization • Today: no decisions (π ∈ S fixed), only randomness:  Monte Carlo simulation of known function r( X) and known dimension of X (“spreadsheet simulation”)  Discrete - event simulation of more complex r( X ) Model r() Output r( X, π )  Goals today • Expectation and variance of random variables ( rv’s ) in simulation context • Simulation of a random variable • Discrete - event simulation • Statistical analysis of simulation results  Simulation of model r() Simulation setting Afbeeldingsresultaat voor data Data analysis parameters Probability distribution fit Generating parameter data x  , x  , …, x n “in line with real - life data” Process in model r() Statistical analysis  Motivating example • Project planning with random durations • Number given is expected duration • What is project finish time? • Depends on realizations (“x”) of durations: finish time = r( x A , x B , , x G ) • Objective: expected project finish time  Reminder: Random variables • Randomness is covered in a rv X • Two types of rv’s X: – Discrete: eg, X = sum of rolling two dices – Continuous: eg, X = number drawn from [a, b] ( < a < b) Probability distribution for the sum of two six-sided dice Afbeeldingsresultaat voor throwing a dice probabilities Afbeeldingsresultaat voor uniform distribution   pdf cdf Afbeeldingsresultaat voor uniform distribution   pdf cdf Probability of throwing  is / No direct probability notion Probability that number ≤ ( a+b )/ is / (a + b)/ / (cumulative distribution function ( cdf )) (probability density function (pdf)) (= P(X ≤ x)) (P(x) = probability mass function ( pmf ))  Reminder: Expectation • Random variable X, some function r(X) • The expectation of r(X), E[r(X)], is defined as follows: – X discrete: – X has density f: • In general: E[r(X)] ≠ r(E[X]) • X often multi - dim: complicated calculation • Solution: simulation  Reminder: Variance • The variance of a rv X is defined as σ  ( X) = E[(X – EX)  ] (note: X can be replaced by rv r(X)) • Intuition: “ σ  ( X) measures the spread of samples of X around EX” • σ ( X) is the standard deviation of X  Simulation • How to simulate from r(X ) ? • Generate independent samples x  , x  , …, x n of a random variable X • This gives samples r(x  ), r(x  ), …, r( x n ) of r(X ) • Note : r(X ) is itself a rv thus focus on X • Claim: the sample average (x  + x  + … + x n ) / n converges to EX for increasing n • Why does it converge ?  Law of large numbers (LLN) • Consider rv X i of the i - th random sample • Assume X i iid with X i = X (EX, σ  ( X)) • Then:  E[(X  +…+ X n ) / n] = ( Σ i E X i ) / n = EX  σ  [(X  +…+ X n ) / n] = ( Σ i σ  ( X i ) ) / n  = σ  ( X) / n (see next slide for the used properties) • “ Thus ” : (X  +…+ X n ) / n → EX for n → ∞ d LLN: Sample average of the simulation converges to expected performance (Random) sample average  Reminder: Properties • E[X + Y] = EX + EY • E[ aX ] = aEX • σ  (X + Y) = σ  (X) + σ  (Y) if X and Y independent • σ  ( aX ) = a  σ  (X), σ ( aX ) = a σ (X) • σ (X) has the same unit as X why mathematicians like the variance why practicioners like the standard deviation  Example • Project planning of A, B and C: • Rv’s of durations: X A , X B and X C • X A , X B , X C iid lognormal(, ) distributed • Finish time r(X A ,X B ,X C ) = max(X A , X B ) + X C • Goal: expected finish time E[ r(X A ,X B ,X C ) ] A B C Activity duration lognormal(,  ) pdf  Example (continued) What is E[max(X A ,X B ) + X C ]? A B C  Example (continued) • Simulation outcomes + running averages Average finishing time project C up until  realizations  Distribution sampling • Why LOGNORM INV ? • Inverse transformation method (ITM) •  steps (F is the distribution function of X): ) Sample u from a uniform [,] dist with rv U ) x = F -  (u) is a sample from X x - values Cdf F(x) = P(X ≤ x) “Sample uniformly from this interval [ ,] and return cdf corresponding x - value” u F -  (u ) Remark: Sometimes no exact expression known for F -  (eg, normal distribution , but approximation methods exist )  Working ITM • ITM works because for all x: P(F -  (U)  x) = P(U  F( x)) = F(x) = P(X  x), thus F -  (U) = X Intuition: d Exponential pdf Normal pdf Normal cdf Exponential cdf Intuition ITM: “The steeper F(x) increases on an interval for x, the more likely samples F -  (U ) will map to this interval This is nicely in line with the corresponding pdf”  Sampling in practice • Almost any programming language has a random number generator (RNG) that can sample u from a uniform [,] distribution • Uniform [a, b] sample (ITM ): a + (b – a)* u • Many languages have libraries for F -  • Some languages have functions to directly sample from common distributions (eg, normal and exponential distributions ) • In general : RNGs are not purely random and rely often on seeds  useful for replication purposes  • Excel: – RAND() gives uniform[,] – NORMINV() gives inverse of normal distribution – GAMMAINV () gives inverse of gamma distribution – BINOMINV() gives inverse of binomial distribution – Tools → Data Analysis gives also random numbers • R – runif (), rnorm (), etc generates samples – qnorm ( runif (n)) does same thing as rnorm (n) – setseed () sets seed of random generator Sampling in practice  Really random? Random bitmap based on atmospheric noise Randomorg Bitmap Random bitmap based on PHP's rand() function in Windows PHP rand() on Windows Bitmap Source: https://boallencom/random - numbershtml Really random: based on atmospheric noise  ITM example : sampling from the exponential distribution • Let X ~ Exp ( λ ) with cdf F(x ): F(x) =  - e - λ x • F(x) = u ⇔F -  (u) = x = - log( - u) / λ • U ~ uniform [,]  U =  - U • Thus inverse transformation method: ) Randomly sample u from [,] ) Calculate - log(u) / λ In Excel: - ln(rand()) / “lambda” d  ITM example : Discrete distribution sampling • Example : P(X = ) = / & P(X = ) = / • F -  (u) = min { x | F(x) ≥ u } • Practice : I Partition [,] into non - overlapping subintervals corresponding to all possible discrete outcomes II Draw from [,], see in which subinterval it falls and return corresponding discrete outcome - ≤ u ≤ /: sample =  / < u ≤ : sample =  F(x) (= P(X ≤ x)) x  Central Limit Theorem • How quickly does average converge? • Take Y n = (X  +…+ X n ) / n (X i iid with X i =X) • LLN: E Y n = EX, σ  ( Y n ) = σ  ( X) / n ( <∞) • To get a limit: study Z n =  n ( Y n – EX) • Then: E Z n = , σ  ( Z n ) = σ  ( X) • Central Limit Theorem (CLT) : Z n → N(, σ  ( X)) d Read as: Z n converges to the distribution as n tends to infinity N(μ, σ  ) is normal distribution with expectation μ and variance σ  ( σ is standard deviation) d  Confidence interval (CI) • The CLT is about a limit • For finite n the normal distribution is used as an approximation • Rv Y n is approximately distributed as N( EX , σ  ( X)/n) Afbeeldingsresultaat voor normal distribution EX % within  standard deviations ( σ ( X)/√n) from EX  Experiment Screen Shot -- at  png Screen Shot -- at  png Screen Shot -- at  png Screen Shot -- at  png Histogram of: •  samples Y  •  samples Y  •  samples Y  •  samples Y  (For a live experiment click here )   ≈P( EX -  σ ( X)/ √ n ≤ Y n ≤ EX +  σ ( X)/ √ n) • Thus : rv Y n is with % probability not further than  σ ( X)/ √ n away from EX • Thus : [ Y n -  σ ( X)/ √ n, Y n +  σ ( X)/ √ n] is a % confidence interval for EX • However : σ ( X) is usual unknown • Estimator σ  ( X): s  = Σ i (X i – Y n )  / (n - ) Confidence interval = P( Y n -  σ ( X)/ √ n ≤ EX ≤ Y n +  σ ( X)/ √ n) The n -  ensures that E [s  ] = σ  ( X) % confidence interval of Y n (More precise:  percentile of standard normal dist =  instead )  • Thus : CI of EX is [ Y n -  s / √ n, Y n + s / √ n] • But s is rv :  →  percentile of t - dist with n -  “degrees of freedom” ≈  (for n large enough) • Note : to double precision,  times as many experiments necessary! Confidence interval EX “% of the CIs generated will contain EX”  Example •  simulations: – Average =  – Standard deviation estimation =  – % CI = [ -  *  / ,  +  *  / ] = [, ] Gives a “well - founded idea” about how far our estimate is from the true EX  Discrete - event simulation Motivation: simulation of real - life processes that are too complex to capture in r( X) (or r( X, π) ) Examples: Afbeeldingsresultaat voor queue If only they used some simulation… Service system with customers Factory with work - in - process Other examples: customers in sales funnel, disease progression in patients (Sometimes called “micro - simulation” or “digital twin”)  Discrete - event simulation (DES) • Simulates a process over time using a discrete sequence of events over time • Keeps track of the state of the process • At each event the state is updated • State unchanged between consecutive events  So repeatedly jump to next event and process • Repeat simulation and keep track of performance measure(s) • Statistical analysis of performance measures  Time (min) # customers in queue           State Event: A service ready Event: New customer arrives DES example • Consider the following queue • DES idea: germany, kiosk, line Exponential interarrival times between customers The service time is lognormally distributed     DES example Queue with exponential interarrival times and lognormal service time distribution “Current event is a departure or an arrival at an empty system”  Determine next event:  Update time:  Update state:  Update next arrival time:  Update next departure time:  DES Tooling • Programming in any programming language – Libraries with random number generators and event list handling – Appropriate for (and even origin of) object - oriented programming • Graphical DES software  promodelscreenshot Graphical DES software • Graphical interfaces • Drag and drop modeling ＋ quick in modeling ＋ easy to present − slow in calculation − lacks flexibility − expensive Screen Shot -- at  png  Screen Shot -- at  png Many tools Screen Shot -- at  png  promodelscreenshot Arena • Simple process : wwwyoutubecom/watch?v=dlbWWFens • For a free restricted student version : https://wwwarenasimulationcom/academic/students  Performance • Often an average over time • Example: average # in queue per day • CI as for Monte Carlo simulation • Variability decreases with length of horizon – Slowly because of long - term correlations  • Expectation and variance of random variables ( rv’s ) in simulation context • Simulation of a random variable • Discrete - event simulation • Statistical analysis of simulation results Recap goals today "
"Statistics, Simulation & Optimization Lecture 13",stastics simulation and optimization,SSO,slides,inf optimality optimal state decision conditions dynamic function a(x) backwards,"ime max costs revenue path voor γ(x,π) backward open recursively",problem min inventory class optimization equation d(x) afbeeldingsresultaat shortest example,,3986,40,Joost Berkhout,2019,"Statistics, Simulation & Optimization Lecture  Joost Berkhout, VU Remark about Assignment  Exercise  a): it may theoretically be the case that you need more than  simulations, so therefore, I changed the way of question asking: Was: “ Simulate the revenue for every price  times to determine the optimum price with a  significance level using an appropriate test ” Now: “Simulate the revenue for every price often enough (eg,  times ) to determine the optimum price with a  significance level using an appropriate test”  Program • Lecture : Linear optimization • Lecture : Advanced modeling • Lecture : Solvers, heuristics, complexity • Lecture : Simulation • Lecture : Simulation optimization • Lecture : Dynamic optimization • Lecture : W rap - up  Today Real - life problem Input “Represented by” - Random parameters X - Decision π ∈ S Problem form: max E[ r( X, π ) ] st π ∈ S Model r() Output r( X, π ) Rv r(X,π) can only be simulated Simulation - based optimization  Goal today Introduction to simulation - based optimization: • Main challenges • Overview of solving strategies in different settings  • More complicated than deterministic optimization – No optimality/performance guarantees – Might overestimate optimum – Simulation might be time - consuming (especially when simulating complex processes = discrete - event simulation) • Example:  values for π, E[ r( X, π ) ] close for different π but r( X, π ) highly variable • For practical reasons often a limited simulation budget (eg, max K simulations) • Challenge: how to spend this budget efficiently? • Even more challenging when feasible region S random (eg, of form E[g(X, π)] ≤ ) Simulation optimization  Simulation optimization Roughly  types of problems: • |S| =  • S small – “ranking and selection” • S big or countable but discrete (eg, a grid) – local search • S continuous – gradient methods S  Comparing scenarios • Using simulation, goal is not EX but comparing two scenarios X and X ’ : Is X better or worse than X ’ ?  Is E X ≠ E X’ ? • Sample from X and X’ and apply t - test for two means of two independent samples (Lecture ) Afbeeldingsresultaat voor production factory Afbeeldingsresultaat voor factory blue robots Layout : throughput rv X Layout : throughput rv X’ Example:  Comparing scenarios • By smartly designing our simulation experiment:  more statistical power • Key idea: “compare scenarios X and X’ in a similar (‘fair’) test setting” • Take matched pairs of samples from X and X’ • Ensure that each sample pair ( X i , X’ i ) is positively correlated by using a similar test setting • Consider Y = X - X’ • Simulate E Y = E[ X - X’ ] and make CI • If  ∉ CI: significant difference t - test for means of matched pairs (Lecture )  Common random numbers • Common random numbers (CRN) can reduce σ  ( Y ) (“shrink CI”  more statistical power ) • Method to reduce σ  ( Y ) = σ  ( X - X’ ) = σ  ( X ) + σ  ( X’ ) - Cov( X , X’ ) • Ensure that samples from X and X’ are positively correlated so that Cov ( X , X’ ) >  • In practice: use the same underlying samples for sampling from X and X ’ (“similar test setting”) Holds also when X & X’ are dependent/correlated Measures the correlation between rv’s X & X’  Example CRN • A new factory layout is simulated and compared with old one • Outcomes x i and x i ’ , difference y i = x i - x i ’ • Confidence interval is based on y i • By taking order arrivals and processing times the same when sampling X and X’ the sample variance s  ( Y ) is reduced Independent traces With CRN  Ranking and selection (R&S) π ∈ S with S finite and small Example Newsvendor Problem: • Buys newspapers for €  and sells for €  • Random demand = X ~Poisson() • Left - overs are worthless • How many papers π to buy for max expected profit? • Random profit = r( X , π ) = min( X , π ) -  π • π ∈ S = {, , , …, } • Simulation - optimization problem : max E[min( X , π )] -  π , for π ∈ S Gerelateerde afbeelding = E[ r( X , π ) ]  Ranking and selection • Simulation budget m : m runs possible • Option  : simulate every solution m /|S| times and return best solution • However, suppose that after < m runs we have the following % CIs for E[ r( X , π ) ] Are these outer solutions still worthwhile to simulate? Idea R&S Option : Discard statistically “bad” solutions more quickly and focus on better ones  Ranking and selection • π ∈ S with S finite and small • Simulation budget m : m runs possible • Option  :  Simulate every solution a number of times and calculate averages and variances  Perform all pairwise t - tests* for ( π , π’ ) to see if π is significantly “outperformed” by a π’  Discard outperformed solutions  Divide remaining simulation budget over the remaining candidate solutions & return best * Because π can be outperformed by |S| -  other solutions a t - test is applied with “conservatively” significance level 𝛼*  R&S Option  Testing • We want to be careful to not discard the best solution with significance level 𝛼 • So for each π , test 𝐻( π ): π is best solution 𝐻( π ) : π is not best solution • Per π , t - test for all π’ ≠ π with significance level 𝛼* 𝐻( π,π’): π is not worse than π’ 𝐻( π,π’): π is worse than π’ • If 𝐻( π,π’) is rejected for a π ’ : discard π Multiple testing problem At least on π’ ≠ π is better |S| -  t - tests (reject 𝐻( π ))  Choosing significance level 𝛼* • |S| -  chances of falsely discarding π, so choose 𝛼* such that overall significance level is 𝛼 • Interpretations:  - 𝛼= probability of not rejecting a true 𝐻( π )  - 𝛼* = probability of not rejecting a true 𝐻( π,π’) • When 𝐻( π ) is true , the probability of not rejecting any (true) 𝐻( π,π’) for π’ ≠ π is ( - 𝛼* ) |S| -  • So choose 𝛼* so that (  - 𝛼* ) |S| -  =  - 𝛼, ie, 𝛼* =  - 𝑆−-𝛼 Šidák correction  R&S Option  Example newsvendor problem • After simulating each solution n =  times in Step : • Which solutions to discard in Step  with significance level 𝛼= ? Gerelateerde afbeelding  R&S Option  Example newsvendor problem ( 𝛼= ) • y( π ) & s  ( π ) are sample average & variance of π • Do not discard π if for all π’ y( π ) > y( π’ ) – β  - 𝛼* √ (s  ( π )+s  ( π’ )) / √n with β  - 𝛼* inverse t - distribution at  - 𝛼* ( df = n - ) • Specifically: 𝛼* =  - −𝛼= , β  - 𝛼* =  • Split remaining budget between solutions in I: I = { π | y( π ) > y( π’ ) - β  - 𝛼* √(s  ( π )+s  ( π’ ))/√n, ∀ π’ } • Return the best solution in I (P(“opt in I ”) =  - 𝛼) • In newsvendor example: I = {,,} Gerelateerde afbeelding T - test: “ 𝐻( π,π’) is not rejected”  Local search • π ∈ S with S big or countable but discrete • Neighborhood N ( π ) for each π ∈ S Local search algorithm: • Initialization: n( π )= & y( π )=, ∀ π , some initial state π * • Repeat until simulation budget spent: – Choose randomly π’ from N ( π * ) – Simulate π’ and π * once – Update n (# visits) and y (average) of π’ and π * – If y( π’ ) > y( π * ), go to π’ (ie, π * = π’ ) • A best guess local optimum: π’ = arg max n( π ) π N ( π ) # visits/simulations to π  Local search comparison Local search algorithm (from previous slide): • Initialization: n( π )= & y( π )= ∀ π , some initial state π * • Repeat until simulation budget spent: – Choose randomly π’ from N ( π * ) – Simulate π’ and π * once – Update n (# visits) and y (average value) of π’ and π * – Go to π’ (ie, π * = π’ ) if y( π’ ) > y( π * ) • A best guess local optimum: π’ = arg max n( π ) Compare with deterministic local search : • Initialization: some initial state π * • Repeat until no improvement for some time: – choose randomly π’ from N ( π * ) – go to π’ if g( π’ ) > g( π * ) • Local optimum: π’ “The randomness in stochastic local search leads to more exploration of the solution space”  Newsvendor example • Newsvendor with buy and sell prices of  and  • Random demand = X ~Poisson() • Maximum shelf space is  ( thus π ≤ ) • When π >  then buy price is  • Use local search simopt with : N ( π ) = { π - , π + },  < π < , N () = {} and N () = {} • See Excel demo ( simulation budget of ) local opt , global opt  Gerelateerde afbeelding  local search runs (each of  simulations) give solutions :  (!) and   Gradient methods • S continuous (example: interval) • Continuous steps to local maximum • Gradient descent algorithm: • Gradient estimate: based on simulation • Step size: theory says γ k =/k , practice often constant new solution old solution step size S π k+ = π k + γ k+ ▽ E[r(X, π k )] The gradient points in the direction of the greatest rate of increase of the function Gerelateerde afbeelding  Example gradient method Consider newsvendor problem as before but now with a continuous product (eg, liters of milk) and continuous demand X ~Gamma(, ) • Gradient descent algorithm: π k+ = π k + γ k+ ▽ E[r( X , π k )] • Approximate the gradient via finite - difference method ▽ E[r(X, π k )] ≈ ( y( π k ) - y( π k +  ) ) /  , for small  >  • In Excel demo: –  =  – Start π  =  – Step size γ k = *() k Gerelateerde afbeelding Afbeeldingsresultaat voor milk  Example gradient method Afbeeldingsresultaat voor milk Gerelateerde afbeelding Afbeeldingsresultaat voor milk "
"Statistics, Simulation & Optimization Lecture 14",stastics simulation and optimization,SSO,slides,optimization problem medicine lecture success ilo probability state simulation animal,new afbeeldingsresultaat production model process beta phase scheduling objective data,voor learning time modeling reinforcement patients gerelateerde png customer current,,1667,31,Joost Berkhout,2019,"Statistics, Simulation & Optimization Lecture  Joost Berkhout, VU Some announcements: • Updated slide  from Lecture  slides: x  =  (instead of ) • Updated some minor typos and added some comments in Lecture  slides • A kindly request to fill in the evaluation forms next week during/after the exam and hand them in afterwards This will be used to improve the course thanks!  Program • Lecture : Linear optimization • Lecture : Advanced modeling • Lecture : Solvers, heuristics, complexity • Lecture : Simulation • Lecture : Simulation optimization • Lecture : Dynamic optimization • Lecture : W rap - up  Today • Reinforcement learning • Wrap - up • Animal feed p roduction planning optimization  Last lecture Stochastic dynamic optimization (DO) problems: • Decision π ∈ S is a sequence of decisions over time: π = ( π  , π  , …, π T ) • X t = random state of the process at time t • Transitions are random with known probabilities: p t (x, π t ,y ) = probability of going from state x to y when choosing π t at time t • The objective function is of the form r(π ) = Σ t = T E[ r t ( X t ,π t )] Real - life problem Input “Represented by” - Random parameters - Decision π ∈ S Problem form: max r( π ) st π ∈ S Model r() Output r( π ) π t is decision at time t Expected reward at time t when choosing π t at state X t  Reinforcement learning (RL) • In practice: transition probabilities unknown • Process information becomes available while operating the system • Examples: – Introduction of a new medicine – New product demand in case of the newsvendor problem – Price elasticity in hotel industry • The challenge is how to balance: i Exploration : Gaining new information ii Exploitation : Maximizing rewards with current information • Framework for this trade - off: Reinforcement learning  Medicine problem • Introduction of a new medicine • Old medicine is well - known and has success - probability s • New medicine has unknown success - probability modeled by a uniform distribution [, ] • T patients to be cured • Goal: cure as many of the T patients Gerelateerde afbeelding Afbeeldingsresultaat voor waiting patients cartoon    … T  DO model medicine problem • Patient t is the t - th arriving sick patient • State = (m, n) is defined as: – m successful treatments with new medicine – n unsuccessful treatments with new medicine • V t (m, n) = maximum expected successful treatments of patients t, t + , …, T • p (m, n) = expected success probability when choosing new medicine that has been applied m + n times with m successes • By assumption initially: p (, ) = EX =  X uniform distr [,] Gerelateerde afbeelding  Success probability in medicine problem • Define random variable X m,n = true success probability of new medicine after m + n trials with m successes • p (m, n) = E X m,n • Bayesian inference states: X m,n ~ Beta(m + , n + ) • Result: p (m, n) = E X m,n = (m+)/(m+n+) (note that this matches intuition) “Closed ” within class of Beta distributions: Beta(successes+,failures+) Gerelateerde afbeelding  Bayesian inference Screen Shot -- at  png Experiment Success Failure Screen Shot -- at  png Screen Shot -- at  png Screen Shot -- at  png Screen Shot -- at  png Screen Shot -- at  png Pdf of success probability at start = uniform on [,] (“no info”) The (“posterior”) pdf’s of the true success probability are updated with each new experiment (= info) X , ~ Beta(,), E X , =  Alternative to randomized trials – applicable to other areas such as online marketing Experiment Success Failure Success Failure X , ~ Beta(,), E X , = / X , ~ Beta(,), E X , = / X , ~ Beta(,), E X , = / X , ~ Beta(,), E X , = / X , ~ Beta(,), E X , = /  “New” DO model medicine problem • V t (m, n) = maximum expected successful V t (m , n) = treatments of patients t, t + , …, T • Goal : Find V  (, ) • Decision: π t ∊ {,},  = if patient t gets old medicine,  else • Optimality equation : for m, n ∊ { ,,…,T} V t (m, n) = max{ p (m, n) ( + V t+ (m+, n)) + ( - p (m, n) ) V t+ (m, n+); s ( + V t+ (m, n)) + ( - s ) V t+ (m, n) } • Boundary conditions : for m, n ∊ {,,…,T } V T+ (m, n) =  “Old” Gerelateerde afbeelding  Balancing between exploration and exploitation • Suppose that true success probability q of new medicine is s < q < : – It still may happen that state (, ) is reached, indicating that the new medicine seems to be bad – However, RL may still consider the new medicine if there are enough remaining patients so that it may be beneficial to try the new medicine a few more times • Problem: State space explosion (it weighs all experiment possibilities) • RL research focuses on approximatively solving these enormous problems Gerelateerde afbeelding Using machine learning techniques  Reinforcement learning • Hot topic in artificial intelligence • Example application domains Afbeeldingsresultaat voor robotics mars Robotics Automatic negotiation Afbeeldingsresultaat voor AlphaGO Playing “games” (Click here for an example with surprising results) Afbeeldingsresultaat voor negotiation\  Today • Reinforcement learning • Wrap - up • Animal feed p roduction planning optimization  Using data and predictions: What should we do? More value, but less used* Statistics part * Source: renowned consultancy company Gartner, see here and here Simulation & optimization part Less value, but more used The big picture  Optimization process • Phase  : Problem identification and study – Problem type? – Benefits of solving? – What would a solution look like? – Ethics? (“ With great power comes great responsibility”) • Phase  : Solution development – Existing methods? – Modeling (see optimization model decision tree next slide) – Data analysis • Phase  : Testing – Statistical analysis of solving historical instances • Phase  : Implementation – User interface – Psychological effects users Iterating phases  Optimization model decision tree dynamic problem with state - dependent actions? (stochastic) dynamic optimization objective for given solution can be calculated? simulation optimization problem with known algorithm? (non)linear optimization formulation? adaptable to local search or other heuristic? YES NO only simulation YES YES apply algorithm NO NO YES YES apply (non)linear optimization NO apply local search/heuristic no method available – decompose?  Today • Reinforcement learning • Wrap - up • Animal feed p roduction planning optimization  Productionplanning optimization in the animal feed industry Joint project with : Jan Stolze , Siem Broersen & Viktor Klein (ENGIE) prof dr Rob van der Mei & dr Eric Pauwels (CWI) Wouter Berkelmans (VU) Public private partnership between : C:\Users\MQ\Downloads\Presentatie Techport (--)\CWI_Logo_Croppedjpg C:\Users\MQ\Downloads\Presentatie Techport (--)\ENGIE_logo_croppedjpg Afbeeldingsresultaat voor vu university  Content • Problem description • Approach • Current results • Concluding remarks  Scheduling in Animal - Feed Plants Gerelateerde afbeelding Afbeeldingsresultaat voor silos World - wide: ,,,, kg animal - feed each year , plants in  countries  plants in the Netherlands  Production Process  Production aspects: • Customer order due dates • Contamination • Changeover time between products (eg, sieve change) • Finite silo capacities • Stochastic production durations with season trends • … Production Scheduling Problem How to “efficiently” schedule the customer orders? Minimize customer weighted tardiness and makespan  Production Scheduling Problem Current situation: planners ‘schedule by hand’ As a result: time - consuming and opportunity loss (inflexible and ‘big data’ unused) Trend: mass - customization & plant automation (  big data)  Project Objective: Optimization (algorithm) Customer orders (due dates) Objective (min due date exceedances & makespan ) Historical data Production process and restrictions Scheduling advice  Approach  Study of production process  Modeling of production scheduling problem Crucial : an accurate model of the process  Model testing at pilot plant  near Eindhoven A lot of technical details …  Modeling phase in short: Afbeeldingsresultaat voor gurobi Simplification: Integer linear optimization (ILO) problem: ILO implementation: Accuracy testing: Solve MILP: Schedule advice:  Modeling schedulingproblem Scheduling problem is modeled as a Integer linear optimization (ILO) problem Objective Minimize tardiness and makespan Constraints Production schedules  Reasons for ILO:  Formulating an ILO forces you to dive into problem fundamentals  It is flexible: easy to add another restriction in modeling phase  Capabilities ILO solvers (eg, Gurobi or CPLEX) and modern PC’s  ILO finds an optimal solution (for comparison with heuristics)  ILO useful as basis for a heuristic for larger instances Note: ILO not capable of solving really large instances (>  hours) Standard ILO formulation:  Comparison to realized schedules for  instances of  hours when solving for  seconds A ll schedules found respect the due dates Current results WED: Fixing customer order sequence in MILP based on due dates  Concluding remarks • Testing results look promising (efficiency gain of %) • Model runs now in pilot plant : propose schedules, get feedback and test real - life performance • Enhance optimization strategy for larger instances:  Reducing ILO solution space by using common practice  (Meta) - heuristics  evolutionary algorithms  Thanks for your attention! Any questions? Mail: jberkhout@cwinl Afbeeldingsresultaat voor silos Afbeeldingsresultaat voor boerenbond deurne Afbeeldingsresultaat voor koe in de wei "