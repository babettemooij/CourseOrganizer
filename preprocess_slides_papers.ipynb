{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_slides(filename):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        data = f.readlines()\n",
    "    # remove newlines\n",
    "    data = [x.replace(\".\", '') for x in data]\n",
    "    data = [x.replace(\"\\n\", '').split(' ') for x in data]\n",
    "    data = itertools.chain(*data)\n",
    "    # remove empty lines\n",
    "    data = list(filter(None, data))\n",
    "    # remove digits as in page number\n",
    "    data = [ ''.join(i for i in s if not i.isdigit()) for s in data]\n",
    "    print(len(data))\n",
    "    str_data = ' '.join([str(elem) for elem in data]) \n",
    "    # write to file\n",
    "    print(len(str_data))\n",
    "    with open(filename, 'w', encoding=\"utf8\") as f:\n",
    "        data = f.write(f\"IO\\t 1\\t{str_data}\")\n",
    "    return str_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n",
      "8814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Applied Machine Learning lecture  Classification II Pascal Mettes – University of Amsterdam \\x0c Classification yesterday F:\\\\Dropbox\\\\VSEpruts\\\\knnpng \\x0c Topics of today Decision trees Random forests Boosting Early introduction to the final project \\x0c https://lhgoogleusercontentcom/GPCBpouQHPaFMMwkCHS_GhSMIgoGsrjSzavVGTllcENvk-mAgyfJbxGcdIKEBlJRkGUTTFzVfsyrgrdYJqqcVgANBoMTGfTdfxeKfyVeuXdBeIFfoQuBV-xM \\x0c Building a decision tree https://lhgoogleusercontentcom/-YazHJiYZGcafZRLQ_UmYRaJWrzLDwuLNvChOuaTQZgtDphzGoFQslaUELFgzcSMkDydRjdXLIBxZqYvnwxNbhdsTNrAp_GNVfL-ucVLCvPSYEQifPmCwfnVmUYA \\x0c Benefits of decision trees  Explain the reasoning behind a decision  Work on discrete and continuous features  Work with incomparable features  Fast to train and to evaluate \\x0c Alternative decision tree Also explains the data, but which one is preferred? https://lhgoogleusercontentcom/ALmNfwafPpovPSceXsXEphTWgjxVXMgZSdIiNDeJ_FToDusHsMbSjjBPnxLWLSdldhAvDvqTuFxtKEwOFZOXJgKE-eua_yqMvNknVjVtvyIMWEqKpbbONaQ \\x0c How to split data into a tree Main notions We want a simple tree Each node should be as “pure” as possible, one feature at a time Pureness measure Evaluation using entropy The more uniform a distribution in a node, the higher the entropy https://lhgoogleusercontentcom/GSKOWFBMlO_bKOkQpkeRLt_RxQxJvLqavxDvyG-XqD-LcgUwyjPbZfJ_msyklwyEPFGzNsHNNZm_yFKHOkXzSyrsAbUj_RBuCf-wxyoFWLW_RTHXjVJ-TDP \\x0c Entropy for two classes Notation S denotes the training set P denotes the ratio of positives in S N denotes the ratio of negatives in S Entropy formula 𝐸𝑆=−𝑃𝑙𝑜𝑔𝑃−𝑁𝑙𝑜𝑔(𝑁) https://lhgoogleusercontentcom/HnoiEfPXnbTwNCKMzcZuzhbTyFvvQHPZGQcizOstFfpBdjkiFmppYBhLzrjr-KhCNXU-DGmiurmKZGJEBm-FMTCdkUoLJLXBiacueqK-OiASlCIk \\x0c Entropy examples  positives,  negatives:  positives,  negatives:  positives,  negative: 𝐸𝑆=−𝑃𝑙𝑜𝑔𝑃−𝑁𝑙𝑜𝑔(𝑁) −𝑙𝑜𝑔−𝑙𝑜𝑔= −𝑙𝑜𝑔−𝑙𝑜𝑔= −𝑙𝑜𝑔−𝑙𝑜𝑔= \\x0c Practicing tree splitting Practice on the board \\x0c Multi - class decision trees \\x0c Multi - class entropy The entropy at a certain node: Weighted average of entropies at a split A : 𝐸𝑆=−𝑃𝑙𝑜𝑔𝑃−𝑃𝑙𝑜𝑔𝑃−⋯−𝑃𝑛𝑙𝑜𝑔𝑃𝑛 𝐸𝑆=−\\u0dcd 𝑖= 𝑛 𝑃𝑖𝑙𝑜𝑔𝑃𝑖 𝐸𝑆=−\\u0dcd 𝑖= 𝑇(𝐴) 𝑆𝑖 𝑆 𝐸(𝑆𝑖) \\x0c Information gain Which split is the best? The one that decreases the entropy the most G 𝑆,𝐴=𝐸𝑆−𝐼(𝑆,𝐴) https://lhgoogleusercontentcom/DhtEUCafwZskTwqgaiaiIGcxmSjIkGXHqODPagDyGMy-YZnlFkrnPcODMnAUZRtvYTU-DjueDkWGUCGlXkMreGgRsg_wefzfo-QKElMwWW-bvgU_wncls \\x0c Limits of decision trees  For many examples and features, we obtain deep trees  Perfect training separation, but poor generalization Three solutions  Stop growing the tree (suboptimal training fit)  Grow full tree, then prune (shorten) tree a posteriori  Combine multiple trees \\x0c Random forests https://lhgoogleusercontentcom/qbuhPpDauWqTioMinCPmLLQxHoXHzhaUCYFDzjglvboCjRBxeFfdrkKnDXcULKrDtnAECADyORewbrrKDVbQwpjTAcNBbyJPhElxRCWiVlmeFabbYvrpUVoSRU \\x0c Random forests https://lhgoogleusercontentcom/qbuhPpDauWqTioMinCPmLLQxHoXHzhaUCYFDzjglvboCjRBxeFfdrkKnDXcULKrDtnAECADyORewbrrKDVbQwpjTAcNBbyJPhElxRCWiVlmeFabbYvrpUVoSRU Rather than training one tree perfectly, train multiple trees imperfectly Train many trees, each trained on a random subset of features and/or examples \\x0c Training random forests https://lhgoogleusercontentcom/qbuhPpDauWqTioMinCPmLLQxHoXHzhaUCYFDzjglvboCjRBxeFfdrkKnDXcULKrDtnAECADyORewbrrKDVbQwpjTAcNBbyJPhElxRCWiVlmeFabbYvrpUVoSRU Parameters K trees, D tree depth, R #features per split Training • Initialize forest F • For tree k = ,, K • For depth d = ,, D • Randomly select R features • Split node based on features • Add k to F \\x0c Inference in random forests https://lhgoogleusercontentcom/qbuhPpDauWqTioMinCPmLLQxHoXHzhaUCYFDzjglvboCjRBxeFfdrkKnDXcULKrDtnAECADyORewbrrKDVbQwpjTAcNBbyJPhElxRCWiVlmeFabbYvrpUVoSRU Traverse each tree separately for a test example Average class scores over trees and select maximum average score \\x0c Why are random forests effective? The average of many estimates with low bias and high variance is close to true estimate https://lhgoogleusercontentcom/BTpJBlgYbwaPAqmzBthdDgb_yDikOtKIGGnOeoaCngVKbzkBWzsCLaBcICkqIyVm_jBpjyqrGzzTHXLyjBwllMwwECnebjREUOAZVrVXnEbSRwIqFoFbiE \\x0c Using many classifiers for prediction https://lhgoogleusercontentcom/hkVtozoPLjIMzzcJjiQvdTky_jNkmDVWSbViSvwbBRU-_OgXBwNlmh-WitzbegKIIcwwCDqnVySPqTzOcSBsdeEIX-t_GfDOSCQ-wzULvWeUyxrvKlNQ https://lhgoogleusercontentcom/qoMh-ShtlSKSPbgFC-laYuEUVgJjgcdFQsOFK_QUSNoAYsaILKVsmHtSQjUL-zvxNoyHNSuQnkCxAcDrVXmKcMlsy-kJmR-UdEgkmCXnRHGrGadwHGyxqKYbE \\x0c Ensembling Main idea: Generate multiple models and combine them into one classifier Two problems: How do we get multiple models? How do we aggregate outputs of multiple models? \\x0c Bagging Bootstrap sampling: For training set D containing N examples, create a training subset D’ by drawing N examples at random with replacement from D Bagging: • Create k bootstrap subsets D  ,…, D k • Train a classifier for each subset • Classify test example by majority vote over subset classifiers \\x0c Break \\x0c Boosting Within standard bagging and ensembling , models are trained separately Is there a gain if we train models to complement each other? \\x0c AdaBoost: main idea Attach a weight to each training example Select a “weak classifier” with the lowest training error Increase weight of incorrectly classified training examples Iteratively select new best weak classifier with lower weighted error \\x0c Toy example of AdaBoost https://lhgoogleusercontentcom/bDIMgcHCVPQhnsVdEIEzLIDjBdlblSfVSVlQTPsLTualSveHEHLlQYlD-hleNSrGYnHBMuOC-_YDsyWjiOhkKRqOeszdvCMvBfM_MNUMStSarrFxSALntxuEDPE https://lhgoogleusercontentcom/OqEPwQBEGSEBA-PYbPFsKpLjdohVTUtruagaPTXnnnaXDGubzSm_QSQgdrAGhlIbxTAvENSumRjIoLNCsWckTsTCcW-VylVGlVvQyPqJrkGwXQGnXc \\x0c Toy example of AdaBoost https://lhgoogleusercontentcom/Hcgqn-EYaMREBurCYyhHTpldjuxOuqRDJsuNfZhbvIn-YFepcUuvHwovyfVBGkohoZi_zPjmlJEkgANAofbPe-aBN-fpbrJUFAMgjYVfImlIsCZNNsbU https://lhgoogleusercontentcom/brMjiJZaRQqPBoMlPAEGJntQxYhHPstiaDzxYzgHsLjeRdDqnrwBWpODMjkKdEjbgiLbTwyPWrPofcEcRqXKrr_G_vKoMoh-yiBDZiWooCxPq-TqhSpLPd \\x0c Toy example of AdaBoost https://lhgoogleusercontentcom/BvLwXYJCCXINahaFHaOcEzAZTkQNPHoUqDgvizdQkgtRE-kiJpZIlOnIAjyuyzgOpeFaVhjCpfAXrFovQjIrTZXsgWkEJuWWrscafROjunlkkbmFFlEHrvo \\x0c Toy example of AdaBoost https://lhgoogleusercontentcom/DiycGDUa-lCMzdnsrUhkRIPpXETDCZfblMXDYawklzujimSkjpCujlZZtfcJFOCU-zWSzbLKVy-kAGnvwDaLWQheMR-OfEiQpZKctNcybOglwWXOQKZlifsIpQLg \\x0c Toy example of AdaBoost https://lhgoogleusercontentcom/xjS_hFdGsNWcbyi-SlTNoiCztmGFJjIAzSlAmevstiiKwZsymxWAuORXmwlAdPVdGSwakWCfVxVvZIVyxhYHtqxeeEZudKArisGkrFGPxYr-tDYFbbJTw \\x0c https://lhgoogleusercontentcom/icThEOLNflSooKzOlGcKwYnVQjJAhTDhhiBtJEbUPjeYKBJUbVnDasUeIUdxoft-Op_HPhUbBVOKMrzKLxIUdMKqfN_UJUUpaovkPTeHWDSHn-mRjoCRWcJDBhbo Training data and weights Final classifier Number of iterations Select best weak learner Weak learner weight Update example weights \\x0c AdaBoost properties “ AdBoost asymptotically converges to the minimum possible exponential training loss, as long as weak learners perform above average” Increased complexity due to more iterations and weak classifiers does not necessarily lead to overfitting https://lhgoogleusercontentcom/bbRkfP-nCiGVrIk_zZbnsLzvuTKLsy_VdCXzstSNccuLHOCOqCAvyzWCywBKho-BDLgDaEejSdQelUsShTTGJBLzfsYfzmeJUbaiFvNtUVgYCCEqzNLbOt \\x0c AdaBoost limitations What happens when weak classifiers are strong? No training error, hence no benefit What is an advantage of standard ensembling ? Parallelism! \\x0c Classification in practice \\x0c Why teach something that can be abstracted? \\x0c The final project Kaggle project on one of three highly challenging tasks Leaderboard per task for all groups in the course “Machine learning proving ground” Generally considered to be a highlight of the course \\x0c Task : Food recognition challenge Predict food categories from images https://lhgoogleusercontentcom/mYuXEreeMmdPnflkzHWdXOpHOOcpgMCqxSKdfFUwIQHCDGbxmdwTkRSryGljkBOxcZcELOHnMNocLTWbEU_Ty_wOV_lNdnUyPfxkairCiOJfNfsLo \\x0c Task : Common sense challenge Identify common sense (or not) from sentences https://lhgoogleusercontentcom/frkWT-YIuGBszMkggkJKZ-mSFkEBueQgsHlmJioJbZjoVCUMaZO_bRndlcHoDTRwTAMmlRX-QuxmDxEeTmoWCGUpeW-uGpcqCQjKxSrETzBKOg \\x0c Task : Meme analysis challenge Humor type classification from images with overlaid text https://lhgoogleusercontentcom/iPSlewtnIdxHkKbUwNUcbvAiJgLME_VTVCvdlVZhfV-BGGUOVgh-iQOYmnzHnuOHhdZGKQDCojL-UDozYjvZSVOCohNknCLaqcYboBptiEvrk_w \\x0c Grading Poster and presentation after exam Four axes:  Innovation  Experiments and setup  Analysis  Pitch and poster design https://lhgoogleusercontentcom/iUyapHYLkVTrxc-jEOdkvWYZuCzUVptGaWAD-HwfTNIj-_sgahkssTdhrgzThwZhWfDtzx-ZCcBmw-m_fuRdxLIEmduskdw-pNhHSFzgZYeNwGNctIHSzuEMCANw https://lhgoogleusercontentcom/SUmI-ctHadUGB-ASwnidGmCcPDvbogAxWDyQulYMaLREqLLkpNc-AMfCIdQvzDYwDBnxTvcKBZVJCZcHJxChrcCQy-TACdBixllnnnEwGmYATniChJIuY \\x0c What’s next Lecture Deep Learning I, Monday  -  : - :, Turingzaal \\x0c Thank you \\x0c'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = clean_slides('parctice.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
